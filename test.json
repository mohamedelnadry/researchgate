[
    {
        "title": "Investigation on biomedical waste management of hospitals using cohort intelligence algorithm",
        "date": "December 2021",
        "doi": "10.1016/j.socl.2020.100008",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (13)",
        "abstract": "With the innovative development of advanced technology in the field of medical, there is an enlargement in the generation of other problems such as management of biomedical waste. Hazardous waste generated from hospitals is required to be managed within time and it can be done effectively using some computer science technology. In the proposed methodology, Biomedical Waste (BMW) problem is solved with the consideration of route optimization. Route optimization is important in BMW management because while transporting the BMW from hospital to depot (disposal site) there are many types of risks associated with that route like traffic, vehicle failure, road accident etc. To avoid the dangerous effects of BMW on humans and environment, it is necessary to optimize the distance. It can help in promoting healthy and risk free life. This paper addresses the problem of finding the shortest path using Cohort Intelligence algorithm for BMW management with the consideration of human risk.",
        "reference": [
            {
                "reference_title": "Using an integrated order picking-vehicle routing problem to study the impact of delivery time windows in e-commerce",
                "reference_link": "publication/329411017_Using_an_integrated_order_picking-vehicle_routing_problem_to_study_the_impact_of_delivery_time_windows_in_e-commerce",
                "reference_type": "Article",
                "reference_date": "Jun 2018",
                "reference_abstract": "European e-commerce sales are increasing every year. Nowadays, customers buy more frequently online in smallerquantities. Handling this large amount of customer orders puts the logistic activities of the supply chain underpressure. At the same time, customers have high expectations concerning the delivery of their online purchase. Inorder to meet these expectations at low cost, B2C e-commerce companies have to reconsider their logisticactivities. Instead of optimizing every single process of the supply chain, related problems need to be tackledsimultaneously. In the integrated order picking-vehicle routing problem(I-OP-VRP), picking lists and vehicle routesare determined simultaneously. One possibility to meet the high expectations of customers is to allow customers tochoose the time window in which they want to be delivered. However, the more customers select a time windowduring the purchasing process, the higher the total costs for the B2C e-commerce company since the company hasless flexibility to construct their delivery routes. To cover the cost increase, e-commerce companies often onlyprovide this service at an additional cost. The objective of this paper is to estimate the additional cost of allowingcustomers to choose a preferred delivery time window using the integrated order picking-vehicle routing problem.An experimental design is set up to investigate this service cost under varying circumstances depending oncustomer characteristics, time window characteristics and operator size. Based on the results of the ANOVA it canbe concluded that the investigated factors have a significant influence on the additional cost of allowing customersto select a delivery time window."
            },
            {
                "reference_title": "The Location-Routing Problem with Full Truckloads in Low-Carbon Supply Chain Network Designing",
                "reference_link": "publication/325018241_The_Location-Routing_Problem_with_Full_Truckloads_in_Low-Carbon_Supply_Chain_Network_Designing",
                "reference_type": "Article",
                "reference_date": "May 2018",
                "reference_abstract": "In recent years, low-carbon supply chain network design has been the focus of studies as the development of low-carbon economy. The location-routing problem with full truckloads (LRPFT) is investigated in this paper, which extends the existing studies on the LRP to full truckloads problem within the regional many-to-many raw material supply network. A mathematical model with dual objectives of minimizing total cost and environmental effects simultaneously is developed to determine the number and locations of facilities and optimize the flows among different kinds of nodes and routes of trucks as well. A novel multiobjective hybrid approach named NSGA-II-TS is proposed by combining a known multiobjective algorithm, NSGA-II, and a known heuristics, Tabu Search (TS). A chromosome presentation based on natural number and modified partially mapping crossover operator for the LRPFT are designed. Finally, the computational effectiveness of the hybrid approach is validated by the numerical results and a practical case study is applied to demonstrate the tradeoff between total cost and CO 2 emission in the LRPFT."
            },
            {
                "reference_title": "An ant colony optimization algorithm for solving the full truckload vehicle routing problem with profit",
                "reference_link": "publication/318125012_An_ant_colony_optimization_algorithm_for_solving_the_full_truckload_vehicle_routing_problem_with_profit",
                "reference_type": "Conference Paper",
                "reference_date": "Apr 2017",
                "reference_abstract": null
            },
            {
                "reference_title": "Multi-depot vehicle routing problem for hazardous materials transportation: A fuzzy bilevel programming",
                "reference_link": "publication/313415311_Multi-depot_vehicle_routing_problem_for_hazardous_materials_transportation_A_fuzzy_bilevel_programming",
                "reference_type": "Article",
                "reference_date": "Feb 2017",
                "reference_abstract": "Due to enormous demand and potential threat to public, hazardous materials transportation has been tremendously investigated in recent years. However, there are few studies focusing on multi-depot vehicle routing problem for hazardous materials transportation. In this study, we develop a fuzzy bilevel programming model for minimizing the total expected transportation risk when delivering products of hazardous materials to customers from multiple depots. The upper level formulation allocates customers to depots under the constraints of depot capacities and customer demands, while the lower level determines the optimal path for each group of depots and customers. For obtaining the optimal solution to the proposed model, four fuzzy simulation-based heuristic algorithms are designed. Illustrative examples demonstrate the effectiveness of the proposed model and the four heuristics."
            },
            {
                "reference_title": "The Probabilistic Pickup-and-Delivery Travelling Salesman Problem",
                "reference_link": "publication/329805425_The_Probabilistic_Pickup-and-Delivery_Travelling_Salesman_Problem",
                "reference_type": "Article",
                "reference_date": "Dec 2018",
                "reference_abstract": "Transportation problems are essential in commercial logistics and have been widely studied in the literature during the last decades. Many of them consist in designing routes for vehicles to move commodities between locations. This article approaches a pickup-and-delivery single-vehicle routing problem where there is susceptibility to uncertainty in customer requests. The probability distributions of the requests are assumed to be known, and the objective is to design an a priori route with minimum expected length. The problem has already been approached in the literature, but through a heuristic method. This article proposes the first exact approach to the problem. Two mathematical formulations are proposed: one is a compact model (i.e. defined by a polynomial number of variables and constraints); the other one contains an exponential number of inequalities and is solved within a branch-and-cut framework. Computational results show the upsides as well as the breakdowns of both formulations."
            },
            {
                "reference_title": "Optimal routing of complex transportation system of biomedical waste with multiple depot and disposal options",
                "reference_link": "publication/325985026_Optimal_routing_of_complex_transportation_system_of_biomedical_waste_with_multiple_depot_and_disposal_options",
                "reference_type": "Article",
                "reference_date": "Jan 2018",
                "reference_abstract": "Determination of safe and optimal routes for biomedical waste (BMW) collection and transportation using a fleet of vehicles is a complex issue. The issue gets significant, when the vehicles that outset from multiple depots, collects the BMW from hospitals scattered around a region and carry the waste to multiple disposal sites. The present paper deals with the development of a modified ACS-based approach to determine the optimal and safest route for BMW collection and transportation for such situation. The major objectives considered for route selection of vehicles, are the risk associated with the collection and transportation of BMW; total scheduling time of vehicles and number of vehicles. In this approach, clusters of the hospital nodes are constituted based on their distance from the nearest depot and late time window associated with the hospital node. Thereafter, the routes are scheduled and optimised using modified multi-objective ant colony system (MOACS). The computed results are validated abreast with benchmark solutions, demonstrating effectiveness of the proposed approach. Its applicability is elucidated using an illustrative example based on realistic data."
            },
            {
                "reference_title": "Heuristic algorithm for the Split-Demand One-Commodity Pickup-and-Delivery Travelling Salesman Problem",
                "reference_link": "publication/324768131_Heuristic_algorithm_for_the_Split-Demand_One-Commodity_Pickup-and-Delivery_Travelling_Salesman_Problem",
                "reference_type": "Article",
                "reference_date": "Apr 2018",
                "reference_abstract": "This article addresses the problem of designing routes of minimum cost for a capacitated vehicle moving a commodity between a set of customers, allowing two characteristics uncommon in the pickup-and-delivery literature. One characteristic is that a customer may be visited several times. The other characteristic is that a customer may be used as intermediate location to temporarily collect and deliver product. The article describes a matheuristic algorithm that iteratively applies a constructive procedure and a refinement procedure. The constructive procedure represents each customer by a set of nodes each one associated with a potential visit|, decomposes each customer demand into partial demands associated with its nodes, and solves a one-commodity pickup-and-delivery travelling salesman problem with a variable neighbourhood search. The refinement procedure is a branch-and-cut algorithm to optimize some pieces of a given solution. Exhaustive computational results on benchmark instances demonstrate the good performance of the algorithm when solving instances with up to 500 customers."
            },
            {
                "reference_title": "Robust vehicle routing problem with hard time windows under demand and travel time uncertainty",
                "reference_link": "publication/323312495_Robust_vehicle_routing_problem_with_hard_time_windows_under_demand_and_travel_time_uncertainty",
                "reference_type": "Article",
                "reference_date": "Feb 2018",
                "reference_abstract": "Due to an increase in customer-oriented service strategies designed to meet more complex and exacting customer requirements, meeting a scheduled time window has become an important part of designing vehicle routes for logistics activities. However, practically, the uncertainty in travel times and customer demand often means vehicles miss these time windows, increasing service costs and decreasing customer satisfaction. In an effort to find a solution that meets the needs of real-world logistics, we examine the vehicle routing problem with hard time windows under demand and travel time uncertainty. To address the problem, we build a robust optimization model based on novel route-dependent uncertainty sets. However, due to the complex nature of the problem, the robust model is only able to tackle small-sized instances using standard solvers. Therefore, to tackle large instances, we design a two-stage algorithm based on a modified adaptive variable neighborhood search heuristic. The first stage of the algorithm minimizes the total number of vehicle routes, while the second stage minimizes the total travel distance. Extensive computational experiments are conducted with modified versions of Solomon\u2019s benchmark instances. The numerical results show that the proposed two-stage algorithm is able to find optimal solutions for small-sized instances and good-quality robust solutions for large-sized instances with little increase to the total travel distance and/or the number of vehicles used. A detailed analysis of the results also reveals several managerial insights for decision-makers in the logistics industry."
            },
            {
                "reference_title": "Need of Biomedical Waste Management System in Hospitals \u2013 An Emerging issue \u2013 A Review",
                "reference_link": "publication/320713279_Need_of_Biomedical_Waste_Management_System_in_Hospitals_-_An_Emerging_issue_-_A_Review",
                "reference_type": "Article",
                "reference_date": "Sep 2012",
                "reference_abstract": null
            },
            {
                "reference_title": "Using metaheuristic algorithms to solve a multi-objective industrial hazardous waste location-routing problem considering incompatible waste types",
                "reference_link": "publication/319707351_Using_metaheuristic_algorithms_to_solve_a_multi-objective_industrial_hazardous_waste_location-routing_problem_considering_incompatible_waste_types",
                "reference_type": "Article",
                "reference_date": "Sep 2017",
                "reference_abstract": "Rapid progress in technology is a primary cause of acceleration in the rate of the industrial hazardous waste generation all over the world. Management of hazardous waste has magnetized researcher's attention because of its considerable impacts on the economy, ecology, and the environment. In this regard, this paper addresses a new industrial hazardous waste location-routing problem by putting emphasis on some new aspects in its formulation such as considering restriction about the incompatibility between some kinds of wastes and incorporating routing decisions into the model. Simultaneously minimization of three significant criteria, including total cost, total transportation risk of hazardous waste related to population exposure, and site risk persuades authors to implement two multi-objective evolutionary algorithms, Nondominated Sorting Genetic Algorithm (NSGA-II) and Multi-Objective Particle Swarm Optimization (MOPSO) for tackling the problem. The results obtained from experiments on several problem instances confirm the superiority of NSGA-II over MOPSO in terms of most of the evaluation metrics. Therefore, the significance of the paper is firstly the novelty of the model, and secondly, the comparison of two solution methods allows for the identification of the method resulting in the best results."
            }
        ]
    },
    {
        "title": "Toward Understanding the Conditions that Promote Higher Attention in Software Developments -A First Step on Music and Standups",
        "date": "December 2020",
        "doi": null,
        "conferance": "Conference: 46th Euromicro Conference on Software Engineering and Advanced Applications",
        "citations_count": null,
        "reference_count": "References (53)",
        "abstract": "Nowadays, Computer Science tightly entered all spheres of human activity. To improve quality and speed of development process, it is important to help programmers improve their working conditions. This paper proposes a vision on exploring this issue and presents in conjunction a factor that has been claimed multiple time to affect the effectiveness of software production, concentration and attention of software developers. We choose to focus on developers brain activity and features that can be extracted from it. CCS CONCEPTS \u2022 Software and its engineering \u2192 Collaboration in software development.",
        "reference": [
            {
                "reference_title": "The critical importance of meetings to leader and organizational success",
                "reference_link": "publication/319246903_The_critical_importance_of_meetings_to_leader_and_organizational_success",
                "reference_type": "Article",
                "reference_date": "Aug 2017",
                "reference_abstract": null
            },
            {
                "reference_title": "Attention Recognition in EEG-Based Affective Learning Research Using CFS+KNN Algorithm",
                "reference_link": "publication/309030502_Attention_Recognition_in_EEG-Based_Affective_Learning_Research_Using_CFSKNN_Algorithm",
                "reference_type": "Article",
                "reference_date": "Oct 2016",
                "reference_abstract": "The research detailed in this paper focuses on the processing of Electroencephalography (EEG) data to identify attention during the learning process. The identification of affect using our procedures is integrated into a simulated distance learning system that provides feedback to the user with respect to attention and concentration. The authors propose a classification procedure that combines correlation-based feature selection (CFS) and a k-nearest-neighbor (KNN) data mining algorithm. To evaluate the CFS+KNN algorithm, it was test against CFS+C4.5 algorithm and other classification algorithms. The classification performance was measured 10 times with different 3-fold cross validation data. The data was derived from 10 subjects while they were attempting to learn material in a simulated distance learning environment. A self-assessment model of self-report was used with a single valence to evaluate attention on 3 levels (high, neutral, low). It was found that CFS+KNN had a much better performance, giving the highest correct classification rate (CCR) of 80:84 3:0% for the valence dimension divided into 3 classes."
            },
            {
                "reference_title": "Effect of auditory stimulus in EEG signal using a Brain-Computer Interface",
                "reference_link": "publication/308823425_Effect_of_auditory_stimulus_in_EEG_signal_using_a_Brain-Computer_Interface",
                "reference_type": "Conference Paper",
                "reference_date": "Jun 2015",
                "reference_abstract": "The neuroscience research which related to the study of sounds that affected to the person and focused on create the system that observe the effects of binaural beat and instrumental relaxing music by analyzing the recorded Electroencephalogram (EEG) signal of the participants. In addition, the Brain-Computer Interface (BCI) is widely used for recording electrical brain wave signal readings and distinguishes the complexity of the EEG brain spectrum between the different EEG frequencies band including Delta, Theta, Alpha, Beta and Gamma. Moreover, there are 40 subjects was participated in this research. The statistic-based method Hypothesis Testing and Paired T-test were selected as a decision maker to analyze the EEG recorded from the participants using the Brain-Computer Interface to find the significant difference of each stimulus for every groups that separate by the stress level of participants which measured by using a Depression Anxiety and Stress Scales (DASS) questionnaire."
            },
            {
                "reference_title": "Cooperation, Collaboration and Pair-Programming: Field Studies on Backup Behavior",
                "reference_link": "publication/259992733_Cooperation_Collaboration_and_Pair-Programming_Field_Studies_on_Backup_Behavior",
                "reference_type": "Article",
                "reference_date": "May 2014",
                "reference_abstract": "Considering that pair programming has been extensively studied for more than a decade, it can seem quite surprising that there is such a lack of consensus on both its best use and its benefits. We argue that pair programming is not a replacement of usual developer interactions, but rather a formalization and enhancement of naturally occurring interactions. Consequently, we study and classify a broader range of developer interactions, evaluating them for type, purpose and patterns of occurrence, with the aim to identify situations in which pair programming is likely to be truly needed and thus most beneficial. We study the concrete pair programming practices in both academic and industrial settings. All interactions between teammates were recorded as Backup Behavior activities. In each of these two projects, developers were free to interact when needed. All team interactions were self-recorded by the teammates. The analysis of the interaction tokens show two salient features: solo work is an important component of teamwork and team interactions have two main purposes, namely cooperation and collaboration. Cooperative backup behavior occurs when a developer provides help to a teammate. Collaborative backup behavior occurs when the teammates are sharing the same goal toward solving an issue. We found that collaborative Backup Behavior, which occurred much less often, is close to the formal definition of pair programming. This study suggests that mandatory pair programming may be less efficient in organizations where solo work could be done and when some interactions are for cooperative activities. Based on these results, we discussed the potential implications concerning the best use of pair programming in practice, a more effective evaluation of its use, its potential benefits and emerging directions of future research."
            },
            {
                "reference_title": "A Method for Characterizing Energy Consumption in Android Smartphones",
                "reference_link": "publication/258432492_A_Method_for_Characterizing_Energy_Consumption_in_Android_Smartphones",
                "reference_type": "Conference Paper",
                "reference_date": "May 2013",
                "reference_abstract": "Cellular phones and tablets are ubiquitous, with a market penetration that is counted in millions of active users and units sold. The increasing computing capabilities and strict autonomy requirements on mobile devices drive a particular concern on energy utilization and optimization of this kind of equipment. In this paper, we investigate an approach to relate the energy consumption of smartphones with the operational status of the device, surveying parameters exposed by the operating system using an Android application. Our goal is to explore the means to expand the information that may help to produce more reliable measurements that can be used in further research for designing energy optimization profiles for mobile devices and identify optimization needs."
            },
            {
                "reference_title": "Knowledge transfer in system modeling and its realization through an optimal allocation of information granularity",
                "reference_link": "publication/257635400_Knowledge_transfer_in_system_modeling_and_its_realization_through_an_optimal_allocation_of_information_granularity",
                "reference_type": "Article",
                "reference_date": "Aug 2012",
                "reference_abstract": "In this study, we introduce and discuss a concept of knowledge transfer in system modeling. In a nutshell, knowledge transfer is about forming ways on how a source of knowledge (namely, an existing model) can be used in presence of new, very limited experimental evidence. In virtue of the nature of the problem at hand (a situation encountered quite commonly, e.g. in project cost estimation), new data could be very limited and this scarcity of data makes it insufficient to construct a new model. At the same time, the new data originate from a similar (but not the same) phenomenon (process) for which the original model has been constructed so the existing model, even though it could applied, has to be treated with a certain level of reservation. Such situations can be encountered, e.g. in software engineering where in spite existing similarities, each project, process, or product exhibits its own unique characteristics. Taking this into consideration, the existing model is generalized (abstracted) by forming its granular counterpart \u2013 granular model where its parameters are regarded as information granules rather than numeric entities, viz. their non-numeric (granular) version is formed based on the values of the numeric parameters present in the original model. The results produced by the granular model are also granular and in this manner they become reflective of the differences existing between the current phenomenon and the process for which the previous model has been formed."
            },
            {
                "reference_title": "Frontal EEG theta/beta ratio during mind wandering episodes",
                "reference_link": "publication/329021782_Frontal_EEG_thetabeta_ratio_during_mind_wandering_episodes",
                "reference_type": "Article",
                "reference_date": "Nov 2018",
                "reference_abstract": "Background: \nIn resting-state EEG, the ratio between frontal power in the slow theta frequency band and the fast beta frequency band (the theta/beta ratio, TBR) has previously been negatively related to attentional control. Also, increased theta and reduced beta power were observed during mind wandering (MW) compared to episodes of focused attention. Thus, increased resting-state frontal TBR could be related to MW, suggesting that previously observed relationships between TBR and attentional control could reflect MW episodes increasing the average resting state TBR in people with low attentional control.\n\nGoals: \nTo replicate and extend the previous theta and beta MW effects for frontal TBR recordings and test if MW related changes in frontal TBR are related to attentional control.\n\nMethod: \nTwenty-six healthy participants performed a 40-minute breath-counting task, after a baseline EEG recording, while EEG was measured and participants indicated MW episodes with button presses.\n\nResults: \nFrontal TBR was significantly higher during MW episodes than during on-task periods. However, no relation between frontal TBR and attentional control was found.\n\nConclusions: \nThis confirms that frontal TBR varies with MW, which is thought to reflect, among other things, a state of reduced top-down attentional control over thoughts. 194 words."
            },
            {
                "reference_title": "Competitive Dynamics Research: Critique and Future Directions",
                "reference_link": "publication/324525534_Competitive_Dynamics_Research_Critique_and_Future_Directions",
                "reference_type": "Chapter",
                "reference_date": "Feb 2008",
                "reference_abstract": "A series of actions (moves) and reactions (countermoves) among firms in an industry create competitive dynamics. These action/reaction dynamics reflect the normal and innovative movement of firms in pursuit of profits. Firms act creatively (introduce a new product, a new promotion, or a new marketing agreement! to enhance or improve profits, competitive advantage, and industry position; successful actions (actions which generate new customers and profits) promote competitive reaction as rivals attempt to block or imitate the action. The study of competitive dynamics is thus the study of how firm action (moves) affects competitors, competitive advantage, and performance. Sometimes these actions and reactions can escalate among firms so that the industry performance is adversely affected; at other times, the pattern of behavior can be more gentlemanly and profitable."
            },
            {
                "reference_title": "Cognitive task difficulty analysis using EEG and data mining",
                "reference_link": "publication/320654131_Cognitive_task_difficulty_analysis_using_EEG_and_data_mining",
                "reference_type": "Conference Paper",
                "reference_date": "Mar 2017",
                "reference_abstract": null
            },
            {
                "reference_title": "Lean Software Development in Action",
                "reference_link": "publication/287338519_Lean_Software_Development_in_Action",
                "reference_type": "Book",
                "reference_date": "Jun 2014",
                "reference_abstract": "This book illustrates how goal-oriented, automated measurement can be used to create Lean organizations and to facilitate the development of Lean software, while also demonstrating the practical implementation of Lean software development by combining tried and trusted tools. In order to be successful, a Lean orientation of software development has to go hand in hand with a company's overall business strategy. To achieve this, two interrelated aspects require special attention: measurement and experience management. In this book, Janes and Succi provide the necessary knowledge to establish \"Lean software company thinking,\" while also exploiting the latest approaches to software measurement. A comprehensive, company-wide measurement approach is exactly what companies need in order to align their activities to the demands of their stakeholders, to their business strategy, etc. With the automatic, non-invasive measurement approach proposed in this book, even small and medium-sized enterprises that do not have the resources to introduce heavyweight processes will be able to make their software development processes considerably more Lean. The book is divided into three parts. Part I, \"Motivation for Lean Software Development,\" explains just what \"Lean Production\" means, why it can be advantageous to apply Lean concepts to software engineering, and which existing approaches are best suited to achieving this. Part II, \"The Pillars of Lean Software Development,\" presents the tools needed to achieve Lean software development: Non-invasive Measurement, the Goal Question Metric approach, and the Experience Factory. Finally, Part III, \"Lean Software Development in Action,\" shows how different tools can be combined to enable Lean Thinking in software development. The book primarily addresses the needs of all those working in the field of software engineering who want to understand how to establish an efficient and effective software development process. This group includes developers, managers, and students pursuing an M.Sc. degree in software engineering. \u00a9 Springer-Verlag Berlin Heidelberg 2014. All rights are reserved."
            }
        ]
    },
    {
        "title": "Middle School Teachers' Self-efficacy in Teaching Computer Science and Digital Literacy",
        "date": "December 2020",
        "doi": "10.1145/3408877.3439592",
        "conferance": "Conference: SIGCSE ' 20",
        "citations_count": null,
        "reference_count": "References (3)",
        "abstract": "This pilot study explores the impact of the CS Pathways professional development (PD) program on the teachers' self-efficacy in teaching a middle school computer science and digital literacy (CSDL) curriculum. The main goal of the study is to investigate the attributes that describe the teachers' self-efficacy after their first-year participation in the PD. A total of 19 middle school teachers from two states, NY and MA, attended the CS Pathway PD program and completed the end-of-year survey pertaining to self-efficacy in CSDL; more than half accepted the interview to help further understand their perceptions (n=10). Principal Component Analysis (PCA) is applied to study the attributes of the teachers' self-efficacy. The preliminary results capture teachers' self-efficacy patterns, which inform the PD and indicate its effectiveness and challenges.",
        "reference": [
            {
                "reference_title": "Teacher Stress and Teacher Self-Efficacy as Predictors of Engagement, Emotional Exhaustion, and Motivation to Leave the Teaching Profession",
                "reference_link": "publication/306310055_Teacher_Stress_and_Teacher_Self-Efficacy_as_Predictors_of_Engagement_Emotional_Exhaustion_and_Motivation_to_Leave_the_Teaching_Profession",
                "reference_type": "Article",
                "reference_date": "Jan 2016",
                "reference_abstract": "The purpose of this study was to explore how seven potentially stressful school context variables (potential stressors) predicted senior high school teachers\u2019 experiences of teacher self-efficacy, emotional stress, emotional exhaustion, engagement in teaching, and motivation to leave the teaching profession. A total of 523 Norwegian teachers in senior high school participated in the study. Four of the potential stressors were significantly but differently related to self-efficacy and emotional stress and indirectly to emotional exhaustion, engagement, and motivation to leave the profession. The study shows that different potential stressors predict emotional exhaustion, engagement, and motivation through different psychological processes. SEM-analysis indicated two main routes to teachers\u2019 motivation to leave the profession: 1) one route from time pressure via emotional stress and exhaustion to motivation to quit and 2) another route from lack of supervisory support and trust, low student motivation and value conflicts via lower self-efficacy and lower engagement to motivation to quit."
            },
            {
                "reference_title": "Computer science teacher professional development in the United States: a review of studies published between 2004 and 2014",
                "reference_link": "publication/286636769_Computer_science_teacher_professional_development_in_the_United_States_a_review_of_studies_published_between_2004_and_2014",
                "reference_type": "Article",
                "reference_date": "Dec 2015",
                "reference_abstract": "While there has been a remarkable interest to make computer science a core K-12 academic subject in the United States, there is a shortage of K-12 computer science teachers to successfully implement computer sciences courses in schools. In order to enhance computer science teacher capacity, training programs have been offered through teacher professional development. In this study, the main goal was to systematically review the studies regarding computer science professional development to understand the scope, context, and effectiveness of these programs in the past decade (2004\u20132014). Based on 21 journal articles and conference proceedings, this study explored: (1) Type of professional development organization and source of funding, (2) professional development structure and participants, (3) goal of professional development and type of evaluation used, (4) specific computer science concepts and training tools used, (5) and their effectiveness to improve teacher practice and student learning."
            },
            {
                "reference_title": "Improving Impact Studies of Teachers' Professional Development: Toward Better Conceptualizations and Measures",
                "reference_link": "publication/242224358_Improving_Impact_Studies_of_Teachers'_Professional_Development_Toward_Better_Conceptualizations_and_Measures",
                "reference_type": "Article",
                "reference_date": "Apr 2009",
                "reference_abstract": "The author suggests that we apply recent research knowledge to improve our conceptualization, measures, and methodology for studying the effects of teachers' professional development on teachers and students. She makes the case that there is a research consensus to support the use of a set of core features and a common conceptual framework in professional development impact studies. She urges us to move away from automatic biases either for or against observation, interviews, or surveys in such studies. She argues that the use of a common conceptual framework would elevate the quality of professional development studies and subsequently the general understanding of how best to shape and implement teacher learning opportunities for the maximum benefit of both teachers and students."
            }
        ]
    },
    {
        "title": "Twitter et la linguistique situ\u00e9e. R\u00e9flexions m\u00e9thodologiques \u00e0 partir de l\u2019exemple de tweets sur la m\u00e9tropole de Dijon",
        "date": "December 2020",
        "doi": null,
        "conferance": "Conference: DATA SHS 2020",
        "citations_count": null,
        "reference_count": null,
        "abstract": "Cette communication s\u2019inscrit dans le cadre du projet de recherche interdisciplinaire POPSU visant \u00e0 entre autres \u00e0 analyser l\u2019identit\u00e9 m\u00e9tropolitaine et identifier les int\u00e9r\u00eats des citoyens. Pour alimenter la discussion propos\u00e9e, nous exploiterons un corpus original de 300 000 tweets collect\u00e9s en temps r\u00e9el depuis juillet 2019 dans le cadre du projet interdisciplinaire POPSU Dijon, h\u00e9berg\u00e9 \u00e0 la MSH Dijon et dont le but est d\u2019explorer les interactions des utilisateurs de Twitter lorsqu\u2019ils parlent de Dijon et de la m\u00e9tropole. Nous commencerons par discuter les aspects li\u00e9s \u00e0 la collecte de donn\u00e9es Twitter :\n- juridiques : protections des donn\u00e9es personnelles, Open Data ;\n- relatifs \u00e0 la constitution du jeu de donn\u00e9es : o\u00f9 placer le curseur entre l\u2019exhaustivit\u00e9 des donn\u00e9es recueillies et le risque de recueillir des donn\u00e9es trop \u00e9loign\u00e9es de l\u2019objet d\u2019\u00e9tude (au niveau temporel, g\u00e9ographique ou th\u00e9matique) ;\n- techniques : li\u00e9s \u00e0 l\u2019API gratuite de Twitter (diff\u00e9rentes m\u00e9thodes de collectes et leur exhaustivit\u00e9) ;\n- relatifs aux jeux de donn\u00e9es dont les traitements ne n\u00e9cessitent pas d\u2019infrastructure Big Data.\nLes questionnements informatiques sont consubstantiels aux interrogations linguistiques qui ont \u00e9maill\u00e9 l\u2019ensemble de ce projet \u00e0 l\u2019interface entre informatique, linguistique et g\u00e9ographie. Les donn\u00e9es collect\u00e9es nous ont amen\u00e9s \u00e0 \u00e9valuer leur int\u00e9grit\u00e9 et leur mise en corpus ; cette phase de r\u00e9flexion est fondamentale dans le cadre de l\u2019analyse de discours, qui \u00e9tait le but initial du projet de recherche. Or, si l\u2019int\u00e9grit\u00e9 du corpus n\u2019est pas garantie, une analyse de discours\nne peut \u00eatre envisag\u00e9e en tant que telle (cf. la d\u00e9finition s\u00e9minale de Busse/Teubert 1994).\nCette mise en d\u00e9faut n\u2019est toutefois pas r\u00e9dhibitoire dans la mesure o\u00f9 les donn\u00e9es Twitter peuvent alimenter d\u2019autres r\u00e9flexions linguistiques et amener \u00e0 consid\u00e9rer d\u2019autres n\u0153uds entre mat\u00e9rialisations linguistiques et structures \u00e9pist\u00e9miques. En tant que source de donn\u00e9es hypersynchroniques, Twitter permet une analyse de ph\u00e9nom\u00e8nes linguistiques contemporains et authentiques (cf. la r\u00e9flexion men\u00e9e dans Bach 2020) ; toutefois, Twitter n\u2019est pas un instrument magique pour la linguistique situ\u00e9e, et un certain nombre de desiderate sont \u00e0 formuler en ce sens pour une d\u00e9marche scientifique rigoureuse (c\u2019est-\u00e0-dire falsifiable et en accord avec les innovations issues des autres champs de la cognition). Ce projet a en effet mis en \u00e9vidence deux types de limitations : techniques et \u00e9thodologiques. Une grande partie de ces limitations ont \u00e9t\u00e9 contourn\u00e9es, et il nous est possible d\u2019ajuster la m\u00e9thodologie pour les futurs projets. Notre exp\u00e9rience montre que, sur le long terme, une collaboration \u00e9troite et r\u00e9guli\u00e8re avec une implication continue et forte de chacun des acteurs est n\u00e9cessaire.",
        "reference": []
    },
    {
        "title": "Volumen 16 (2020)",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (11)",
        "abstract": "In the work, a system of didactic procedures is proposed to perfect the investigative skills of Computer Science students. It was prepared as part of a doctoral thesis, using the Functional Structural Systemic Method. It has been applied at the Universidad de Oriente (UO) for five courses, with results that have confirmed that the system provides sufficient evidence about its possibilities of improving the investigative skills of the aforementioned students, by contributing to the development of computational investigative thinking in them.\nEn el trabajo se propone un sistema de procedimientos did\u00e1cticos para perfeccionar las habilidades investigativas de los estudiantes de Ciencia de la Computaci\u00f3n. El mismo fue elaborado como parte de una tesis doctoral, empleando el M\u00e9todo Sist\u00e9mico Estructural Funcional. Ha sido aplicado en la Universidad de Oriente (UO) por cinco cursos, con resultados que han confirmado que el sistema brinda suficientes evidencias sobre sus posibilidades de perfeccionar las habilidades investigativas de los citados estudiantes, al contribuir al desarrollo de un pensamiento investigativo computacional en estos.",
        "reference": [
            {
                "reference_title": "Perfeccionando los procedimientos did\u00e1cticos para la formaci\u00f3n investigativa de estudiantes de Ciencia de la Computaci\u00f3n/Improving didactic procedures for research training of Computer Science students",
                "reference_link": "publication/327872016_Perfeccionando_los_procedimientos_didacticos_para_la_formacion_investigativa_de_estudiantes_de_Ciencia_de_la_ComputacionImproving_didactic_procedures_for_research_training_of_Computer_Science_students",
                "reference_type": "Article",
                "reference_date": "Sep 2018",
                "reference_abstract": "La aplicaci\u00f3n de las ciencias computacionales constituye una de las prioridades para el desarrollo de cualquier sociedad, es por ello que la formaci\u00f3n investigativa de los estudiantes de estas carreras necesita un perfeccionamiento continuo. Este trabajo tiene como objetivo la corroboraci\u00f3n de la pertinencia y validez de un sistema de procedimientos did\u00e1cticos para perfeccionar dicha formaci\u00f3n. Este sistema fue aplicado mediante un cuasiexperimento pedag\u00f3gico en la carrera Ciencia de la Computaci\u00f3n de la Universidad de Oriente, Cuba. Pudo concluirse en el contexto estudiado que el sistema de procedimientos did\u00e1cticos incrementa significativamente la aprehensi\u00f3n del pensamiento investigativo computacional en los estudiantes.\nThe application of Computational Sciences is one priority for the development of any society, for this reason the research training of the students of these courses needs continuous improvement. This work aims to corroborate the relevance and validity of a system of didactic procedures to perfect this training. This system was applied through a quasi-pedagogic experiment in Computer Science career at the Oriente University, Cuba. Was concluded in the context studied that system of didactic procedures increase significantly the apprehension of computational research thinking in students."
            },
            {
                "reference_title": "Estudio Exploratorio sobre la Formaci\u00f3n Investigativa de los Estudiantes de Licenciatura en Ciencia de la Computaci\u00f3n",
                "reference_link": "publication/312892538_Estudio_Exploratorio_sobre_la_Formacion_Investigativa_de_los_Estudiantes_de_Licenciatura_en_Ciencia_de_la_Computacion",
                "reference_type": "Article",
                "reference_date": "Jan 2014",
                "reference_abstract": "de Cuba, Cuba. CP 90900 RESUMEN El art\u00edculo analiza los resultados obtenidos en un estudio diagn\u00f3stico sobre la formaci\u00f3n investigativa de los estudiantes de la carrera de Licenciatura en Ciencia de la Computaci\u00f3n, de la Universidad de Oriente. La investigaci\u00f3n, de car\u00e1cter exploratorio, utiliz\u00f3 m\u00e9todos cuantitativos y cualitativos para determinar las principales dificultades relacionadas con el comportamiento de dicha formaci\u00f3n. Los resultados indican que existen insuficiencias que tienen su base en la forma deficiente en que se desarrolla la din\u00e1mica del proceso formativo en asignaturas de la carrera, revelando oportunidades para una intervenci\u00f3n pedag\u00f3gica en dicha din\u00e1mica. PALABRAS CLAVE: diagn\u00f3stico, ciencia de la computaci\u00f3n, formaci\u00f3n investigativa. ABSTRACT The article analyzes the results obtained in a diagnosis study on the investigative formation of the students of the career of Licentiate in Computation Science, in Oriente University. The investigation, of exploratory character, combines quantitative and qualitative methods, to determine the main difficulties related with the behavior of this formation. The obtained results indicate that there are inadequacies based in the faulty form in that the dynamics of the formative process is developed in subjects of the career, that facilitate opportunities for a pedagogic intervention in this dynamics. KEY WORDS: diagnosis, computation science, investigative formation. INTRODUCCI\u00d3N Las nuevas condiciones tecnol\u00f3gicas de la \u00absociedad informacional\u00bb imprimen una nueva din\u00e1mica a la organizaci\u00f3n social, la que se inspira en la transformaci\u00f3n de la informaci\u00f3n en conocimiento y de este \u00faltimo en innovaci\u00f3n, como condici\u00f3n indispensable para transformar la realidad. Bajo estas condiciones cobra especial relevancia la Ciencia de la Computaci\u00f3n, que abarca el estudio de las bases te\u00f3ricas de la informaci\u00f3n y la computaci\u00f3n, encaminada a la soluci\u00f3n de dis\u00edmiles problemas informativo-computacionales que emergen del quehacer social, cient\u00edfico y t\u00e9cnico. Esta ciencia permite automatizar sistemas en todas las esferas de actuaci\u00f3n de la sociedad e intervenir de manera activa y pro-activa en importantes sistemas computacionales (Lissabet, 2011). A nivel internacional la Ciencia de la Computaci\u00f3n se reconoce como una ciencia independiente, en pleno desarrollo y con \u00e1reas de problemas propios, la que tiene un amplio rango de aplicabilidad a la soluci\u00f3n de problemas de car\u00e1cter complejo e interdisciplinario en diversos dominios de actuaci\u00f3n (Denning, 2000; Baeza, 2001; Arana, 2005). La relevancia actual de esta ciencia ha llevado a que exista la necesidad de formar en la misma profesionales competentes, surgiendo as\u00ed la carrera de Ciencia de la Computaci\u00f3n en numerosos pa\u00edses, con la aspiraci\u00f3n de que estos profesionales sean capaces de resolver problemas \u00abinformativo-computacionales\u00bb que surgen"
            },
            {
                "reference_title": "CONSIDERACIONES EPISTEMOL\u00d3GICAS SOBRE LA FORMACI\u00d3N INVESTIGATIVA DEL LICENCIADO EN CIENCIA DE LA COMPUTACI\u00d3N",
                "reference_link": "publication/312891180_CONSIDERACIONES_EPISTEMOLOGICAS_SOBRE_LA_FORMACION_INVESTIGATIVA_DEL_LICENCIADO_EN_CIENCIA_DE_LA_COMPUTACION",
                "reference_type": "Article",
                "reference_date": "Jan 2015",
                "reference_abstract": "Fecha de recepci\u00f3n: 12-05-2015 Fecha de aceptaci\u00f3n: 20-07-2015 RESUMEN La gran importancia que han cobrado en la actualidad las investigaciones en Ciencia de la Computaci\u00f3n, junto a la complejidad y dificultades detectadas en su ense\u00f1anza, han contribuido a interesar a didactas e investigadores en el estudio de los procesos formativos que al respecto se llevan a cabo en esta ciencia. Consecuentemente, el presente trabajo tiene como objetivo develar, desde el punto de vista did\u00e1ctico, las esencialidades del proceso de formaci\u00f3n investigativa en la carrera de Licenciatura en Ciencia de la Computaci\u00f3n, tomando en cuenta la contradicci\u00f3n dial\u00e9ctica que se manifiesta entre la sistematizaci\u00f3n de la informaci\u00f3n proveniente de la situaci\u00f3n probl\u00e9mica y la validaci\u00f3n del correspondiente sistema de informaci\u00f3n computacional, la que debe dinamizar la l\u00f3gica investigativa en la carrera, favoreciendo el desempe\u00f1o profesional. De aqu\u00ed que se fundamente la necesidad de crear una nueva propuesta did\u00e1ctica para desarrollar la din\u00e1mica del proceso de formaci\u00f3n investigativa, que considere la citada contradicci\u00f3n e involucre en su soluci\u00f3n a todas las asignaturas"
            },
            {
                "reference_title": "FORMACI\u00d3N DE HABILIDADES PARA LA INVESTIGACI\u00d3N DESDE EL PREGRADO",
                "reference_link": "publication/262460589_FORMACION_DE_HABILIDADES_PARA_LA_INVESTIGACION_DESDE_EL_PREGRADO",
                "reference_type": "Article",
                "reference_date": "Dec 2007",
                "reference_abstract": null
            },
            {
                "reference_title": "SISTEMA DE PROCEDIMIENTOS DID\u00c1CTICOS PARA LA FORMACI\u00d3N INVESTIGATIVA EN CIENCIA DE LA COMPUTACI\u00d3N",
                "reference_link": "publication/320430770_SISTEMA_DE_PROCEDIMIENTOS_DIDACTICOS_PARA_LA_FORMACION_INVESTIGATIVA_EN_CIENCIA_DE_LA_COMPUTACION",
                "reference_type": "Thesis",
                "reference_date": "Dec 2016",
                "reference_abstract": "The investigation starts from the scientific problem: insufficient understanding of the problematic situations, which are manifested in the limited analysis and designs of the investigations, not favoring the future professional performance of the students of Bachelor in Computer Science; being considered as research object, the process of research training in the degree course in Computer Science, and as a field of action, the dynamics of this process. The objective was to develop a system of didactic procedures, based on a model of the dynamics of the process of research training in the degree course in Computer Science, which will favor the future performance of the professionals of the mentioned race. The main contributions were the aforementioned model and the system of didactic procedures. The relevance and scientific feasibility of the results were satisfactorily evaluated through two socialization workshops with specialists from the Universidad de Oriente and the Central University of Las Villas, the partial application of the didactic procedures system in the third year of the course and the exemplification of said system in the subject Software Engineering. The scientific novelty consisted in having based a didactic logic for the formative dynamics of the investigation in Computer Science, which is constituted in a necessary condition for the development of a computational research thinking, when establishing the relevance of the investigative methods, in its passing through the systems: user, intermediary and computational information, as well as explaining the transit through the interpretive, translational and evaluative levels of digital hermeneutics."
            },
            {
                "reference_title": "LA CULTURA INFORM\u00c1TICA DEL PROFESOR DE COMPUTACI\u00d3N EN CUBA",
                "reference_link": "publication/227431202_LA_CULTURA_INFORMATICA_DEL_PROFESOR_DE_COMPUTACION_EN_CUBA",
                "reference_type": "Article",
                "reference_date": null,
                "reference_abstract": "En el art\u00edculo se exponen los criterios del autor relacionados con la necesidad de formar en los docentes de computaci\u00f3n una cultura inform\u00e1tica como parte de su Cultura General Integral. A Partir de las deficiencias existentes en el proceso formativo de dicho profesor y en su desempe\u00f1o profesional as\u00ed como de las carencias te\u00f3ricas en las concepciones sobre cultura inform\u00e1tica a nivel nacional y en el extranjero, se propone una concepci\u00f3n de dicha cultura que integre cinco componentes: informacional, tecnol\u00f3gico, sociocultural, humano y pedag\u00f3gico. Sobre esta base, concibe a su proceso de formaci\u00f3n desde un enfoque integrador con orientaci\u00f3n cultural, que tenga en cuenta la estructura de la cultura inform\u00e1tica, y que incorpore el uso del aprendizaje por proyectos con uso de las TIC."
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2012",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2016",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2005",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2014",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Researcher Profile: Przemys\u0142aw Prusinkiewicz",
        "date": "December 2020",
        "doi": "10.1093/insilicoplants/diaa014",
        "conferance": null,
        "citations_count": null,
        "reference_count": null,
        "abstract": "Przemys\u0142aw (Przemek) Prusinkiewicz is a Professor of Computer Science at the University of Calgary, Canada, where he creates models, simulations and visualizations of plant development. He received his MSc and PhD at the Technical University of Warsaw where he studied Computer Science and Engineering under Prof. Stanislaw Budkowski.",
        "reference": []
    },
    {
        "title": "Evaluating an Educational Escape Room Conducted Remotely for Teaching Software Engineering",
        "date": "December 2020",
        "doi": "10.1109/ACCESS.2020.3044380",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (52)",
        "abstract": "With the rise of distance learning, new challenges have emerged for educators. Among these challenges, developing effective and motivating group activities for students in the remote classroom is one of the top priorities to be addressed. According to existing literature, educational escape rooms have proven to be engaging and effective learning activities when conducted face-to-face. However, no prior research has analyzed the instructional effectiveness of these activities when they are conducted remotely. Furthermore, none of the educational escape rooms reported in the literature has been designed for teaching software modeling. This article analyzes an educational escape room conducted remotely in a software engineering fundamentals course for teaching software modeling. A total of three evaluation instruments were used: a pre-test and a post-test to measure students\u2019 learning gains, a questionnaire to collect students\u2019 perceptions, and a web platform for automatically gathering data on students\u2019 interactions. The contribution of this article is two-fold. On the one hand, it provides, for the first time, evidence that remote educational escape rooms can be effective learning activities. On the other hand, it provides, also for the first time, proof that educational escape rooms are effective and engaging activities for teaching software modeling.",
        "reference": [
            {
                "reference_title": "New approaches to adapt escape game activities to large audience in Chemical Engineering: numeric supports and students\u2019 participation",
                "reference_link": "publication/341802111_New_approaches_to_adapt_escape_game_activities_to_large_audience_in_Chemical_Engineering_numeric_supports_and_students'_participation",
                "reference_type": "Article",
                "reference_date": "Jun 2020",
                "reference_abstract": "Gamification is a widespread phenomenon that relies on using game mechanics in other areas, such as the learning situation. One of the most exciting types of games in the late 2010s is escape games, where the principle is for the players to manage to escape from a room in which they are locked by finding hidden items and following a sequence of puzzles. The goal of using this type of game is to motivate/involve learners, to make them work and develop adaptability and responsiveness skills. Unfortunately, these escape games are only practiced in small groups, and the design is expensive and time-consuming. That is why cost-effective alternatives are proposed in this paper. They are either dematerialized, entirely based on a digital medium (smartphone/tablets/computer), or directly created by students, also with a digital medium, allowing integration into large classes, or at open house events."
            },
            {
                "reference_title": "Unlocking Student Engagement: Creation, Adaptation, and Application of an Educational Escape Room Across Three Pharmacy Campuses",
                "reference_link": "publication/338489610_Unlocking_Student_Engagement_Creation_Adaptation_and_Application_of_an_Educational_Escape_Room_Across_Three_Pharmacy_Campuses",
                "reference_type": "Article",
                "reference_date": "Jan 2020",
                "reference_abstract": "Background. Educational escape rooms are positively received by students, increase knowledge, and serve as a platform for the active application of teamwork and team-based communication.\nAim. This article focuses on detailing an educational escape room that is adaptable and transferable for use with any course or discipline.\nMethods. Puzzles are created around the educational objectives of the course or unit. Puzzles include ciphers, jumbles, coded messages, combination locks, rebuses, and data hunts. Students work in teams to solve content-specific puzzles to escape a room. Teams which solve all of the puzzles in the allotted time are considered to have successfully escaped the room. Gameplay can range from 60 to 75 minutes. Facultyled debriefing is an important part of the educational innovation.\nResults and Conclusion. This escape room uses collaborative learning to increase student knowledge and skills in educational content. The learning experience is enhanced through dynamic student engagement with the focused topic. This topic can easily be changed to a different course topic and the corresponding gameplay puzzles adapted and transferred for use with a variety of disciplines. This manuscript details the transferability of the educational escape room to 3 campuses and provides insight for successful implementation."
            },
            {
                "reference_title": "Analyzing Learning Effectiveness and Students' Perceptions of an Educational Escape Room in a Programming Course in Higher Education",
                "reference_link": "publication/338019541_Analyzing_Learning_Effectiveness_and_Students'_Perceptions_of_an_Educational_Escape_Room_in_a_Programming_Course_in_Higher_Education",
                "reference_type": "Article",
                "reference_date": "Dec 2019",
                "reference_abstract": "In recent years, educational escape rooms have started to gain momentum in the academic community. Prior research has reported on the use of educational escape rooms in several fields. However, earlier works have failed to assess the impact of this sort of activities for teaching programming in terms of learning effectiveness. This work fills the existing gap in the literature by examining an educational escape room for teaching programming in a higher education setting by means of three different instruments: (1) a pre-test and a post-test for measuring learning gains, (2) a survey for assessing students' perceptions, and (3) a web platform for recording student interaction data during the activity. The results of this work provide, for the first time, empirical evidence that educational escape rooms are an effective and engaging way of teaching programming."
            },
            {
                "reference_title": "the Quest: An Escape Room Inspired Interactive Museum Exhibition",
                "reference_link": "publication/336732456_the_Quest_An_Escape_Room_Inspired_Interactive_Museum_Exhibition",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2019",
                "reference_abstract": "In this project, we report on designing an interactive museum exhibit in a technology museum, inspired by escape room game mechanics and technology. The project aims to create a deeper more immersed engagement with and interest in the exhibition, and thereby increase the interest in the exhibit's subject. In the game, the players take on the role of grandchildren to a known (fictitious) turn-of-the-century explorer and set out to find the treasures she hid around the world during her years of adventure. Clues to the treasures are hidden within the museum exhibition and by using knowledge found around the exhibition the players can solve the riddles and find the treasure, while also picking up some knowledge along the way."
            },
            {
                "reference_title": "An escape-room inspired game for genetics review",
                "reference_link": "publication/345380051_An_escape-room_inspired_game_for_genetics_review",
                "reference_type": "Article",
                "reference_date": "Dec 2019",
                "reference_abstract": "Active learning strategies engage students in instruction by making them an integral part of the teaching process. Gamification is one type of active learning approach that incentivises student participation by incorporating gaming elements into the learning experience. Although gamification might be an effective strategy for the introduction of new material, adding gaming elements to a review activity can motivate students to study, help them organise material, and identify material that they should review further before an exam. Escape rooms are collaborative problem-solving challenges and aspects of escape-room experiences can be applied to active learning. Here, we describe an escape room inspired review game for an advanced genetic analysis class at Northwestern University. We provide a detailed account of each puzzle within the review game and assess the impact of the game on learning outcomes."
            },
            {
                "reference_title": "Physical and Digital Educational Escape Room for Teaching Chemical Bonding",
                "reference_link": "publication/343303955_Physical_and_Digital_Educational_Escape_Room_for_Teaching_Chemical_Bonding",
                "reference_type": "Article",
                "reference_date": "Jul 2020",
                "reference_abstract": null
            },
            {
                "reference_title": "Escape the (Remote) Classroom: An Online Escape Room for Remote Learning",
                "reference_link": "publication/343201558_Escape_the_Remote_Classroom_An_Online_Escape_Room_for_Remote_Learning",
                "reference_type": "Article",
                "reference_date": "Jul 2020",
                "reference_abstract": null
            },
            {
                "reference_title": "Applying Digital Escape Rooms Infused with Science Teaching in Elementary School: Learning Performance, Learning Motivation, and Problem-Solving Ability",
                "reference_link": "publication/342756475_Applying_Digital_Escape_Rooms_Infused_with_Science_Teaching_in_Elementary_School_Learning_Performance_Learning_Motivation_and_Problem-Solving_Ability",
                "reference_type": "Article",
                "reference_date": "Jul 2020",
                "reference_abstract": "In this study, a teaching approach involving a digital escape room (DER) was introduced into science teaching for fourth-graders in elementary school to investigate the effect of this method on students\u2019 learning performance, learning motivation, and problem-solving ability. Based on a quasi-experimental approach, four research tools were applied with 40 students: Science Learning Performance, the Learning Motivation Scale (LMS), the Test of Problem Solving (TPS), and a feedback form. The students were split into an experimental and comparison group, with 20 students in each group. The experimental group received the experimental teaching for a 10-week period, during which seven DERs were infused into the science teaching activities, while the comparison group\u2019s classes used direct teaching methods. In each class infused with DER activity, the students used tablets to receive and complete tasks by using props, riddles, clues, crossword games, and puzzle challenges within a 25-min window according to the teacher\u2019s instructions. The data were analyzed with ANCOVA. The results showed that the students in the experimental group had higher learning motivation and problem-solving ability scores than those in the comparison group. However, the two groups had the same learning performance levels in science class. In general, the students had positive perceptions of the DER experience, and they believed the DER teaching strategy was compelling and effective. Finally, we have offered suggestions based on this research study\u2019s results with the hope of providing references for teaching practice and future research."
            },
            {
                "reference_title": "Application of escape lab-room to heat transfer evaluation for Chemical Engineers",
                "reference_link": "publication/342621177_Application_of_escape_lab-room_to_heat_transfer_evaluation_for_Chemical_Engineers",
                "reference_type": "Article",
                "reference_date": "Jul 2020",
                "reference_abstract": "Heat transfer is an elementary discipline across to many engineering degrees. As other basic engineering subjects, it needs an active participation of the students in the learning process. However, nowadays, one of the main difficult to teach this kind of disciplines is to keep the student\u2019s motivation and participation in the learning process. For this reason, the use of new tools, as the educational gamification, can help to change the way to understand and teach this subject [1]. In this context, the application of game-based activities to engineering education has increased in popularity among students [2]. In this work, we present the application of a escape room-based to heat transfer interactive evaluation for third year chemical engineering students.\nThe plot of our escape lab-room is focused on a crazy scientific (Dr. Nusselito Graetzoff) that is obsessed with the heat transfer. He has retained a group of scientists and has the code for a missile launch. It is a national security case! The students (distributed by groups) play the role of heat transfer specialists from different Research Centers, which have to complete two missions: release the imprisoned scientists, and avoid the missile launch planned by Dr. Nusselito. Therefore, the students have to solve different questions/cases related to heat transfer issues, obtaining the codes that will allow them to complete their mission. The plot of the activity is conducted by using videos, in which the different parts of the activity are described. In addition, the professors supervise and help the correct progress of the escape room. We used the same kind of theoretical and applied problems in both evaluation methods, traditional exam and escape room one. After this experience, we can confirm the increase of student participation regarding to previous years. In addition, the students showed enthusiastic attitude (Fig. 1a) and a high satisfaction level. Therefore, the students\u2019 participation of escape lab-room, to heat transfer teaching was completely successful, although this activity only accounts for a 10% of the total course score."
            },
            {
                "reference_title": "\u2018Museum Escape\u2019: A Game to Increase Museum Visibility",
                "reference_link": "publication/337390104_'Museum_Escape'_A_Game_to_Increase_Museum_Visibility",
                "reference_type": "Chapter",
                "reference_date": "Nov 2019",
                "reference_abstract": "A peripheral museum with a nevertheless important collection of ancient items, wished to increase its visibility and digital presence in order to become more known and attract new visitors. Among the different solutions designed and implemented, we also designed a series of games of different kinds (i.e. mini vs complex games) for different purposes, like profiling of potential visitors and advertising the venue in social media. Based on lessons learnt from physical games played at museums of the same type in the same region, we designed and implemented a museum escape game. The game is presented here, to provide implementation details regarding the concept, the game mechanics, the interface design, the technical details, as well as the game art. We conclude with primary user testing and future steps that include plans for creating museum escape games with different technologies, like augmented reality and virtual reality."
            }
        ]
    },
    {
        "title": "Development of Program Comprehension Skills by Novice Programmers - Longitudinal Eye Tracking Studies",
        "date": "December 2020",
        "doi": "10.15388/infedu.2020.23",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (35)",
        "abstract": "The article discusses the findings of longitudinal studies (three stages spanning 6 months) which were to investigate the process of acquiring the ability to comprehension program code by the computer science students having started to learn to program. The studies were conducted with the use of a knowledge measurement test, the diagnostic survey, and eye tracking technology that enabled the recording of movement of the subjects\u2019 eyes and an analysis of the patterns of information processing during solving programming problems. The obtained results have shown that the students solved the tasks most effectively in the last stage of the research during which they obtained the highest indicator of correct answers in the significantly shortest time. In the last stage of the research the dominant form of the algorithmic problem analysis was code, in two previous it was flowchart. The eye tracking data have shown that regardless of the research stage the code analysis was definitely connected with a greater number of fixations, with very near values of time devoted to solving those two forms of the algorithm. The participants who increased their competences in a scope of the program code analysis had a significantly greater saccade amplitude average (SAA) and a significantly shorter fixation duration average (FDA) in the last stage of the research comparing to previous ones. The results suggest that the FDA and SAA are parameters sensitive to the development of program comprehension skills.",
        "reference": [
            {
                "reference_title": "A Survey on the Usage of Eye-Tracking in Computer Programming",
                "reference_link": "publication/322375999_A_Survey_on_the_Usage_of_Eye-Tracking_in_Computer_Programming",
                "reference_type": "Article",
                "reference_date": "Jan 2018",
                "reference_abstract": "Traditional quantitative research methods of data collection in programming, such as questionnaires and interviews, are the most common approaches for researchers in this field. However, in recent years, eye-tracking has been on the rise as a new method of collecting evidence of visual attention and the cognitive process of programmers. Eye-tracking has been used by researchers in the field of programming to analyze and understand a variety of tasks such as comprehension and debugging. In this article, we will focus on reporting how experiments that used eye-trackers in programming research are conducted, and the information that can be collected from these experiments. In this mapping study, we identify and report on 63 studies, published between 1990 and June 2017, collected and gathered via manual search on digital libraries and databases related to computer science and computer engineering. Among the five main areas of research interest are program comprehension and debugging, which received an increased interest in recent years, non-code comprehension, collaborative programming, and requirements traceability research, which had the fewest number of publications due to possible limitations of the eye-tracking technology in this type of experiments. We find that most of the participants in these studies were students and faculty members from institutions of higher learning, and while they performed programming tasks on a range of programming languages and programming representations, we find Java language and Unified Modeling Language (UML) representation to be the most used materials. We also report on a range of eye-trackers and attention tracking tools that have been utilized, and find Tobii eye-trackers to be the most used devices by researchers."
            },
            {
                "reference_title": "The impact of syntax colouring on program comprehension",
                "reference_link": "publication/293652017_The_impact_of_syntax_colouring_on_program_comprehension",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2015",
                "reference_abstract": "We present an empirical study investigating the effect of syntax highlighting on program comprehension and its interaction with programming experience. Quantitative data was captured from 10 human subjects using an eye tracker during a controlled, randomised, within-subjects study. We observe that syntax highlighting significantly improves task completion time, and that this effect becomes weaker with an increase in programming experience."
            },
            {
                "reference_title": "The impact of identifier style on effort and comprehension",
                "reference_link": "publication/257560017_The_impact_of_identifier_style_on_effort_and_comprehension",
                "reference_type": "Article",
                "reference_date": "Apr 2013",
                "reference_abstract": "A family of studies investigating the impact of program identifier style on human comprehension is presented. Two popular identifier styles are examined, namely camel case and underscore. The underlying hypothesis is that identifier style affects the speed and accuracy of comprehending source code. To investigate this hypothesis, five studies were designed and conducted. The first study, which investigates how well humans read identifiers in the two different styles, focuses on low-level readability issues. The remaining four studies build on the first to focus on the semantic implications of identifier style. The studies involve 150 participants with varied demographics from two different universities. A range of experimental methods is used in the studies including timed testing, read aloud, and eye tracking. These methods produce a broad set of measurements and appropriate statistical methods, such as regression models and Generalized Linear Mixed Models (GLMMs), are applied to analyze the results. While unexpected, the results demonstrate that the tasks of reading and comprehending source code is fundamentally different from those of reading and comprehending natural language. Furthermore, as the task becomes similar to reading prose, the results become similar to work on reading natural language text. For more \u201csource focused\u201d tasks, experienced software developers appear to be less affected by identifier style; however, beginners benefit from the use of camel casing with respect to accuracy and effort."
            },
            {
                "reference_title": "Increasing student commitment in introductory programming learning",
                "reference_link": "publication/243464895_Increasing_student_commitment_in_introductory_programming_learning",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2012",
                "reference_abstract": "High failure rates are common in many programming courses worldwide. Many causes for the learning problems have already been identified and different solutions have been proposed. However, the situation remains mostly unchanged. So, new pedagogical approaches are necessary, looking to create learning contexts that motivate students, increase their involvement with course activities, and maximize their learning possibilities. In this paper we present the changes made in the structure of a non-majors introductory programming course, and discuss the results obtained. We also present the results obtained in the first implementation of the new course structure."
            },
            {
                "reference_title": "A Multi-National Study of Reading and Tracing Skills in Novice Programmers",
                "reference_link": "publication/234800767_A_Multi-National_Study_of_Reading_and_Tracing_Skills_in_Novice_Programmers",
                "reference_type": "Article",
                "reference_date": "Jan 2005",
                "reference_abstract": "A study by a ITiCSE 2001 working group (\"the McCracken Group\") established that many students do not know how to program at the conclusion of their introductory courses. A popular explanation for this incapacity is that the students lack the ability to problem-solve. That is, they lack the ability to take a problem description, decompose it into sub-problems and implement them, then assemble the pieces into a complete solution. An alternative explanation is that many students have a fragile grasp of both basic programming principles and the ability to systematically carry out routine programming tasks, such as tracing (or \"desk checking\") through code. This ITiCSE 2004 working group studied the alternative explanation, by testing students from seven countries, in two ways. First, students were tested on their ability to predict the outcome of executing a short piece of code. Second, students were tested on their ability, when given the desired function of short piece of near-complete code, to select the correct completion of the code from a small set of possibilities. Many students were weak at these tasks, especially the latter task, suggesting that such students have a fragile grasp of skills that are a prerequisite for problem-solving."
            },
            {
                "reference_title": "Visually Analyzing Students' Gaze on C++ Code Snippets",
                "reference_link": "publication/335780448_Visually_Analyzing_Students'_Gaze_on_C_Code_Snippets",
                "reference_type": "Conference Paper",
                "reference_date": "May 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Eye Movements in Code Reading: Relaxing the Linear Order",
                "reference_link": "publication/301476421_Eye_Movements_in_Code_Reading_Relaxing_the_Linear_Order",
                "reference_type": "Conference Paper",
                "reference_date": "May 2015",
                "reference_abstract": null
            },
            {
                "reference_title": "Syntax highlighting as an influencing factor when reading and comprehending source code",
                "reference_link": "publication/290454188_Syntax_highlighting_as_an_influencing_factor_when_reading_and_comprehending_source_code",
                "reference_type": "Article",
                "reference_date": "Jan 2016",
                "reference_abstract": "Syntax highlighting or syntax colouring, plays a vital role in programming development environments by colour-coding various code elements differently. The supposition is that this syntax highlighting assists programmers when reading and analysing code. However, academic text books are largely only available in black-and-white which could influence the comprehension of novice and beginner programmers. This study investigated whether student programmers experience more difficulty in reading and comprehending source code when it is presented without syntax highlighting. Number of fixations, fixation durations and regressions were all higher for black-and-white code than for colour code but not significantly so. Subjectively students indicated that the colour code snippets were easier to read and more aesthetically pleasing. Based on the analysis it could be concluded that students do not experience significantly more difficulty when reading code in black-and-white as printed in text books."
            },
            {
                "reference_title": "Eye-tracking verification of the strategy used to analyse algorithms expressed in a flowchart and pseudocode",
                "reference_link": "publication/282401858_Eye-tracking_verification_of_the_strategy_used_to_analyse_algorithms_expressed_in_a_flowchart_and_pseudocode",
                "reference_type": "Article",
                "reference_date": "Sep 2016",
                "reference_abstract": "The results of qualitative and quantitative investigations conducted with individuals who learned algorithms in school are presented in this article. In these investigations, eye-tracking technology was used to follow the process of solving algorithmic problems. The algorithmic problems were presented in two comparable variants: in a pseudocode and flowchart. The eye-tracking data resulted in both qualitative (films registering the gaze paths) and quantitative measures, which allowed the detection and interpretation of the differences in the task-solving strategies between those who found the correct answer and those that did not. The results confirmed a hypothesis that use of the formal notation characteristic of a programming language for presenting algorithms is often a practical difficulty in the process of solving even simple tasks. This study opens a new direction of research; they show that eye-tracking technology can be used to optimise the educational process of learning programming."
            },
            {
                "reference_title": "A review of using eye-tracking technology in exploring learning from 2000 to 2012",
                "reference_link": "publication/259165122_A_review_of_using_eye-tracking_technology_in_exploring_learning_from_2000_to_2012",
                "reference_type": "Article",
                "reference_date": "Dec 2013",
                "reference_abstract": "This study aims to disclose how eye-tracking technology has been applied to studies of learning, and what eye movement measures have been used for investigations by reviewing studies that have employed the eye-tracking approach. A total of 81 papers including 113 studies were selected from the Social Sciences Citation Index database from 2000 to 2012. Content analysis showed that eye movements and learning were studied under the following seven themes: patterns of information processing, effects of instructional design, reexamination of existing theories, individual differences, effects of learning strategies, patterns of decision making, and conceptual development. As for eye-tracking measurements, the most often used indices were temporal measures, followed by count and spatial measures, although the choice of measures was often motivated by the specific research question. Research development trends show that the use of the eye-tracking method has proliferated recently. This study concludes that the eye-tracking method provides a promising channel for educational researchers to connect learning outcomes to cognitive processes."
            }
        ]
    },
    {
        "title": "Creating a sustainable digital infrastructure: The role of service-oriented architecture",
        "date": "December 2020",
        "doi": "10.13140/RG.2.2.34362.00963",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (49)",
        "abstract": "Note: This paper was presented at the Centeris conference Oct 22 2020. The paper is forthcoming in Procedia Computer Science, Elsevier. \n\nAbstract: The United Nations' goal of generating sustainable industry, innovation, and infrastructure is the point of departure for our reflective paper. The paper elaborates on the concepts of digital infrastructure, service-oriented architecture, and microservices. It emphasizes the benefits and challenges of creating a sustainable infrastructure based on a service-oriented environment, in which cloud services constitute an important part. We outline the prerequisites for obtaining a sustainable digital infrastructure based on services. Service-oriented architecture (SOA) and recently, microservice architecture, and cloud services, can provide organizations with the improved agility and flexibility essential for generating sustainability in a market focusing on digitalization. The reuse capability of SOA provides a common pool of information technology (IT) resources and qualifies as a green IT approach that impacts environmental protection. Previous research has identified IT and business alignment together with SOA governance as the most critical criteria when implementing SOA. This paper discusses these issues in-depth to explain sustainability.",
        "reference": [
            {
                "reference_title": "An Energy Efficient Architecture of IoT based on Service Oriented Architecture (SOA)",
                "reference_link": "publication/332906616_An_Energy_Efficient_Architecture_of_IoT_based_on_Service_Oriented_Architecture_SOA",
                "reference_type": "Article",
                "reference_date": "Mar 2019",
                "reference_abstract": "In the present scenario, IoT is a platform where everyday gadgets are becoming smarter, each day processing will become shrewd, and every day verbal exchange is becoming informative. Even as the IoT is still searching for its basic form, its result has already started in making exquisite strides as a universal solution media for the associated situation. The study which focused on architecture always paves the conformation of associated discipline. The shortage of overall architectural abilities is at this time resisting the researchers to get a way of the scope of procedures based on energy efficient IoT. In this endeavor Service Oriented Architecture (SOA) based IoT architecture and algorithm have been proposed to address energy efficiency in IoT environment. Experimental result has been shown to lay bare the effectiveness of the proposed approach."
            },
            {
                "reference_title": "Using architectural modifiability tactics to examine evolution qualities of Service- and Microservice-Based Systems",
                "reference_link": "publication/330969869_Using_architectural_modifiability_tactics_to_examine_evolution_qualities_of_Service-_and_Microservice-Based_Systems",
                "reference_type": "Article",
                "reference_date": "Jun 2019",
                "reference_abstract": "Software evolvability is an important quality attribute, yet one difficult to grasp. A certain base level of it is allegedly provided by Service-and Microservice-Based Systems, but many software professionals lack systematic understanding of the reasons and preconditions for this. We address this issue via the proxy of architectural modifiability tactics. By qualitatively mapping principles and patterns of Service-Oriented Architecture (SOA) and Microservices onto tactics and analyzing the results, we cannot only generate insights into service-oriented evolution qualities, but can also provide a modifiability comparison of the two popular service-based architectural styles. The results suggest that both SOA and Microservices possess several inherent qualities beneficial for software evolution. While both focus strongly on loose coupling and encapsulation, there are also differences in the way they strive for modifiability (e.g. governance vs. evolutionary design). To leverage the insights of this research, however, it is necessary to find practical ways to incorporate the results as guidance into the software development process."
            },
            {
                "reference_title": "Microservices: The Journey So Far and Challenges Ahead",
                "reference_link": "publication/324959590_Microservices_The_Journey_So_Far_and_Challenges_Ahead",
                "reference_type": "Article",
                "reference_date": "May 2018",
                "reference_abstract": "Microservices are an architectural approach emerging out of service-oriented architecture, emphasizing self-management and lightweightness as the means to improve software agility, scalability, and autonomy. This article examines microservice evolution from the technological and architectural perspectives and discusses key challenges facing future microservice developments."
            },
            {
                "reference_title": "Will Cloud Computing make the Information Technology (IT) Department Obsolete?",
                "reference_link": "publication/317503796_Will_Cloud_Computing_make_the_Information_Technology_IT_Department_Obsolete",
                "reference_type": "Article",
                "reference_date": "Jul 2017",
                "reference_abstract": "The rapid adoption and growth of cloud computing is creating unprecedented change in the manner in which IT services are procured, managed, and deployed. Cloud computing is forcing firms to rethink traditional IT governance practices while raising new and fundamental questions for scholars and practitioners. This paper identifies the major areas of change and highlights governance issues that arise with the adoption of cloud computing. The focus of this paper is on the organizational impact on IT governance under cloud computing. The paper posits: (i) That successful IT departments under cloud computing will transform into new roles that address internal customer-facing issues and external cloud-facing issues, (ii) Firms that mitigate information asymmetry under cloud computing will show higher firm performance and, (iii) Firms that offer superior cloud-sourced IT service attributes of internal prices, quality, variety and competition in the cloud will show higher firm performance."
            },
            {
                "reference_title": "Architectural Principles for Cloud Software",
                "reference_link": "publication/317348634_Architectural_Principles_for_Cloud_Software",
                "reference_type": "Article",
                "reference_date": "Jun 2017",
                "reference_abstract": "A cloud is a distributed Internet-based software system providing resources as tiered services. Through service-orientation and virtualization for resource provisioning, cloud applications can be deployed and managed dynamically. We discuss the building blocks of an architectural style for cloud-based software systems. We capture style-defining architectural principles and patterns for control-theoretic, model-based architectures for cloud software. While service-orientation is agreed upon in the form of service-oriented architecture and microservices, challenges resulting from multi-tiered, distributed and heterogeneous cloud architectures cause uncertainty that has not been sufficiently addressed. We define principles and patterns needed for effective development and operation of adaptive cloud-native systems."
            },
            {
                "reference_title": "Re-Infrastructuring for eHealth: Dealing with Turns in Infrastructure Development",
                "reference_link": "publication/313828197_Re-Infrastructuring_for_eHealth_Dealing_with_Turns_in_Infrastructure_Development",
                "reference_type": "Article",
                "reference_date": "Apr 2017",
                "reference_abstract": "http://rdcu.be/HebS (link to full text)\n\nIn this paper, we examine infrastructuring in the context of developing national, public eHealth services in Norway. Specifically, we analyze the work of a project team engaged in the design and development of new web-based capabilities for communication between citizens and primary healthcare practitioners. We frame the case as a study of re-infrastructuring to signify a particular occasion of infrastructuring that entails facilitating a new logic within established social and technological networks. To make sense of the particularities of re-infrastructuring, we draw from research in infrastructure studies which considers embeddedness as a resource in infrastructure evolution. We analyze how actors worked to re-infrastructure through adapting primary care information systems, information flows and representations of patient data. Our findings show how the work of re-infrastructuring revolves around addressing two key design concerns: a) bringing novelty without being trapped in the existing arrangements or harming what is in place, b) bringing changes that are within a specific direction although they happen through distributed decision taking."
            },
            {
                "reference_title": "Enterprise Architecture Adoption Challenges: An exploratory Case Study of the Norwegian Higher Education Sector",
                "reference_link": "publication/313175958_Enterprise_Architecture_Adoption_Challenges_An_exploratory_Case_Study_of_the_Norwegian_Higher_Education_Sector",
                "reference_type": "Article",
                "reference_date": "Dec 2016",
                "reference_abstract": "IT is a challenge to implement Enterprise Architecture (EA) in an organisation. This is also the case in the public sector, like public universities and colleges. There is a very limited research on such issues. It is therefore important to investigate how AE is implemented in various sectors, which benefits are realised and which challenges are most prominent. This interpretive case study investigates the efforts taken towards a common EA in the Norwegian higher education sector. We find that the progress was severely impeded by the lack of top level directions from the ministry, the lack of an overarching architecture council, and the lack of EA competence at the top management level at the individual institutions. The perceived most important benefits were business agility, economies of scale and better decision making."
            },
            {
                "reference_title": "Understanding Service-Oriented Architecture (SOA): A systematic literature review and directions for further investigation",
                "reference_link": "publication/338665070_Understanding_Service-Oriented_Architecture_SOA_A_systematic_literature_review_and_directions_for_further_investigation",
                "reference_type": "Article",
                "reference_date": "Jul 2020",
                "reference_abstract": "Service-Oriented Architecture (SOA) has emerged as an architectural approach that enhances the service delivery performance of existing traditional systems while still retaining their most important features. This approach, due to its flexibility of adoption, has gained the attention of both academic and business entities, especially in the development of world-leading technologies such as Cloud Computing (CC) and the Internet of Things (IoT). Although many studies have listed the success factors of SOA, a few minor failures have also been reported in the literature. Despite the availability of rich material on SOA, there is a lack of systematic reviews covering the different aspects of the SOA concept in Information Systems (IS) research. Therefore, the central objective of this study is to review existing issues of SOA and share the findings with academia. Hence, a systematic literature review (SLR) was conducted to analyse existing studies related to SOA and the factors that led to SOA success and failure from 2009 to 2019. To completely cover all SOA-related research in the IS field, a two-stage review protocol that included automatic and manual searching was applied, resulting in 103 primary studies. The articles were categorised into four research themes, namely: SOA Adoption, SOA Concepts, SOA Impact, and SOA Practice. The result shows that the academic research interest on SOA increased recently with most of the articles covering SOA Practice followed by SOA Adoption. Moreover, the findings of this review highlighted SOA Governance, SOA Strategy, Financial Issues and Costs, and Education and Training as the most significant factors of SOA adoption and implementation. Consequently, the outcomes will assist professionals and experts in organisations as well as academic researchers to focus more on these factors for successfully adopting and implementing SOA."
            },
            {
                "reference_title": "Migrating towards microservices: migration and architecture smells",
                "reference_link": "publication/327129237_Migrating_towards_microservices_migration_and_architecture_smells",
                "reference_type": "Conference Paper",
                "reference_date": "Sep 2018",
                "reference_abstract": "Migrating to microservices is an error-prone process with deep pitfalls resulting in high costs for mistakes. Microservices is a relatively new architectural style, resulting in the lack of general guidelines for migrating monoliths towards microservices. We present 9 common pitfalls in terms of bad smells with their potential solutions. Using these bad smells, pitfalls can be identified and corrected in the migration process."
            },
            {
                "reference_title": "Understanding Cloud Computing Adoption Issues: A Delphi Study Approach",
                "reference_link": "publication/301734320_Understanding_Cloud_Computing_Adoption_Issues_A_Delphi_Study_Approach",
                "reference_type": "Article",
                "reference_date": "Apr 2016",
                "reference_abstract": "This research paper reports on a Delphi study focusing on the most important issues enterprises are confronted with when making cloud computing (CC) adoption decisions. We had 34 experts from different domain backgrounds participated in a Delphi panel. The panelists were IT and CC specialists representing a heterogeneous group of clients, providers and academics, divided into three subpanels. The Delphi procedure comprised three stages: brainstorming, narrowing down and ranking. The panelists identified 55 issues of concerns in the first stage, which were analyzed and grouped into 10 categories: security, strategy, legal and ethical, IT governance, migration, culture, business, awareness, availability and impact. The top 18 issues for each subpanel were ranked, and a moderate intrapanel consensus was obtained. Additionally, 16 follow-up interviews were conducted with the experts to get a deeper understanding of the issues and why certain issues were more significant than others. The findings indicate that security, strategy and legal and ethical issues are the most important. The discussion resulted in highlighting certain inhibitors and drivers for CC adoption into a framework. The paper is concluded with key recommendations with focus on change management, competence and maturity to inform decision-makers in CC adoption decisions."
            }
        ]
    },
    {
        "title": "Experimental pedagogical activity when teaching computer science to younger students",
        "date": "December 2020",
        "doi": "10.22363/2312-8631-2020-17-1-18-25",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (17)",
        "abstract": "Problem and goal. One of the important components of the educational and methodical work of a teacher who teaches computer science to younger students is the quality control of the acquired system of knowledge of schoolchildren, which involves experimental pedagogical activity. Such experimental pedagogical activities include pedagogical experiments, the results of which are processed and analyzed using mathematical methods. Pedagogical measurements allow to justify the effectiveness of the implemented teaching methodology. Methodology. Pedagogical measurements are carried out in pedagogical research aimed at improving the content, methods, forms and means of teaching computer science to younger students using didactic games. The pedagogical experiment itself includes ascertaining, searching, forming and controlling stages. The analysis of the results of pedagogical measurements aimed at identifying the quality of the obtained knowledge in computer science of younger students can be effectively carried out using mathematical and statistical methods. Results. The conducted pedagogical measurements allow to draw a conclusion about how much younger students have subject knowledge in computer science, which is taught using didactic games; to identify the level of their worldview, logical thinking. Conclusion. Experimental pedagogical activity of the teacher in the process of teaching computer science to younger students allows to identify the effectiveness of such training, the level of subject knowledge of students. Analysis of the results of pedagogical measurements allows, if necessary, to correct the methodology and content of training in computer science.",
        "reference": [
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2007",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2008",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": null,
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2014",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2007",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2008",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2002",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2004",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2006",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2004",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Conte\u00fado Conceitual e Aspectos Pr\u00e1ticos da Ci\u00eancia da Computa\u00e7\u00e3o",
        "date": "December 2020",
        "doi": "10.22533/at.ed.010201412",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (162)",
        "abstract": "A Ci\u00eancia da Computa\u00e7\u00e3o, traz in\u00fameros benef\u00edcios para a sociedade\nmoderna, tais como: a cria\u00e7\u00e3o de empregos, o desenvolvimento de novos\nequipamentos, o ganho de produtividade nas empresas e o acesso \u00e0 informa\u00e7\u00e3o.\nOs estudos realizados nesta \u00e1rea s\u00e3o aplicados em diversas outras \u00e1reas do\nconhecimento, proporcionando a resolu\u00e7\u00e3o de diferentes problemas da sociedade,\ntrazendo avan\u00e7os significativos para a vida de in\u00fameras pessoas, fazendo com que\ncada vez mais estes profissionais sejam valorizados, requisitados e prestigiados no\nmercado de trabalho.",
        "reference": [
            {
                "reference_title": "DSS (A Data Science Suite)",
                "reference_link": "publication/337717164_DSS_A_Data_Science_Suite",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2019",
                "reference_abstract": "This paper presents a set of applications with graphical interface for\ndata science tasks. In the course of the paper we shall be discussing applications\nfor data visualization, machine and deep learning, natural language processing,\nGraph data analysis and time series analysis. This suite of applications runs in\na service at LNCC provided by the DEXL Lab Group and can be individually\ninstalled in a local machine using docker images."
            },
            {
                "reference_title": "Multi-Criteria Review-Based Recommender System \u2013 The State of the Art",
                "reference_link": "publication/337435472_Multi-Criteria_Review-Based_Recommender_System_-_The_State_of_the_Art",
                "reference_type": "Article",
                "reference_date": "Nov 2019",
                "reference_abstract": "In recent times, the recommender systems (RSs) have considerable importance in academia, commercial activities, and industry. They are widely used in various domains such as shopping (Amazon), music (Pandora), movies (Netflix), travel (TripAdvisor), restaurant (Yelp), people (Facebook), and articles (TED). Most of the RSs approaches rely on a single-criterion rating (overall rating) as a primary source for the recommendation process. However, the overall rating is not enough to gain high accuracy of recommendations because the overall rating cannot express fine-grained analysis behind the user\u2019s behavior. To solve this problem, multi-criteria recommender systems (MCRSs) have been developed to improve the accuracy of the RS performance. Additionally, a new source of information represented by the user-generated reviews is incorporated in the recommendation process because of the rich and numerous information included (i.e. review elements) related to the whole item or to a certain feature of the item or the user\u2019s preferences. The valuable review elements are extracted using either text mining or sentiment analysis. MCRSs benefit from the review elements of the user-generated reviews in building their criteria forming multi-criteria review based recommender systems. The review elements improve the accuracy of the RS performance and mitigate most of the RS\u2019s problems such as the cold start and sparsity. In this review, we focused on the multi-criteria review-based recommender system and explained the user reviews elements in detail and how these can be integrated into the RSs to help develop their criteria to enhance the RSs performance. Finally, based on the survey, we presented four future trends based on this type of RSs to support researchers who wish to pursue studies in this area."
            },
            {
                "reference_title": "SISTEMAS DE INFORMA\u00c7\u00c3O GEOGR\u00c1FICA: UMA ABORDAGEM CONTEXTUALIZADA NA HIST\u00d3RIA",
                "reference_link": "publication/335526583_SISTEMAS_DE_INFORMACAO_GEOGRAFICA_UMA_ABORDAGEM_CONTEXTUALIZADA_NA_HISTORIA",
                "reference_type": "Article",
                "reference_date": "Jan 2008",
                "reference_abstract": "Desde as remotas sociedades, passando pela inven\u00e7\u00e3o do tel\u00e9grafo el\u00e9trico no s\u00e9culo XIX, pelos meios de comunica\u00e7\u00e3o de massa no s\u00e9culo XX, e at\u00e9 mais recentemente, o surgimento da Internet, onde a informa\u00e7\u00e3o come\u00e7ou a fluir com velocidade superior a dos corpos f\u00edsicos, o ser humano convive com um crescimento exponencial do volume de dados dispon\u00edveis das mais diversas \u00e1reas do conhecimento. Todavia, a partir de 1960, os conceitos de dados e informa\u00e7\u00f5es sofreram grandes transforma\u00e7\u00f5es pelo advento dos computadores eletr\u00f4nicos, hardwares e sofisticados softwares como os sistemas de informa\u00e7\u00e3o geogr\u00e1fica (SIG). O uso dos SIG atinge atualmente a quase todas as universidades, institutos de pesquisas e empresas, criando enormes possibilidades \u00e0s mais variadas linhas de pesquisa das geoci\u00eancias, principalmente no que tange a an\u00e1lise espacial de dados geogr\u00e1ficos. O dom\u00ednio da informa\u00e7\u00e3o espacial dispon\u00edvel \u00e9 uma fonte de poder, uma vez que permite analisar fatores do passado, relacionando-os com os fen\u00f4menos da natureza e da sociedade, compreendendo o presente, e principalmente, antevendo o futuro do espa\u00e7o geogr\u00e1fico. Assim, o presente artigo objetiva contextualizar historicamente a evolu\u00e7\u00e3o dos SIG nas \u00faltimas d\u00e9cadas, abordando seu desenvolvimento, funcionalidades e caracter\u00edsticas, al\u00e9m de apresentar os pioneiros nesses sistemas no Brasil. Dispon\u00edvel em: http://www.periodicos.rc.biblioteca.unesp.br/index.php/ageteo/article/view/1775/5203"
            },
            {
                "reference_title": "Uma proposta de arquitetura m\u00f3vel baseada em vis\u00e3o computacional para pessoas com defici\u00eancia visual",
                "reference_link": "publication/339547878_Uma_proposta_de_arquitetura_movel_baseada_em_visao_computacional_para_pessoas_com_deficiencia_visual",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2017",
                "reference_abstract": "Este artigo discute um Sistema Computacional, os aspectos sobre possibilidades da utiliza\u00e7\u00e3o de t\u00e9cnicas e recentes avan\u00e7os na\u00e1rea de Vis\u00e3o Computacional para o desenvolvimento de recursos a partir do paradigma de Computa\u00e7\u00e3o Ub\u00edqua, que proporcionem a melhoria na qualidade de vida de pessoas com necessidades visuais, especialmente na realiza\u00e7\u00e3o de tarefas do cotidiano. Al\u00e9m disso, este artigo prop\u00f5e um Sistema Computacional para mobile que realiza as tarefas: i) classifica\u00e7\u00e3o de cenas; ii) reconhecimento de objetos; iii) detec\u00e7\u00e3o de colis\u00e3o. S\u00e3o tratados aspectos relativos \u00e0 arquitetura do sistema. Tendo em vista que cerca de 285 milh\u00f5es de pessoas ao redor do mundo possuem algum tipo de defici\u00eancia visual [WHO, 2014] e s\u00e3o frequentemente exclu\u00eddas em fun\u00e7\u00e3o de suas limita\u00e7\u00f5es, o desenvolvimento de Tecnologia Assistiva se torna necess\u00e1rio para proporcionar a inclus\u00e3o das mesmas. Dessa forma, a motiva\u00e7\u00e3o do presente projeto \u00e9 a inclus\u00e3o social de pessoas com defici\u00eancia visual atrav\u00e9s de solu\u00e7\u00f5es assistivas."
            },
            {
                "reference_title": "Utiliza\u00e7\u00e3o do software ImageJ para avaliar \u00e1rea de les\u00e3o dermonecr\u00f3tica",
                "reference_link": "publication/337749041_Utilizacao_do_software_ImageJ_para_avaliar_area_de_lesao_dermonecrotica",
                "reference_type": "Article",
                "reference_date": "Aug 2019",
                "reference_abstract": "Objetivo: verificar como se utiliza o software livre de an\u00e1lise de imagens ImageJ para avaliar \u00e1rea de les\u00f5es em processo de dermonecrose em fotografia digital. M\u00e9todo: realizou-se revis\u00e3o da literatura de artigos indexados nas bases de dados BVS/LILACS e PubMed com os descritores DeCS/MeSH, filtros: ingl\u00eas, portugu\u00eas e espanhol; estudos experimentais; 2017 e 2018. Busca direta no PubMed e sele\u00e7\u00e3o de artigos similares. Resultados: foram selecionados e analisados 10 artigos. Em dois deles o ImageJ foi utilizado para avalia\u00e7\u00e3o do processo cicatricial, um validou o sistema de medi\u00e7\u00e3o port\u00e1til com uso de software para avaliar dimens\u00f5es de feridas e os demais as ferramentas do ImageJ para processamento de imagens Conclus\u00e3o: ImageJ se apresentou como importante ferramenta no tratamento e an\u00e1lise quantitativa de imagens, oferecendo muitos recursos como parte de seu pacote padr\u00e3o e outros tantos como plugins e extens\u00f5es do software, que se adaptam para cada tipo de uso."
            },
            {
                "reference_title": "Desenvolvimento de Laborat\u00f3rio Remoto Utilizando M\u00f3dulo Did\u00e1tico para Ensino de Microcontroladores",
                "reference_link": "publication/337407976_Desenvolvimento_de_Laboratorio_Remoto_Utilizando_Modulo_Didatico_para_Ensino_de_Microcontroladores",
                "reference_type": "Conference Paper",
                "reference_date": "Nov 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "A Multi-criteria Collaborative Filtering Recommender System Using Learning-to-Rank and Rank Aggregation",
                "reference_link": "publication/336105543_A_Multi-criteria_Collaborative_Filtering_Recommender_System_Using_Learning-to-Rank_and_Rank_Aggregation",
                "reference_type": "Article",
                "reference_date": "Sep 2019",
                "reference_abstract": "Recommender system suggests a top-N list from unseen items for its users through a prediction or a ranking order process. From the recommendation perspective, the item\u2019s order in the generated list is more important than its predicted rating. Moreover, finding the top-N list for a multi-criteria recommendation is a challenging problem as we have many criterions for each item. One can find the average over all criteria; however, this requires a score from each criterion and hence a compensation effect will occur. This resembles many prediction-based recommendation systems working in parallel. Alternately, this paper proposes a three-step hybrid ranking order system for finding the top-N list for the multi-criteria recommendation system. The first step decomposes the multi-criteria user-item matrix into many single-rating user-item matrices while the second step finds partial-ranked lists for each item using a learning-to-rank method. This allows us to reflect the interest of the user for each criterion and then pass on this information for the next stage. The last step aggregates the partial-ranked lists into a global-ranked list using a ranking aggregation method. This will reduce the processing time and improve the recommendation quality by representing the user preference for each criterion. Three different sets of experiments are conducted on Yahoo!Movie dataset, and the results show that the proposed multi-criteria-ranking approach outperforms both the traditional no-ranking item-based collaborative recommendation and single-criteria-ranking approach that uses two popular learning-to-rank methods."
            },
            {
                "reference_title": "UMA PROPOSTA DE APLICA\u00c7\u00c3O DE JOGOS S\u00c9RIOS PARA AUXILIAR NA IDENTIFICA\u00c7\u00c3O DE DISLEXIA E DISLALIA EM CRIAN\u00c7AS",
                "reference_link": "publication/333939351_UMA_PROPOSTA_DE_APLICACAO_DE_JOGOS_SERIOS_PARA_AUXILIAR_NA_IDENTIFICACAO_DE_DISLEXIA_E_DISLALIA_EM_CRIANCAS",
                "reference_type": "Chapter",
                "reference_date": "Jun 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Expert fuzzy modeling of dynamic properties of complex systems",
                "reference_link": "publication/328132100_Expert_fuzzy_modeling_of_dynamic_properties_of_complex_systems",
                "reference_type": "Article",
                "reference_date": "Jan 2016",
                "reference_abstract": "The purpose of this article is to represent an extension of the classical notion of fuzzy sets to estimate the condition of complex systems, which is based on the dynamic fuzzy sets concept. In this article, the authors prove the necessity to use dynamic fuzzy sets to estimate the condition of a complex system. Firstly, we present the basic definition and construction method of dynamic fuzzy sets. There are described analytical and graphical representations of dynamic fuzzy sets, which are represented by dynamic membership functions constructed in a three-dimensional coordinate system, where one of the axes in the graph captures the time variation of the properties of the economic objects expressed by a set of numbers with fuzzy function. We develop the simulation of dynamic membership functions by expressions of expert assessments on multilevel fuzzy description of the complex indicators, which are made in strict sequence from a reference point and bifurcation points, selection of functional dependencies, to build a static membership functions, and then the dynamic membership functions on the whole interval assessment. Finally, we develop a few software systems to interactive construction dynamic membership function. Thus, this paper can provide interactive process of expert assessment of complex dynamic systems, which include quantitative and qualitative indicators. \u00a9 2006-2016 Asian Research Publishing Network (ARPN). All rights reserved."
            },
            {
                "reference_title": "Evaluating the Performance of Wearable Tecassist Device using Aural and Tactile Feedbacks",
                "reference_link": "publication/323505901_Evaluating_the_Performance_of_Wearable_Tecassist_Device_using_Aural_and_Tactile_Feedbacks",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2017",
                "reference_abstract": "This work presents a methodology and results of tests in order to evaluate a wearable electronic device (cap) adapted for people with visual impairment using in urban locomotion. At the time two versions were tested, one using aural feedback and another using tactile feedback. Visually impaired participants of the Association of Blind People of Passo Fundo tested these versions. A controlled environment simulated a real path with obstacles to be avoided by the participants. Preliminary results showed that most part of participants preferred the tactile feedback, but we observed better performances with the aural feedback. New evaluations are necessary in order to validate this approach considering a larger group of users."
            }
        ]
    },
    {
        "title": "5G Networks Course Outline SYSC5804 Advanced Topics in Communications Systems COMP 5900 Selected Topics in Computer Science ITEC 5910W Selected Topics in Network Technologies",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": null,
        "abstract": "1. Erik Dahlman, Stefan Parkvall, Johan Skold, 5G NR: The Next Generation Wireless\nAccess Technology, Academic Press, 2018.\n2. Saad Z. Asif, 5G Mobile Communications - Concepts and Technologies, Taylor &\nFrancis Group LLC - CRC Press, 2019.",
        "reference": []
    },
    {
        "title": "miRID: Multi-Modal Image Registration Using Modality-Independent and Rotation-Invariant Descriptor",
        "date": "December 2020",
        "doi": "10.3390/sym12122078",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (32)",
        "abstract": "Axiomatically, symmetry is a fundamental property of mathematical functions defining similarity measures, where similarity measures are important tools in many areas of computer science, including machine learning and image processing. In this paper, we investigate a new technique to measure the similarity between two images, a fixed image and a moving image, in multi-modal image registration (MIR). MIR in medical image processing is essential and useful in diagnosis and therapy guidance, but still a very challenging task due to the lack of robustness against the rotational variance in the image transformation process. Our investigation leads to a novel, local self-similarity descriptor, called the modality-independent and rotation-invariant descriptor (miRID). By relying on the mean of the intensity values, an miRID is simply computable and can effectively handle the complicated intensity relationship between multi-modal images. Moreover, it can also overcome the problem of rotational variance by sorting the numerical values, each of which is the absolute difference between each pixel\u2019s intensity and the mean of all pixel intensities within a patch of the image. The experimental result shows that our method outperforms others in both multi-modal rigid and non-rigid image registrations.",
        "reference": [
            {
                "reference_title": "Analogy in Terms of Identity, Equivalence, Similarity, and Their Cryptomorphs",
                "reference_link": "publication/333733105_Analogy_in_Terms_of_Identity_Equivalence_Similarity_and_Their_Cryptomorphs",
                "reference_type": "Article",
                "reference_date": "Jun 2019",
                "reference_abstract": "Analogy belongs to the class of concepts notorious for a variety of definitions generating continuing disputes about their preferred understanding. Analogy is typically defined by or at least associated with similarity, but as long as similarity remains undefined this association does not eliminate ambiguity. In this paper, analogy is considered synonymous with a slightly generalized mathematical concept of similarity which under the name of tolerance relation has been the subject of extensive studies over several decades. In this approach, analogy can be mathematically formalized in terms of the sequence of binary relations of increased generality, from the identity, equivalence, tolerance, to weak tolerance relations. Each of these relations has cryptomorphic presentations relevant to the study of analogy. The formalism requires only two assumptions which are satisfied in all of the earlier attempts to formulate adequate definitions which met expectations of the intuitive use of the word analogy in general contexts. The mathematical formalism presented here permits theoretical analysis of analogy in the contrasting comparison with abstraction, showing its higher level of complexity, providing a precise methodology for its study and informing philosophical reflection. Also, arguments are presented for the legitimate expectation that better understanding of analogy can help mathematics in establishing a unified and universal concept of a structure."
            },
            {
                "reference_title": "Robust Self-Similarity Descriptor for Multimodal Image Registration",
                "reference_link": "publication/327133048_Robust_Self-Similarity_Descriptor_for_Multimodal_Image_Registration",
                "reference_type": "Conference Paper",
                "reference_date": "Jun 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "SimpleITK Image-Analysis Notebooks: a Collaborative Environment for Education and Reproducible Research",
                "reference_link": "publication/321329824_SimpleITK_Image-Analysis_Notebooks_a_Collaborative_Environment_for_Education_and_Reproducible_Research",
                "reference_type": "Article",
                "reference_date": "Nov 2017",
                "reference_abstract": "Modern scientific endeavors increasingly require team collaborations to construct and interpret complex computational workflows. This work describes an image-analysis environment that supports the use of computational tools that facilitate reproducible research and support scientists with varying levels of software development skills. The Jupyter notebook web application is the basis of an environment that enables flexible, well-documented, and reproducible workflows via literate programming. Image-analysis software development is made accessible to scientists with varying levels of programming experience via the use of the SimpleITK toolkit, a simplified interface to the Insight Segmentation and Registration Toolkit. Additional features of the development environment include user friendly data sharing using online data repositories and a testing framework that facilitates code maintenance. SimpleITK provides a large number of examples illustrating educational and research-oriented image analysis workflows for free download from GitHub under an Apache 2.0 license: github.com/InsightSoftwareConsortium/SimpleITK-Notebooks."
            },
            {
                "reference_title": "miLBP: a robust and fast modality-independent 3D LBP for multimodal deformable registration",
                "reference_link": "publication/303738913_miLBP_a_robust_and_fast_modality-independent_3D_LBP_for_multimodal_deformable_registration",
                "reference_type": "Article",
                "reference_date": "Jun 2016",
                "reference_abstract": "Purpose: \nComputer-assisted intervention often depends on multimodal deformable registration to provide complementary information. However, multimodal deformable registration remains a challenging task.\n\nMethods: \nThis paper introduces a novel robust and fast modality-independent 3D binary descriptor, called miLBP, which integrates the principle of local self-similarity with a form of local binary pattern and can robustly extract the similar geometry features from 3D volumes across different modalities. miLBP is a bit string that can be computed by simply thresholding the voxel distance. Furthermore, the descriptor similarity can be evaluated efficiently using the Hamming distance.\n\nResults: \nmiLBP was compared to vector-valued self-similarity context (SSC) in artificial image and clinical settings. The results show that miLBP is more robust than SSC in extracting local geometry features across modalities and achieved higher registration accuracy in different registration scenarios. Furthermore, in the most challenging registration between preoperative magnetic resonance imaging and intra-operative ultrasound images, our approach significantly outperforms the state-of-the-art methods in terms of both accuracy ([Formula: see text]) and speed (29.2 s for one case).\n\nConclusions: \nRegistration performance and speed indicate that miLBP has the potential of being applied to the time-sensitive intra-operative computer-assisted intervention."
            },
            {
                "reference_title": "Comparison of manual and automatic MR-CT registration for radiotherapy of prostate cancer",
                "reference_link": "publication/302970533_Comparison_of_manual_and_automatic_MR-CT_registration_for_radiotherapy_of_prostate_cancer",
                "reference_type": "Article",
                "reference_date": "May 2016",
                "reference_abstract": "In image\u2010guided radiotherapy (IGRT) of prostate cancer, delineation of the clinical target volume (CTV) often relies on magnetic resonance (MR) because of its good soft\u2010tissue visualization. Registration of MR and computed tomography (CT) is required in order to add this accurate delineation to the dose planning CT. An automatic approach for local MR\u2010CT registration of the prostate has previously been developed using a voxel property\u2010based registration as an alternative to a manual landmark\u2010based registration. The aim of this study is to compare the two registration approaches and to investigate the clinical potential for replacing the manual registration with the automatic registration. Registrations and analysis were performed for 30 prostate cancer patients treated with IGRT using a Ni\u2010Ti prostate stent as a fiducial marker. The comparison included computing translational and rotational differences between the approaches, visual inspection, and computing the overlap of the CTV. The computed mean translational difference was 1.65, 1.60, and 1.80 mm and the computed mean rotational difference was 1.51\u00b0, 3.93\u00b0, and 2.09\u00b0 in the superior/inferior, anterior/posterior, and medial/lateral direction, respectively. The sensitivity of overlap was 87%. The results demonstrate that the automatic registration approach performs registrations comparable to the manual registration.\nPACS number(s): 87.57.nj, 87.61.\u2010c, 87.57.Q\u2010, 87.56.J\u2010"
            },
            {
                "reference_title": "Survey of medical image registration",
                "reference_link": "publication/288624109_Survey_of_medical_image_registration",
                "reference_type": "Article",
                "reference_date": "Jan 2013",
                "reference_abstract": "Computerized Image Registration approaches can offer automatic and accurate image alignments without extensive user involvement and provide tools for visualizing combined images. The aim of this survey is to present a review of publications related to Medical Image Registration. This paper paints a comprehensive picture of image registration methods and their applications. This paper is an introduction for those new to the \ufb01eld, an overview for those working in the \ufb01eld and a reference for those searching for literature on a speci\ufb01c application. Methods are classi\ufb01ed according to the di\ufb00erent aspects of Medical Image Registration.\nKeywords:\nimage registration deformable model multimodal extrinsic elastic rigid non rigid voxel based feature based"
            },
            {
                "reference_title": "Sorted self-similarity for multi-modal image registration",
                "reference_link": "publication/309333026_Sorted_self-similarity_for_multi-modal_image_registration",
                "reference_type": "Conference Paper",
                "reference_date": "Aug 2016",
                "reference_abstract": "In medical image analysis, registration of multimodal images has been challenging due to the complex intensity relationship between images. Classical multi-modal registration approaches evaluate the degree of the alignment by measuring the statistical dependency of the intensity values between images to be aligned. Employing statistical similarity measures, such as mutual information, is not promising in those cases with complex and spatially dependent intensity relations. A new similarity measure is proposed based on the assessing the similarity of pixels within an image, based on the idea that similar structures in an image are more probable to undergo similar intensity transformations. The most significant pixel similarity values are considered to transmit the most significant self-similarity information. The proposed method is employed in a framework to register different modalities of real brain scans and the performance of the method is compared to the conventional multi-modal registration approach. Quantitative evaluation of the method demonstrates the better registration accuracy in both rigid and non-rigid deformations."
            },
            {
                "reference_title": "Self-similarity measure for multi-modal image registration",
                "reference_link": "publication/307515841_Self-similarity_measure_for_multi-modal_image_registration",
                "reference_type": "Conference Paper",
                "reference_date": "Sep 2016",
                "reference_abstract": null
            },
            {
                "reference_title": "A taxonomy of mutual information in medical image registration",
                "reference_link": "publication/304816684_A_taxonomy_of_mutual_information_in_medical_image_registration",
                "reference_type": "Conference Paper",
                "reference_date": "May 2016",
                "reference_abstract": null
            },
            {
                "reference_title": "Development and validations of a deformable registration algorithm for medical images: Applications to brain, breast and prostate studies",
                "reference_link": "publication/304017360_Development_and_validations_of_a_deformable_registration_algorithm_for_medical_images_Applications_to_brain_breast_and_prostate_studies",
                "reference_type": "Article",
                "reference_date": "Jan 2012",
                "reference_abstract": "This dissertation presents work on deformable registration of medical images. Deformable registration is a fundamental problem in medical image computing, and is central in analytic methods for understanding population trends of imaging phenotypes, for measuring longitudinal change, for fusing multi-modality information, and for capturing structure-function correlations. ^ Despite over 20 years of extensive research and technology innovations, two limitations still exist in the literature of image registration, which are central in this dissertation. The first one is the ambiguity in determining anatomical correspondences between images. This is caused by the fact that most registration methods match images based on image intensities. However, intensity alone does not necessarily represent the anatomical or geometric context in the images. The second challenge is the problem of missing correspondences. This arises from the presence of pathologies in images, whose correspondences are not present in the images of healthy subjects. Moreover, many registration methods are fine-tuned to a particular problem, thereby losing generality. The proposed work contributes towards overcoming these limitations. It reduces ambiguity by matching voxels based on their geometric contexts instead of intensities alone. It handles missing correspondences by a newly developed mutual-saliency metric, which automatically identifies the regions having difficulty finding correspondences in the other image, and reduces their negative impact. The proposed method is designed in a general way in that it does not rely on any task-specific annotations of tissue, structure or feature points. ^ Extensive experiments are presented to demonstrate generality, accuracy and robustness, which are the key properties of the proposed method. Experiments involve images of various organs (brain, breast, heart) in various registration settings (cross-subject, longitudinal) on various public and in-house datasets, and include quantitative comparisons with many state-of-the-art methods. ^ To demonstrate the wide application of the proposed method in clinical and research studies, five topics utilizing the proposed method are presented. They are examples of population studies, longitudinal studies, and atlas-based segmentations. ^ The proposed method is fully-implemented, extensively-tested and publicly-released to meet the growing needs of many large-scale clinical and academic studies."
            }
        ]
    },
    {
        "title": "Game theory and its place in socioeconomic life",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (7)",
        "abstract": "Game theory is a mathematical model of the interaction between independent agents who want to maximize their interests. The application of the theory is evaluated through the mathematical models of the computer, which is the tool of digital technology. In this respect, theory is increasingly being applied to include disciplines as diverse as political science, biology, psychology, economics, linguistics, sociology, and computer science. In game theory, individuals are accepted as players and take steps by taking each other's moves into account in the decision-making process. The theory is based on how to achieve rational decision-making by modeling these behaviors.",
        "reference": [
            {
                "reference_title": "On Modal Logic Interpretations of Games.",
                "reference_link": "publication/220838082_On_Modal_Logic_Interpretations_of_Games",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2002",
                "reference_abstract": "Multi-agent environments comprise decision makers whose deliberations involve reasoning about the expected behav- ior of other agents. Apposite concepts of rational choice have been studied and formalized in game theory and our particular interest is with their integration in a logical specification language for multi- agent systems. This paper concerns the logical analysis of the game- theoretical notions of a (subgame perfect) Nash equilibrium and that of a (subgame perfect) best response strategy. Extensive forms of games are conceived of as Kripke frames and a version of Proposi- tional Dynamic Logic is employed to describe them. We show how formula schemes of our language characterize those classes of frames in which the strategic choices of the agents can be said to be Nash- optimal. Our analysis focuses on extensive games of perfect infor- mation without repetition."
            },
            {
                "reference_title": "Toward a Theory of Play: A Logical Perspective on Games and Interaction",
                "reference_link": "publication/49616487_Toward_a_Theory_of_Play_A_Logical_Perspective_on_Games_and_Interaction",
                "reference_type": "Article",
                "reference_date": "Dec 2011",
                "reference_abstract": "Logic and game theory have had a few decades of contacts by now, with the classical results of epistemic game theory as major high-lights. In this paper, we emphasize a recent new perspective toward \u201clogical dynamics\u201d, designing logical systems that focus on the actions that change information, preference, and other driving forces of agency. We show how this dynamic turn works out for games, drawing on some recent advances in the literature. Our key examples are the long-term dynamics of information exchange, as well as the much-discussed issue of extensive game rationality. Our paper also proposes a new broader interpretation of what is happening here. The combination of logic and game theory provides a fine-grained perspective on information and interaction dynamics, and we are witnessing the birth of something new which is not just logic, nor just game theory, but rather a Theory of Play."
            },
            {
                "reference_title": "Modal logic and game theory: Two alternative approaches",
                "reference_link": "publication/4857212_Modal_logic_and_game_theory_Two_alternative_approaches",
                "reference_type": "Article",
                "reference_date": "Dec 2002",
                "reference_abstract": "Two views of game theory are discussed: (1) game theory as a description of the behavior of rational individuals who recognize each other's rationality and reasoning abilities, and (2) game theory as an internally consistent recommendation to individuals on how to act in interactive situations. It is shown that the same mathematical tool, namely modal logic, can be used to explicitly model both views."
            },
            {
                "reference_title": "An Overview of Game Theory and Some Applications",
                "reference_link": "publication/326903669_An_Overview_of_Game_Theory_and_Some_Applications",
                "reference_type": "Article",
                "reference_date": "Aug 2018",
                "reference_abstract": "p>Abstract not available\n\nPhilosophy and Progress, Vol#59-60-; No#1-2; Jan-Dec 2016</p"
            },
            {
                "reference_title": "An experimental analysis of ultimatum bargaining: An experimental study",
                "reference_link": "publication/306535260_An_experimental_analysis_of_ultimatum_bargaining_An_experimental_study",
                "reference_type": "Article",
                "reference_date": "Jan 1982",
                "reference_abstract": null
            },
            {
                "reference_title": "20 Modal logic for games and information",
                "reference_link": "publication/251467772_20_Modal_logic_for_games_and_information",
                "reference_type": "Article",
                "reference_date": "Dec 2007",
                "reference_abstract": "Game-theoretic ideas have long played an influential rule in analyzing various branches of logic. This chapter focuses on using modal logics to describe and reason about games. It also investigates the uses of modal logic. Some structure on the various strands of research, to create an organization, which highlights the essential lines of research are focused. The chapter also discusses modeling imperfect information and multi-agent information update through dynamic epistemic logics; reasoning about game structure through operations for combining games; and logics of collective action and the power of coalitions of agents over time. Game trees can be viewed as Kripke models, where the possible moves are modeled by an accessibility relation and additional information about payoffs and turn taking are encoded by propositional atoms. Structural equivalence notions such as bisimulation turn into game equivalence notions, and extensions of the modal language, which can capture game-theoretic solution concepts such as the subgame-perfect equilibrium are investigated."
            },
            {
                "reference_title": "Handbook of Modal Logic",
                "reference_link": "publication/50852994_Handbook_of_Modal_Logic",
                "reference_type": "Article",
                "reference_date": "Jan 2006",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Toward understanding the conditions that promote higher attention in software developments -- a first step on music and standups",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (43)",
        "abstract": "Nowadays, Computer Science tightly entered all spheres of human activity. To improve quality and speed of development process, it is important to help programmers improve their working conditions. This paper proposes a vision on exploring this issue and presents in conjunction a factor that has been claimed multiple time to affect the effectiveness of software production, concentration and attention of software developers. We choose to focus on developers brain activity and features that can be extracted from it.",
        "reference": [
            {
                "reference_title": "The critical importance of meetings to leader and organizational success",
                "reference_link": "publication/319246903_The_critical_importance_of_meetings_to_leader_and_organizational_success",
                "reference_type": "Article",
                "reference_date": "Aug 2017",
                "reference_abstract": null
            },
            {
                "reference_title": "Attention Recognition in EEG-Based Affective Learning Research Using CFS+KNN Algorithm",
                "reference_link": "publication/309030502_Attention_Recognition_in_EEG-Based_Affective_Learning_Research_Using_CFSKNN_Algorithm",
                "reference_type": "Article",
                "reference_date": "Oct 2016",
                "reference_abstract": "The research detailed in this paper focuses on the processing of Electroencephalography (EEG) data to identify attention during the learning process. The identification of affect using our procedures is integrated into a simulated distance learning system that provides feedback to the user with respect to attention and concentration. The authors propose a classification procedure that combines correlation-based feature selection (CFS) and a k-nearest-neighbor (KNN) data mining algorithm. To evaluate the CFS+KNN algorithm, it was test against CFS+C4.5 algorithm and other classification algorithms. The classification performance was measured 10 times with different 3-fold cross validation data. The data was derived from 10 subjects while they were attempting to learn material in a simulated distance learning environment. A self-assessment model of self-report was used with a single valence to evaluate attention on 3 levels (high, neutral, low). It was found that CFS+KNN had a much better performance, giving the highest correct classification rate (CCR) of 80:84 3:0% for the valence dimension divided into 3 classes."
            },
            {
                "reference_title": "Effect of auditory stimulus in EEG signal using a Brain-Computer Interface",
                "reference_link": "publication/308823425_Effect_of_auditory_stimulus_in_EEG_signal_using_a_Brain-Computer_Interface",
                "reference_type": "Conference Paper",
                "reference_date": "Jun 2015",
                "reference_abstract": "The neuroscience research which related to the study of sounds that affected to the person and focused on create the system that observe the effects of binaural beat and instrumental relaxing music by analyzing the recorded Electroencephalogram (EEG) signal of the participants. In addition, the Brain-Computer Interface (BCI) is widely used for recording electrical brain wave signal readings and distinguishes the complexity of the EEG brain spectrum between the different EEG frequencies band including Delta, Theta, Alpha, Beta and Gamma. Moreover, there are 40 subjects was participated in this research. The statistic-based method Hypothesis Testing and Paired T-test were selected as a decision maker to analyze the EEG recorded from the participants using the Brain-Computer Interface to find the significant difference of each stimulus for every groups that separate by the stress level of participants which measured by using a Depression Anxiety and Stress Scales (DASS) questionnaire."
            },
            {
                "reference_title": "Cooperation, Collaboration and Pair-Programming: Field Studies on Backup Behavior",
                "reference_link": "publication/259992733_Cooperation_Collaboration_and_Pair-Programming_Field_Studies_on_Backup_Behavior",
                "reference_type": "Article",
                "reference_date": "May 2014",
                "reference_abstract": "Considering that pair programming has been extensively studied for more than a decade, it can seem quite surprising that there is such a lack of consensus on both its best use and its benefits. We argue that pair programming is not a replacement of usual developer interactions, but rather a formalization and enhancement of naturally occurring interactions. Consequently, we study and classify a broader range of developer interactions, evaluating them for type, purpose and patterns of occurrence, with the aim to identify situations in which pair programming is likely to be truly needed and thus most beneficial. We study the concrete pair programming practices in both academic and industrial settings. All interactions between teammates were recorded as Backup Behavior activities. In each of these two projects, developers were free to interact when needed. All team interactions were self-recorded by the teammates. The analysis of the interaction tokens show two salient features: solo work is an important component of teamwork and team interactions have two main purposes, namely cooperation and collaboration. Cooperative backup behavior occurs when a developer provides help to a teammate. Collaborative backup behavior occurs when the teammates are sharing the same goal toward solving an issue. We found that collaborative Backup Behavior, which occurred much less often, is close to the formal definition of pair programming. This study suggests that mandatory pair programming may be less efficient in organizations where solo work could be done and when some interactions are for cooperative activities. Based on these results, we discussed the potential implications concerning the best use of pair programming in practice, a more effective evaluation of its use, its potential benefits and emerging directions of future research."
            },
            {
                "reference_title": "A Method for Characterizing Energy Consumption in Android Smartphones",
                "reference_link": "publication/258432492_A_Method_for_Characterizing_Energy_Consumption_in_Android_Smartphones",
                "reference_type": "Conference Paper",
                "reference_date": "May 2013",
                "reference_abstract": "Cellular phones and tablets are ubiquitous, with a market penetration that is counted in millions of active users and units sold. The increasing computing capabilities and strict autonomy requirements on mobile devices drive a particular concern on energy utilization and optimization of this kind of equipment. In this paper, we investigate an approach to relate the energy consumption of smartphones with the operational status of the device, surveying parameters exposed by the operating system using an Android application. Our goal is to explore the means to expand the information that may help to produce more reliable measurements that can be used in further research for designing energy optimization profiles for mobile devices and identify optimization needs."
            },
            {
                "reference_title": "A multivariate classification of open source developers",
                "reference_link": "publication/256720982_A_multivariate_classification_of_open_source_developers",
                "reference_type": "Article",
                "reference_date": "Feb 2013",
                "reference_abstract": "Open source software development is becoming always more relevant. Understanding the behavior of developers in open source software projects and identifying the kinds of their contributions is an essential step to improve the efficiency of the development process and to organize the development teams more effectively. Moreover, understanding the level of participation of the different developers helps to understand which members of the development team are more important than others and who are the actual key developers. This paper investigates the behavior of open source developers and the structure of the development of open source projects through the analysis of a very large dataset: 10 well-known and widely used open source software projects for a total of more than 4 MLOC (millions of lines of code) modified distributed in more than 200 K versions. This study builds on the top of other studies in this area applying a set of rigorous statistical techniques, analyzing how developers contribute to the projects. Its novelty is in the fine gain analysis of the developers that have commit rights on the repository of the project they work on, in the automated identification of key contributors of the project, in the size of the analyzed datasets, and in the statistical techniques used to classify the behavior of the developers in an automated way. To collect such large volume of data and to ensure their integrity, a tool to automatically mine open source version control systems has been used. The main result of this study is the identification of a recurrent pattern of four kinds of contributors with the same characteristics in all the projects analyzed even if the projects are very different in domain, size, language, etc."
            },
            {
                "reference_title": "Frontal EEG theta/beta ratio during mind wandering episodes",
                "reference_link": "publication/329021782_Frontal_EEG_thetabeta_ratio_during_mind_wandering_episodes",
                "reference_type": "Article",
                "reference_date": "Nov 2018",
                "reference_abstract": "Background: \nIn resting-state EEG, the ratio between frontal power in the slow theta frequency band and the fast beta frequency band (the theta/beta ratio, TBR) has previously been negatively related to attentional control. Also, increased theta and reduced beta power were observed during mind wandering (MW) compared to episodes of focused attention. Thus, increased resting-state frontal TBR could be related to MW, suggesting that previously observed relationships between TBR and attentional control could reflect MW episodes increasing the average resting state TBR in people with low attentional control.\n\nGoals: \nTo replicate and extend the previous theta and beta MW effects for frontal TBR recordings and test if MW related changes in frontal TBR are related to attentional control.\n\nMethod: \nTwenty-six healthy participants performed a 40-minute breath-counting task, after a baseline EEG recording, while EEG was measured and participants indicated MW episodes with button presses.\n\nResults: \nFrontal TBR was significantly higher during MW episodes than during on-task periods. However, no relation between frontal TBR and attentional control was found.\n\nConclusions: \nThis confirms that frontal TBR varies with MW, which is thought to reflect, among other things, a state of reduced top-down attentional control over thoughts. 194 words."
            },
            {
                "reference_title": "Competitive Dynamics Research: Critique and Future Directions",
                "reference_link": "publication/324525534_Competitive_Dynamics_Research_Critique_and_Future_Directions",
                "reference_type": "Chapter",
                "reference_date": "Feb 2008",
                "reference_abstract": "A series of actions (moves) and reactions (countermoves) among firms in an industry create competitive dynamics. These action/reaction dynamics reflect the normal and innovative movement of firms in pursuit of profits. Firms act creatively (introduce a new product, a new promotion, or a new marketing agreement! to enhance or improve profits, competitive advantage, and industry position; successful actions (actions which generate new customers and profits) promote competitive reaction as rivals attempt to block or imitate the action. The study of competitive dynamics is thus the study of how firm action (moves) affects competitors, competitive advantage, and performance. Sometimes these actions and reactions can escalate among firms so that the industry performance is adversely affected; at other times, the pattern of behavior can be more gentlemanly and profitable."
            },
            {
                "reference_title": "Cognitive task difficulty analysis using EEG and data mining",
                "reference_link": "publication/320654131_Cognitive_task_difficulty_analysis_using_EEG_and_data_mining",
                "reference_type": "Conference Paper",
                "reference_date": "Mar 2017",
                "reference_abstract": null
            },
            {
                "reference_title": "Lean Software Development in Action",
                "reference_link": "publication/287338519_Lean_Software_Development_in_Action",
                "reference_type": "Book",
                "reference_date": "Jun 2014",
                "reference_abstract": "This book illustrates how goal-oriented, automated measurement can be used to create Lean organizations and to facilitate the development of Lean software, while also demonstrating the practical implementation of Lean software development by combining tried and trusted tools. In order to be successful, a Lean orientation of software development has to go hand in hand with a company's overall business strategy. To achieve this, two interrelated aspects require special attention: measurement and experience management. In this book, Janes and Succi provide the necessary knowledge to establish \"Lean software company thinking,\" while also exploiting the latest approaches to software measurement. A comprehensive, company-wide measurement approach is exactly what companies need in order to align their activities to the demands of their stakeholders, to their business strategy, etc. With the automatic, non-invasive measurement approach proposed in this book, even small and medium-sized enterprises that do not have the resources to introduce heavyweight processes will be able to make their software development processes considerably more Lean. The book is divided into three parts. Part I, \"Motivation for Lean Software Development,\" explains just what \"Lean Production\" means, why it can be advantageous to apply Lean concepts to software engineering, and which existing approaches are best suited to achieving this. Part II, \"The Pillars of Lean Software Development,\" presents the tools needed to achieve Lean software development: Non-invasive Measurement, the Goal Question Metric approach, and the Experience Factory. Finally, Part III, \"Lean Software Development in Action,\" shows how different tools can be combined to enable Lean Thinking in software development. The book primarily addresses the needs of all those working in the field of software engineering who want to understand how to establish an efficient and effective software development process. This group includes developers, managers, and students pursuing an M.Sc. degree in software engineering. \u00a9 Springer-Verlag Berlin Heidelberg 2014. All rights are reserved."
            }
        ]
    },
    {
        "title": "Ethical implications of emotion mining in medicine",
        "date": "December 2020",
        "doi": "10.1016/j.hlpt.2020.11.006",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (51)",
        "abstract": "The tools of emotion mining were developed in the field of computer science to detect and evaluate human emotions. In recent years we have seen an evolution of novel health technologies that utilise data-driven emotion mining for mental health diagnostics and assessment. Our paper describes the new ethical scenarios posed by these models through a series of case studies and discusses how existing ethics frameworks can be adapted to meet these new challenges.",
        "reference": [
            {
                "reference_title": "What do medical students actually need to know about artificial intelligence?",
                "reference_link": "publication/342312492_What_do_medical_students_actually_need_to_know_about_artificial_intelligence",
                "reference_type": "Article",
                "reference_date": "Dec 2020",
                "reference_abstract": "With emerging innovations in artificial intelligence (AI) poised to substantially impact medical practice, interest in training current and future physicians about the technology is growing. Alongside comes the question of what, precisely, should medical students be taught. While competencies for the clinical usage of AI are broadly similar to those for any other novel technology, there are qualitative differences of critical importance to concerns regarding explainability, health equity, and data security. Drawing on experiences at the University of Toronto Faculty of Medicine and MIT Critical Data\u2019s \u201cdatathons\u201d, the authors advocate for a dual-focused approach: combining robust data science-focused additions to baseline health research curricula and extracurricular programs to cultivate leadership in this space."
            },
            {
                "reference_title": "Toward clinical digital phenotyping: a timely opportunity to consider purpose, quality, and safety",
                "reference_link": "publication/335672073_Toward_clinical_digital_phenotyping_a_timely_opportunity_to_consider_purpose_quality_and_safety",
                "reference_type": "Article",
                "reference_date": "Sep 2019",
                "reference_abstract": "The use of data generated passively by personal electronic devices, such as smartphones, to measure human function in health and disease has generated significant research interest. Particularly in psychiatry, objective, continuous quantitation using patients\u2019 own devices may result in clinically useful markers that can be used to refine diagnostic processes, tailor treatment choices, improve condition monitoring for actionable outcomes, such as early signs of relapse, and develop new intervention models. If a principal goal for digital phenotyping is clinical improvement, research needs to attend now to factors that will help or hinder future clinical adoption. We identify four opportunities for research directed toward this goal: exploring intermediate outcomes and underlying disease mechanisms; focusing on purposes that are likely to be used in clinical practice; anticipating quality and safety barriers to adoption; and exploring the potential for digital personalized medicine arising from the integration of digital phenotyping and digital interventions. Clinical relevance also means explicitly addressing consumer needs, preferences, and acceptability as the ultimate users of digital phenotyping interventions. There is a risk that, without such considerations, the potential benefits of digital phenotyping are delayed or not realized because approaches that are feasible for application in healthcare, and the evidence required to support clinical commissioning, are not developed. Practical steps to accelerate this research agenda include the further development of digital phenotyping technology platforms focusing on scalability and equity, establishing shared data repositories and common data standards, and fostering multidisciplinary collaborations between clinical stakeholders (including patients), computer scientists, and researchers."
            },
            {
                "reference_title": "The Future of Digital Psychiatry",
                "reference_link": "publication/335147588_The_Future_of_Digital_Psychiatry",
                "reference_type": "Article",
                "reference_date": "Aug 2019",
                "reference_abstract": "Purpose of Review\nTreatments in psychiatry have been rapidly changing over the last century, following the development of psychopharmacology and new research achievements. However, with advances in technology, the practice of psychiatry in the future will likely be influenced by new trends based on computerized approaches and digital communication. We examined four major areas that will probably impact on the clinical practice in the next few years: telepsychiatry; social media; mobile applications and internet of things; artificial intelligence; and machine learning.\n\nRecent Findings\nDevelopments in these four areas will benefit patients throughout the journey of the illness, encompassing early diagnosis, even before the patients present to a clinician; personalized treatment on demand at anytime and anywhere; better prediction on patient outcomes; and even how mental illnesses are diagnosed in the future.\n\nSummary\nThough the evidence for many technology-based interventions or mobile applications is still insufficient, it is likely that such advances in technology will play a larger role in the way that patient receives mental health interventions in the future, leading to easier access to them and improved outcomes."
            },
            {
                "reference_title": "Artificial intelligence: opportunities and risks for public health",
                "reference_link": "publication/332965889_Artificial_intelligence_opportunities_and_risks_for_public_health",
                "reference_type": "Article",
                "reference_date": "May 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Risk Assessment Tools and Data-Driven Approaches for Predicting and Preventing Suicidal Behavior",
                "reference_link": "publication/331076315_Risk_Assessment_Tools_and_Data-Driven_Approaches_for_Predicting_and_Preventing_Suicidal_Behavior",
                "reference_type": "Article",
                "reference_date": "Feb 2019",
                "reference_abstract": "Risk assessment of suicidal behavior is a time-consuming but notoriously inaccurate activity for mental health services globally. In the last 50 years a large number of tools have been designed for suicide risk assessment, and tested in a wide variety of populations, but studies show that these tools suffer from low positive predictive values. More recently, advances in research fields such as machine learning and natural language processing applied on large datasets have shown promising results for health care, and may enable an important shift in advancing precision medicine. In this conceptual review, we discuss established risk assessment tools and examples of novel data-driven approaches that have been used for identification of suicidal behavior and risk. We provide a perspective on the strengths and weaknesses of these applications to mental health-related data, and suggest research directions to enable improvement in clinical practice."
            },
            {
                "reference_title": "Artificial intelligence, bias and clinical safety",
                "reference_link": "publication/330347947_Artificial_intelligence_bias_and_clinical_safety",
                "reference_type": "Article",
                "reference_date": "Jan 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Hidden in Plain Sight \u2014 Reconsidering the Use of Race Correction in Clinical Algorithms",
                "reference_link": "publication/342250521_Hidden_in_Plain_Sight_-_Reconsidering_the_Use_of_Race_Correction_in_Clinical_Algorithms",
                "reference_type": "Article",
                "reference_date": "Jun 2020",
                "reference_abstract": "Physicians still lack consensus on the meaning of race. When the Journal took up the topic in 2003 with a debate about the role of race in medicine, one side argued that racial and ethnic categories reflected underlying population genetics and could be clinically useful.1 Others held that any small benefit was outweighed by potential harms that arose from the long, rotten history of racism in medicine.2 Weighing the two sides, the accompanying Perspective article concluded that though the concept of race was \"fraught with sensitivities and fueled by past abuses and the potential for future abuses,\" race-based medicine still had potential: \"it seems unwise to abandon the practice of recording race when we have barely begun to understand the architecture of the human genome.\"3 The next year, a randomized trial showed that a combination of hydralazine and isosorbide dinitrate reduced mortality due to heart failure among patients who identified themselves as black. The Food and Drug Administration granted a race-specific indication for that product, BiDil, in 2005.4 Even though BiDil's ultimate commercial failure cast doubt on race-based medicine, it did not lay the approach to rest. Prominent geneticists have repeatedly called on physicians to take race seriously,5,6 while distinguished social scientists vehemently contest these calls.7,8 Our understanding of race and human genetics has advanced considerably since 2003, yet these insights have not led to clear guidelines on the use of race in medicine. The result is ongoing conflict between the latest insights from population genetics and the clinical implementation of race. For example, despite mounting evidence that race is not a reliable proxy for genetic difference, the belief that it is has become embedded, sometimes insidiously, within medical practice. One subtle insertion of race into medicine involves diagnostic algorithms and practice guidelines that adjust or \"correct\" their outputs on the basis of a patient's race or ethnicity. Physicians use these algorithms to individualize risk assessment and guide clinical decisions. By embedding race into the basic data and decisions of health care, these algorithms propagate race-based medicine. Many of these race-adjusted algorithms guide decisions in ways that may direct more attention or resources to white patients than to members of racial and ethnic minorities. To illustrate the potential dangers of such practices, we have compiled a partial list of raceadjusted algorithms (Table 1). We explore several of them in detail here. Given their potential to perpetuate or even amplify race-based health inequities, they merit thorough scrutiny."
            },
            {
                "reference_title": "Enhancing validity in psychological research",
                "reference_link": "publication/337924401_Enhancing_validity_in_psychological_research",
                "reference_type": "Article",
                "reference_date": "Dec 2019",
                "reference_abstract": "Methods to increase Campbell's (1957) internal and external validity as well as Cook and Campbell's (1979) construct and conclusion validity are reviewed. For internal validity or valid causal inference, designs and methods to draw causal conclusions from nonrandomized studies are considered. Greater collaboration between the causal inference and structural equation modeling traditions would benefit both. For external validity, generalizing results, treating partners and studies as well as participants as random is strongly encouraged. For construct validity, particularly the psychological meaning of measures, multivariate models that treat measures from both overtime and dyadic data as being a combination of multiple constructs are discussed. For conclusion validity or valid statistical inference, the problem of low power when generalizability is high and the assumption of independence are discussed. Finding the truth in psychological research is a challenge, and seemingly insurmountable difficulties are often encountered. Nonetheless, persistent and diligent efforts using strategies developed by generations of methodologists should lead to scientific advancement. (PsycINFO Database Record (c) 2019 APA, all rights reserved)."
            },
            {
                "reference_title": "Mitigating Gender Bias in Natural Language Processing: Literature Review",
                "reference_link": "publication/335783455_Mitigating_Gender_Bias_in_Natural_Language_Processing_Literature_Review",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Digital phenotyping: a global tool for psychiatry",
                "reference_link": "publication/327512548_Digital_phenotyping_a_global_tool_for_psychiatry",
                "reference_type": "Article",
                "reference_date": "Oct 2018",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Automatic Removal of Identifying Information in Official EU Languages for Public Administrations: The MAPA Project",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (12)",
        "abstract": null,
        "reference": [
            {
                "reference_title": "Use and Understanding of Anonymization and De-Identification in the Biomedical Literature: Scoping Review",
                "reference_link": "publication/332693863_Use_and_Understanding_of_Anonymization_and_De-Identification_in_the_Biomedical_Literature_Scoping_Review",
                "reference_type": "Article",
                "reference_date": "May 2019",
                "reference_abstract": "Background: The secondary use of health data is central to biomedical research in the era of data science and precision medicine. National and international initiatives, such as the Global Open Findable, Accessible, Interoperable, and Reusable (GO FAIR) initiative, are supporting this approach in different ways (eg, making the sharing of research data mandatory or improving the legal and ethical frameworks). Preserving patients\u2019 privacy is crucial in this context. De-identification and anonymization are the two most common terms used to refer to the technical approaches that protect privacy and facilitate the secondary use of health data. However, it is difficult to find a consensus on the definitions of the concepts or on the reliability of the techniques used to apply them. A comprehensive review is needed to better understand the domain, its capabilities, its challenges, and the ratio of risk between the data subjects\u2019 privacy on one side, and the benefit of scientific advances on the other.\n\nObjective: This work aims at better understanding how the research community comprehends and defines the concepts of de-identification and anonymization. A rich overview should also provide insights into the use and reliability of the methods. Six aspects will be studied: (1) terminology and definitions, (2) backgrounds and places of work of the researchers, (3) reasons for anonymizing or de-identifying health data, (4) limitations of the techniques, (5) legal and ethical aspects, and (6) recommendations of the researchers.\n\nMethods: Based on a scoping review protocol designed a priori, MEDLINE was searched for publications discussing de-identification or anonymization and published between 2007 and 2017. The search was restricted to MEDLINE to focus on the life sciences community. The screening process was performed by two reviewers independently.\n\nResults: After searching 7972 records that matched at least one search term, 135 publications were screened and 60 full-text articles were included. (1) Terminology: Definitions of the terms de-identification and anonymization were provided in less than half of the articles (29/60, 48%). When both terms were used (41/60, 68%), their meanings divided the authors into two equal groups (19/60, 32%, each) with opposed views. The remaining articles (3/60, 5%) were equivocal. (2) Backgrounds and locations: Research groups were based predominantly in North America (31/60, 52%) and in the European Union (22/60, 37%). The authors came from 19 different domains; computer science (91/248, 36.7%), biomedical informatics (47/248, 19.0%), and medicine (38/248, 15.3%) were the most prevalent ones. (3) Purpose: The main reason declared for applying these techniques is to facilitate biomedical research. (4) Limitations: Progress is made on specific techniques but, overall, limitations remain numerous. (5) Legal and ethical aspects: Differences exist between nations in the definitions, approaches, and legal practices. (6) Recommendations: The combination of organizational, legal, ethical, and technical approaches is necessary to protect health data.\n\nConclusions: Interest is growing for privacy-enhancing techniques in the life sciences community. This interest crosses scientific boundaries, involving primarily computer science, biomedical informatics, and medicine. The variability observed in the use of the terms de-identification and anonymization emphasizes the need for clearer definitions as well as for better education and dissemination of information on the subject. The same observation applies to the methods. Several legislations, such as the American Health Insurance Portability and Accountability Act (HIPAA) and the European General Data Protection Regulation (GDPR), regulate the domain. Using the definitions they provide could help address the variable use of these two concepts in the research community."
            },
            {
                "reference_title": "Automatic de-identification of textual documents in the electronic health record: A review of recent research",
                "reference_link": "publication/45460408_Automatic_de-identification_of_textual_documents_in_the_electronic_health_record_A_review_of_recent_research",
                "reference_type": "Article",
                "reference_date": "Aug 2010",
                "reference_abstract": "In the United States, the Health Insurance Portability and Accountability Act (HIPAA) protects the confidentiality of patient data and requires the informed consent of the patient and approval of the Internal Review Board to use data for research purposes, but these requirements can be waived if data is de-identified. For clinical data to be considered de-identified, the HIPAA \"Safe Harbor\" technique requires 18 data elements (called PHI: Protected Health Information) to be removed. The de-identification of narrative text documents is often realized manually, and requires significant resources. Well aware of these issues, several authors have investigated automated de-identification of narrative text documents from the electronic health record, and a review of recent research in this domain is presented here.\nThis review focuses on recently published research (after 1995), and includes relevant publications from bibliographic queries in PubMed, conference proceedings, the ACM Digital Library, and interesting publications referenced in already included papers.\nThe literature search returned more than 200 publications. The majority focused only on structured data de-identification instead of narrative text, on image de-identification, or described manual de-identification, and were therefore excluded. Finally, 18 publications describing automated text de-identification were selected for detailed analysis of the architecture and methods used, the types of PHI detected and removed, the external resources used, and the types of clinical documents targeted. All text de-identification systems aimed to identify and remove person names, and many included other types of PHI. Most systems used only one or two specific clinical document types, and were mostly based on two different groups of methodologies: pattern matching and machine learning. Many systems combined both approaches for different types of PHI, but the majority relied only on pattern matching, rules, and dictionaries.\nIn general, methods based on dictionaries performed better with PHI that is rarely mentioned in clinical text, but are more difficult to generalize. Methods based on machine learning tend to perform better, especially with PHI that is not mentioned in the dictionaries used. Finally, the issues of anonymization, sufficient performance, and \"over-scrubbing\" are discussed in this publication."
            },
            {
                "reference_title": "A Survey of Named Entity Recognition and Classification",
                "reference_link": "publication/44062524_A_Survey_of_Named_Entity_Recognition_and_Classification",
                "reference_type": "Article",
                "reference_date": "Aug 2007",
                "reference_abstract": "The term \u0093Named Entity\u0094, now widely used in Natural Language Processing, was coined for the Sixth Message Understanding Conference (MUC-6) (R. Grishman & Sundheim 1996). At that time, MUC was focusing on Information Extraction (IE) tasks where structured information of company activities and defense related activities is extracted from unstructured text, such as newspaper articles. In defining the task, people noticed that it is essential to recognize information units like names, including person, organization and location names, and numeric expressions including time, date, money and percent expressions. Identifying references to these entities in text was recognized as one of the important sub-tasks of IE and was called \u0093Named Entity Recognition and Classification (NERC)\u0094. Le terme \u00ab entit\u00e9 nomm\u00e9e \u00bb, maintenant largement utilis\u00e9 dans le cadre du traitement des langues naturelles, a \u00e9t\u00e9 adopt\u00e9 pour la Sixth Message Understanding Conference (MUC 6) (R. Grishman et Sundheim, 1996). \u00c0 cette \u00e9poque, la Conf\u00e9rence \u00e9tait concentr\u00e9e sur les t\u00e2ches d'extraction d'information (EI), dans lesquelles l'information structur\u00e9e relative aux activit\u00e9s des entreprises et aux activit\u00e9s li\u00e9es \u00e0 la d\u00e9fense sont extraites de texte non structur\u00e9, comme les articles de journaux. Au moment de d\u00e9finir cette t\u00e2che, on a remarqu\u00e9 qu'il est essentiel de reconna\u00eetre les unit\u00e9s d'information comme les noms (dont les noms de personnes, d'organisations et de lieux g\u00e9ographiques) et les expressions num\u00e9riques, notamment l'expression de l'heure, de la date, des sommes mon\u00e9taires et des pourcentages. On a alors conclu que l'identification des r\u00e9f\u00e9rences \u00e0 ces entit\u00e9s dans le texte \u00e9tait une des principales sous-t\u00e2ches de l'EI et on a alors nomm\u00e9 cette t\u00e2che Named Entity Recognition and Classification (NERC) (reconnaissance et classification d'entit\u00e9s nomm\u00e9es)."
            },
            {
                "reference_title": "The INCEpTION Platform: Machine-Assisted and Knowledge-Oriented Interactive Annotation",
                "reference_link": "publication/327671893_The_INCEpTION_Platform_Machine-Assisted_and_Knowledge-Oriented_Interactive_Annotation",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2018",
                "reference_abstract": "We introduce INCEpTION, a new annotation platform for tasks including interactive and semantic annotation (e.g., concept linking, fact linking, knowledge base population, semantic frame annotation). These tasks are very time consuming and demanding for annotators, especially when knowledge bases are used. We address these issues by developing an annotation platform that incorporates machine learning capabilities which actively assist and guide annotators. The platform is both generic and modular. It targets a range of research domains in need of semantic annotation, such as digital humanities, bioinformatics, or linguistics. INCEpTION is publicly available as open-source software.\n\nThe full paper is available here: http://aclweb.org/anthology/C18-2002"
            },
            {
                "reference_title": "Anonymisation de d\u00e9cisions de justice",
                "reference_link": "publication/228979520_Anonymisation_de_decisions_de_justice",
                "reference_type": "Article",
                "reference_date": null,
                "reference_abstract": "R\u00e9sum\u00e9 -Abstract La publication de d\u00e9cisions de justice sur le Web permet de rendre la jurisprudence accessible au grand public, mais il existe des domaines du droit pour lesquels la Loi pr\u00e9voit que l'identit\u00e9 de certaines personnes doit demeurer confidentielle. Nous d\u00e9veloppons actuellement un sys-t\u00e8me d'anonymisation automatique \u00e0 l'aide de l'environnement de d\u00e9veloppement GATE. Le syst\u00e8me doit reconna\u00eetre certaines entit\u00e9s nomm\u00e9es comme les noms de personne, les lieux et les noms d'entreprise, puis d\u00e9terminer automatiquement celles qui sont de nature \u00e0 permettre l'identification des personnes vis\u00e9es par les restrictions l\u00e9gales \u00e0 la publication. Publishing court decisions on the Web can make case law available to the general public, but the Law sometimes prohibits the disclosure of the identity of people named in decisions. We are currently developing an automatic anonymization system, using the GATE development envi-ronment. The tasks of the system are the recognition of some named entities like person names, locations and company names, then the automatic selection of the ones that may lead to the identification of people whose identities must be legally kept confidential."
            },
            {
                "reference_title": "Design of the MUC-6 evaluation",
                "reference_link": "publication/220825551_Design_of_the_MUC-6_evaluation",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 1995",
                "reference_abstract": "The sixth in a series of \"Message Understanding Conferences\", which are designed to promote and evaluate research in information extraction, was held last fall. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe the development of the \"message understanding\" task over the course of the prior MUCs, some of the motivations for the new format, and the steps which led up to the formal evaluation."
            },
            {
                "reference_title": "Evaluating the State-of-the-Art in Automatic De-identification",
                "reference_link": "publication/6237676_Evaluating_the_State-of-the-Art_in_Automatic_De-identification",
                "reference_type": "Article",
                "reference_date": "Jun 2007",
                "reference_abstract": "To facilitate and survey studies in automatic de-identification, as a part of the i2b2 (Informatics for Integrating Biology to the Bedside) project, authors organized a Natural Language Processing (NLP) challenge on automatically removing private health information (PHI) from medical discharge records. This manuscript provides an overview of this de-identification challenge, describes the data and the annotation process, explains the evaluation metrics, discusses the nature of the systems that addressed the challenge, analyzes the results of received system runs, and identifies directions for future research. The de-indentification challenge data consisted of discharge summaries drawn from the Partners Healthcare system. Authors prepared this data for the challenge by replacing authentic PHI with synthesized surrogates. To focus the challenge on non-dictionary-based de-identification methods, the data was enriched with out-of-vocabulary PHI surrogates, i.e., made up names. The data also included some PHI surrogates that were ambiguous with medical non-PHI terms. A total of seven teams participated in the challenge. Each team submitted up to three system runs, for a total of sixteen submissions. The authors used precision, recall, and F-measure to evaluate the submitted system runs based on their token-level and instance-level performance on the ground truth. The systems with the best performance scored above 98% in F-measure for all categories of PHI. Most out-of-vocabulary PHI could be identified accurately. However, identifying ambiguous PHI proved challenging. The performance of systems on the test data set is encouraging. Future evaluations of these systems will involve larger data sets from more heterogeneous sources."
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2018",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2018",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2002",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Further strengthening of upper bounds for perfect $k$-Hashing",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (14)",
        "abstract": "For a fixed integer $k$, a problem of relevant interest in computer science and combinatorics is that of determining the asymptotic growth, with $n$, of the largest set for which a perfect $k$-hash family of $n$ functions exists. Equivalently, determining the asymptotic growth of a largest subset of $\\{1,2,\\ldots,k\\}^n$ such that for any $k$ distinct elements in the set, there is a coordinate where they all differ. An important asymptotic upper bound for general $k$ was derived by Fredman and Koml\\'os in the '80s. Only very recently this was improved for general $k$ by Guruswami and Riazanov while stronger results for small values of $k$ were obtained by Arikan, by Dalai, Guruswami and Radhakrishnan and by Dalai and Costa. In this paper, we further improve the bounds for $5\\leq k \\leq 8$. The method we use, which depends on the reduction of an optimization problem to a finite number of cases, shows that further results might be obtained by refined arguments at the expense of higher complexity.",
        "reference": [
            {
                "reference_title": "An Improved Bound on the Zero-Error List-Decoding Capacity of the 4/3 Channel",
                "reference_link": "publication/334978551_An_Improved_Bound_on_the_Zero-Error_List-Decoding_Capacity_of_the_43_Channel",
                "reference_type": "Article",
                "reference_date": "Aug 2019",
                "reference_abstract": "We prove a new upper bound on the size of codes \n$C \\subseteq \\{1,2,3,4\\}^{n}$ \nwith the property that every four distinct codewords in \n$C$ \nhave a coordinate where they all differ. Specifically, we provide a self-contained proof that such codes have size at most \n$2^{6n/19 + o(n)}$ \n, that is, rate bounded asymptotically by 6/19 \u2264 0.3158 (measured in bits). This improves the previous best upper bound of 0.3512 due to (Arikan 1994), which in turn improved the 0.375 bound that followed from general bounds for perfect hashing due to (Fredman and Koml\u00f3s, 1984) and (K\u00f6rner and Marton, 1988). Finally, using a combination of our approach with a simple idea which exploits powerful bounds on the minimum distance of codes in the Hamming space, we further improve the upper bound to 0.31477."
            },
            {
                "reference_title": "Bounds on the Zero-Error List-Decoding Capacity of the q/(q-1) Channel",
                "reference_link": "publication/327089128_Bounds_on_the_Zero-Error_List-Decoding_Capacity_of_the_qq-1_Channel",
                "reference_type": "Conference Paper",
                "reference_date": "Jun 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "An improved bound on the zero-error list-decoding capacity of the 4/3 channel",
                "reference_link": "publication/319361224_An_improved_bound_on_the_zero-error_list-decoding_capacity_of_the_43_channel",
                "reference_type": "Conference Paper",
                "reference_date": "Jun 2017",
                "reference_abstract": null
            },
            {
                "reference_title": "Nombre minimal de contacts de fermeture necessaires pour realiser, une fonction booleenne sym\u00e9trique de n variables",
                "reference_link": "publication/266216736_Nombre_minimal_de_contacts_de_fermeture_necessaires_pour_realiser_une_fonction_booleenne_symetrique_de_n_variables",
                "reference_type": "Article",
                "reference_date": "Jan 1964",
                "reference_abstract": null
            },
            {
                "reference_title": "New Bounds for Perfect Hashing via Information Theory",
                "reference_link": "publication/262175616_New_Bounds_for_Perfect_Hashing_via_Information_Theory",
                "reference_type": "Article",
                "reference_date": "Nov 1988",
                "reference_abstract": "A set of sequences of length t from a b-element alphabet is called k-separated if for every k-tuple of the sequences there exists a coordinate in which they all differ. The problem of finding, for fixed t, b, and k, the largest size N(t, b, k) of a k-separated set of sequences is equivalent to finding the minimum size of a (b, k)-family of perfect hash functions for a set of a given size. We shall improve the bounds for N(t, b, k) obtained by Fredman and Koml\u00f3s [1]. K\u00f6rner [2] has shown that the proof in [1] can be reduced to an application of the sub-additivity of graph entropy [3]. He also pointed out that this sub-additivity yields a method to prove non-existence bounds for graph covering problems. Our new non-existence bound is based on an extension of graph entropy to hypergraphs."
            },
            {
                "reference_title": "On the Size of Separating Systems and Families of Perfect Hash Functions",
                "reference_link": "publication/250956222_On_the_Size_of_Separating_Systems_and_Families_of_Perfect_Hash_Functions",
                "reference_type": "Article",
                "reference_date": "Mar 1984",
                "reference_abstract": "This paper presents two applications of an interesting information theoretic theorem about graphs. The first application concerns the derivation of good bounds for the function $Y(b,k,n)$, which is defined to be the minimum size of a family of functions such that for every subset of size k from an n element universe, there exists a perfect hash function in the family mapping the subset into a table of size b. The second application concerns the derivation of good bounds for the function $M(i,j,n)$, which is defined to be the minimum size of an $(i,j)$-separating system."
            },
            {
                "reference_title": "Perfect Hashing and Probability",
                "reference_link": "publication/232011072_Perfect_Hashing_and_Probability",
                "reference_type": "Article",
                "reference_date": "Sep 1994",
                "reference_abstract": "A simple proof is given of the best-known upper bound on the cardinality of a set of vectors of length t over an alphabet of size b, with the property that, for every subset of k vectors, there is a coordinate in which they all differ. This question is motivated by the study of perfect hash functions."
            },
            {
                "reference_title": "A new upper bound on nonbinary block codes",
                "reference_link": "publication/220189000_A_new_upper_bound_on_nonbinary_block_codes",
                "reference_type": "Article",
                "reference_date": "Aug 1990",
                "reference_abstract": "An upper bound on the information rate of nonbinary block codes is derived via linear programming in the nonbinary Johnson scheme. Combined with the general cross-section method this new result gives the best presently known asymptotic upper bound on nonbinary block codes."
            },
            {
                "reference_title": "Upper bound on the zero-error list-coding capacity",
                "reference_link": "publication/3078572_Upper_bound_on_the_zero-error_list-coding_capacity",
                "reference_type": "Article",
                "reference_date": "Aug 1994",
                "reference_abstract": "Presents an upper bound on the zero-error list-coding capacity of\ndiscrete memoryless channels. Using this bound, we show that the list-3\ncapacity of the 4/3 channel is at most 0.3512 bits, improving the best\nprevious bound. The relation of the bound to earlier similar bounds, in\nparticular, to Korner's (1986) graph-entropy bound, is discussed"
            },
            {
                "reference_title": "Zero error capacity under list decoding",
                "reference_link": "publication/3077299_Zero_error_capacity_under_list_decoding",
                "reference_type": "Article",
                "reference_date": "Oct 1988",
                "reference_abstract": "Shannon's zero-error channel capacities C <sub>OF</sub>,\nC <sub>O</sub> with and without noiseless feedback are\ngeneralized to list decoding: the receiver lists L messages,\nerring if the correct one is not listed. The corresponding capacities\nC <sub>OF</sub>( L ), C <sub>O</sub>( L )\nare nondecreasing in L . For an I -letter input\nalphabet, C <sub>OF</sub>( L ) attains its maximum at\nL = I -1. A lower bound to C <sub>O</sub>( L\n) approaches that maximum as L increases"
            }
        ]
    },
    {
        "title": "Design review of MOOCs: application of e-learning design principles",
        "date": "December 2020",
        "doi": "10.1007/s12528-019-09243-w",
        "conferance": null,
        "citations_count": "Citations (8)",
        "reference_count": "References (69)",
        "abstract": "The purpose of this study is to explore the pedagogical design of massive open online courses (MOOCs) using evidence-based e-learning principles. MOOCs have become an important part of discourse in higher education. However, there has been shared concern on the quality of MOOCs as learning systems for engaging learners as well as fulfilling their needs. The researchers conducted a design review of 40 computer science MOOCs from two major MOOC providers. The findings indicate a relatively low application of the principles in general, with the exception of those related to the organization and presentation of content. MOOC platforms and the difficulty level of MOOCs used the application of e-learning principles and guidelines differently. Implications for future research and design of MOOCs are discussed.",
        "reference": [
            {
                "reference_title": "Quality Frameworks and Learning Design for Open Education",
                "reference_link": "publication/332785862_Quality_Frameworks_and_Learning_Design_for_Open_Education",
                "reference_type": "Article",
                "reference_date": "Apr 2019",
                "reference_abstract": "This article discusses the need to innovate education due to global changes to keep its status as a human right and public good and introduces Open Education as a theory to fulfil these requirements. A systematic literature review confirms the hypothesis that a holistic quality framework for Open Education does not exist. For its development, a brief history and definition of Open Education are provided first. It is argued that Open Education improves learning quality through the facilitation of innovative learning designs and processes. Therefore, sources of learning quality and dimensions of quality development are discussed. To support the improvement of the learning quality and design of Open Education, the Reference Process Model of ISO/IEC 40180 (former ISO/IEC 19796-1) is introduced and modified for Open Education. Adapting the three quality dimensions and applying the macro, meso, and micro levels, the OpenEd Quality Framework is developed. This framework combines and integrates the different quality perspectives in a holistic approach that is mapping them to the learning design, processes, and results. Finally, this article illustrates potential adaptations and benefits of the OpenEd Quality Framework. The OpenEd Quality Framework can be used in combination with other tools to address the complexity of and to increase the quality and impact of Open Education. To summarize, the OpenEd Quality Framework serves to facilitate and foster future improvement of the learning design and quality of Open Education."
            },
            {
                "reference_title": "Evaluating MOOCs According to Instructional Design Principles",
                "reference_link": "publication/330010198_Evaluating_MOOCs_According_to_Instructional_Design_Principles",
                "reference_type": "Article",
                "reference_date": "Jun 2017",
                "reference_abstract": "The aim of this study was to evaluate the massive open online courses that gains popularity day by day in terms of the properties which learning environments should have. For this purpose, six courses, which were selected from a MOOCs platform called Udemy, were evaluated. The evaluation was carried out according to the online learning environment evaluation form, developed by the researchers. The evaluation form was created by reviewing the literature and taking expert opinions. The results of the study showed that all investigated courses met the overall criteria and paid courses had no advantage over free courses. In addition to many advantages of the courses, there were some limitations as inability to provide feedback on a regular basis by the instructor to students, to be accessible for physically disadvantaged persons, to provide opportunities for resource sharing among students, automatically provide personalized learning options to users and to provide the contact information of other students."
            },
            {
                "reference_title": "Instructors\u2019 Experience of Designing MOOCs in Higher Education: Considerations and Challenges",
                "reference_link": "publication/329358211_Instructors'_Experience_of_Designing_MOOCs_in_Higher_Education_Considerations_and_Challenges",
                "reference_type": "Article",
                "reference_date": "Dec 2018",
                "reference_abstract": "As massive open online courses (MOOCs) increase, the large scale and heterogeneity of MOOC participants bring myriad significant design challenges. This mixed methods study explores 143 MOOC instructors\u2019 considerations and challenges in designing MOOCs; 12 of whom were interviewed and had their courses analyzed. The survey, interview, and course review data revealed a variety of considerations and challenges in MOOC design in terms of pedagogy, resources, and logistics. Pedagogical considerations included learning objectives, assessment methods, course length, course content, flexibility, and collaborative learning support. Resource considerations included the affordance of MOOC platforms, support from the host institution and the platform, and the available intellectual and hardware resources. Logistical considerations included the amount of time instructors spent designing the MOOC. The obstacles included pedagogical challenges (engaging students, increasing student interaction, and limited assessment methods), resource challenge (i.e., limitations associated with the affordances of the platform), and logistical challenge (time limitations for designing and developing MOOCs). To address these challenges, the instructors often relied on reviewing other MOOCs. They also sought help from colleagues, their universities, and supporters of the platforms."
            },
            {
                "reference_title": "Unpacking the Strategies of Ten Highly Rated MOOCs: Implications for Engaging Students in Large Online Courses",
                "reference_link": "publication/324559398_Unpacking_the_Strategies_of_Ten_Highly_Rated_MOOCs_Implications_for_Engaging_Students_in_Large_Online_Courses",
                "reference_type": "Article",
                "reference_date": "Apr 2018",
                "reference_abstract": "For an instructor who is interested in conducting large online courses, what factors related to course design and the individual instructors could engage their students? Which of these factors are more valued by the students? What specific strategies are used to implement these factors? To answer these questions, this paper examines the factors and strategies that could engage learners in 10 highly-rated massive open online courses (MOOCs) across different subject disciplines. Ten highly-rated MOOCs were sampled because they could exemplify good practice or teaching strategies. The features of each MOOC were examined in detail, along with a large-scale qualitative analysis of 4,466 participants\u2019 reflection data. The current study also investigated the reasons for students\u2019 disaffection, and students\u2019 suggestions for improvement. Findings suggest six key factors that can engage online students and nine reasons for student disaffection. Collectively, the findings and implications presented in this study provide practical guidelines to other instructors of large online courses. The findings may also offer tips for instructors of traditional e-learning courses. At the very least, the information presented in this study might suggest possible solutions for traditional e-learning courses that might otherwise been overlooked."
            },
            {
                "reference_title": "Instructional quality of massive open online courses: A review of attitudinal change MOOCs",
                "reference_link": "publication/321637741_Instructional_quality_of_massive_open_online_courses_A_review_of_attitudinal_change_MOOCs",
                "reference_type": "Article",
                "reference_date": "Jan 2017",
                "reference_abstract": "This study builds on prior research regarding attitudinal learning MOOCs, and a study examining the quality of MOOCs based on adherence to the first principles of instruction. Nine MOOCs designed for attitudinal instruction were reviewed to rate the degree they incorporated first principles of instruction and first principles of attitudinal instruction. The study found that while none of the MOOCs incorporated all of the first principles, overall, they incorporated the first principles of instruction more consistently than in the prior study. The review also showed that all of the courses did incorporate the attitudinal instruction first principles to some degree. This study provides researchers an approach to evaluating the instructional design quality of attitudinal instruction in general and MOOCs designed for attitudinal learning specifically, while also guiding practitioners in understanding how others have approached the design of attitudinal learning MOOCs."
            },
            {
                "reference_title": "The nature and level of learner\u2013learner interaction in a chemistry massive open online course (MOOC)",
                "reference_link": "publication/315819307_The_nature_and_level_of_learner-learner_interaction_in_a_chemistry_massive_open_online_course_MOOC",
                "reference_type": "Article",
                "reference_date": "Dec 2017",
                "reference_abstract": "Similar to other online courses, massive open online courses (MOOCs) often rely on learner\u2013learner interaction as a mechanism to promote learning. However, little is known at present about learner\u2013learner interaction in these nascent informal learning environments. While some studies have explored MOOC participant perceptions of learner\u2013learner interactions, research is still lacking regarding the content and level of such interactions. Using the interaction analysis model (IAM) as a theoretical framework and social network analysis methods, the present study investigates the nature and level of learner\u2013learner interaction within a popular Chemistry MOOC from Coursera. Findings suggest that learner\u2013learner interaction: was limited to lower phases of the IAM framework (e.g., sharing and comparing information); changed (decreased) over time; and was heavily dependent on a few highly-engaged learners. Potential implications for the design of future MOOCs are discussed."
            },
            {
                "reference_title": "MOOCs to university: a consumer goal and marketing perspective",
                "reference_link": "publication/315740189_MOOCs_to_university_a_consumer_goal_and_marketing_perspective",
                "reference_type": "Article",
                "reference_date": "Apr 2017",
                "reference_abstract": "n this paper we apply consumer goal theories to an educational context by examining how completion of a MOOC (Massive Open Online Course) may motivate enrolment in a university course. We contend that individuals who finish a MOOC are more likely to establish a new goal intention for university than those who do not finish. This new goal intention is likely to be prompted by the individual\u2019s satisfaction with their MOOC experience as well as a sense of discontent in not having fulfilled their broader educational goals. For those who do set a new goal for university study, we contend that the institute hosting the MOOC is likely to form part of the consideration set used by individuals to narrow down their choice of tertiary provider. Moreover, we argue that this same host institute is likely to be chosen from the consideration set where the MOOC experience is a satisfying one and where a strong link can be established between the pedagogical and delivery approaches used in both the MOOC and university settings. This research has implications for how tertiary institutes create and use MOOCs, and offers insights into how providers can more effectively market higher education courses to those progressing through a MOOC pathway."
            },
            {
                "reference_title": "A systematic review of research methods and topics of the empirical MOOC literature (2014\u20132016)",
                "reference_link": "publication/322544342_A_systematic_review_of_research_methods_and_topics_of_the_empirical_MOOC_literature_2014-2016",
                "reference_type": "Article",
                "reference_date": "Jan 2018",
                "reference_abstract": "This study explores the research paradigms and topics of MOOCs to gain a deeper understanding of the MOOC phenomenon by reviewing 146 empirical studies of MOOCs published from October 2014 to November 2016. The results show that: (a) most studies used quantitative research methods followed by mixed research methods and qualitative research methods, (b) the most frequently adopted data collection method was survey, followed by platform database, interviews, and discussion forum, (c) more than half of the collected studies used at least two data collection methods such as survey and interview, (d) the majority of researchers used descriptive statistics to analyze data, followed by inferential statistics and content analysis, and (e) the research focus was mainly on students, followed by design-focused, context and impact-focused, and instructor-focused. Among the foci of that research, learner retention and motivation were the most mentioned, followed by learner experience and satisfaction, assessment, and instructional design."
            },
            {
                "reference_title": "Technological Innovations in Large-Scale Teaching: Five Roots of Massive Open Online Courses",
                "reference_link": "publication/319471170_Technological_Innovations_in_Large-Scale_Teaching_Five_Roots_of_Massive_Open_Online_Courses",
                "reference_type": "Article",
                "reference_date": "Sep 2017",
                "reference_abstract": "There are millions of people worldwide\u2014of all ages, conditions, backgrounds, and motivations\u2014with significant learning needs. Unfortunately, traditional education is not efficient enough to meet these needs. That is, the available educational resources are not fully exploited to help cover the demand. There is an increasing need for large-scale access to cost-effective and high-quality education. The use of technological innovations for large-scale teaching might be part of the solution. In this context, the goals of this study were to identify technological innovations that can be considered historical milestones in large-scale teaching, to systematize experts\u2019 opinions about the topic, and to propose strategies for the successful implementation of massive open online courses (MOOCs). The researchers identified and analyzed a documentary corpus and found that, in the use of technologies for large-scale teaching, there has been a parallel evolution that have led to the emergence of MOOCs and includes five roots: distance education and online learning, testing or teaching machines and computer-assisted instruction, learning management systems, open education and open educational resources, and online massive teaching. The researchers propose three strategies for the successful implementation of MOOCs: careful consideration of the c/x MOOC pedagogical spectrum characteristics, selection of an appropriate MOOC model, and management of implementation challenges."
            },
            {
                "reference_title": "Students' insights on the use of video lectures in online classes",
                "reference_link": "publication/319407483_Students'_insights_on_the_use_of_video_lectures_in_online_classes",
                "reference_type": "Article",
                "reference_date": "Aug 2017",
                "reference_abstract": "Video lectures (VL), considered an effective means for delivering course content and infusing teaching presence in the virtual environment, have become very popular in education. The purpose of this study was to investigate online student experiences with VL focusing on their opinion of usefulness of VL, their satisfaction with them and their perception of learning derived from them. Our findings show that students' satisfaction with VL has a strong relationship with positive overall learning experience and perception of impact of video on learning. Furthermore, VL can enhance a feeling of engagement with content because of learners' control of the media and instructors' presence. The findings also alert us on the importance of careful planning and balanced integration of VL with other course materials. This provides important information on the effectiveness of video-lectures in college teaching and learning and implications for practice in online course design."
            }
        ]
    },
    {
        "title": "A Case for a New IT Ecosystem: On-The-Fly Computing",
        "date": "December 2020",
        "doi": "10.1007/s12599-019-00627-x",
        "conferance": null,
        "citations_count": "Citations (1)",
        "reference_count": "References (88)",
        "abstract": "The complexity of development and deployment in today\u2019s IT world is enormous. Despite the existence of so many pre-fabricated components, frameworks, cloud providers, etc., building IT systems still remains a major challenge and most likely overtaxes even a single ambitious developer. This results in spreading such development and deployment tasks over different team members with their own specialization. Nevertheless, not even highly competent IT personnel can easily succeed in developing and deploying a nontrivial application that comprises a multitude of different components running on different platforms (from frontend to backend). Current industry trends such as DevOps strive to keep development and deployment tasks tightly integrated. This, however, only partially addresses the underlying complexity of either of these two tasks. But would it not be desirable to simplify these tasks in the first place, enabling one person \u2013 maybe even a non-expert \u2013 to deal with all of them? Today\u2019s approaches to the development and deployment of complex IT applications are not up to this challenge. \u201cOn-The-Fly Computing\u201d offers an approach to tackle this challenge by providing complex IT services through largely automated configuration and execution. The configuration of such services is based on simple, flexibly combinable services that are provided by different software providers and traded in a market. This constitutes a highly relevant challenge for research in many branches of computer science, information systems, business administration, and economics. In this research note, it is analyzed which pieces of this new \u201cOn-The-Fly Computing\u201d ecosystem already exist and where additional, often significant research efforts are necessary.",
        "reference": [
            {
                "reference_title": "Design of review systems \u2013 A strategic instrument to shape online reviewing behavior and economic outcomes",
                "reference_link": "publication/331026732_Design_of_review_systems_-_A_strategic_instrument_to_shape_online_reviewing_behavior_and_economic_outcomes",
                "reference_type": "Article",
                "reference_date": "Feb 2019",
                "reference_abstract": "As online reviews play a decisive role in consumers\u2019 purchase decisions, e-commerce platforms are using review systems strategically to obtain a competitive advantage. However, the strategic potential can only be leveraged if the review system is designed appropriately. Research on the design of review systems and the effects of design choices has not yet been summarized or synthesized in a review article. We aim to close this gap by providing a scoping review. In our synthesis we posit that the design of review systems moderates the impact of online reviews on economic outcomes and the factors that drive the formation of reviews. After reviewing current research findings, we identify gaps and provide a research agenda covering three key themes: Design features, environments, and devices."
            },
            {
                "reference_title": "Platform Launch Strategies",
                "reference_link": "publication/322643147_Platform_Launch_Strategies",
                "reference_type": "Article",
                "reference_date": "Apr 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Microservices: Granularity vs. Performance",
                "reference_link": "publication/320075000_Microservices_Granularity_vs_Performance",
                "reference_type": "Article",
                "reference_date": "Sep 2017",
                "reference_abstract": "Microservice Architectures (MA) have the potential to increase the agility of software development. In an era where businesses require software applications to evolve to support software emerging requirements, particularly for Internet of Things (IoT) applications, we examine the issue of microservice granularity and explore its effect upon application latency. Two approaches to microservice deployment are simulated; the first with microservices in a single container, and the second with microservices partitioned across separate containers. We observed a neglibible increase in service latency for the multiple container deployment over a single container."
            },
            {
                "reference_title": "A Review of Technologies for Conversational Systems",
                "reference_link": "publication/318166894_A_Review_of_Technologies_for_Conversational_Systems",
                "reference_type": "Conference Paper",
                "reference_date": "Jun 2018",
                "reference_abstract": "During the last 50 years, since the development of ELIZA by Weizenbaum, technologies for developing conversational systems have made a great stride. The number of conversational systems is increasing. Conversational systems emerge almost in every digital device in many application areas. In this paper, we present the review of the development of conversational systems regarding technologies and their special features including language tricks."
            },
            {
                "reference_title": "Decomposing the Variance of Consumer Ratings and the Impact on Price and Demand",
                "reference_link": "publication/329580122_Decomposing_the_Variance_of_Consumer_Ratings_and_the_Impact_on_Price_and_Demand",
                "reference_type": "Article",
                "reference_date": "Dec 2018",
                "reference_abstract": "Consumer ratings play a decisive role in purchases by online shoppers. Although the effects of the average and the number of consumer ratings on future product pricing and demand have been studied with some conclusive results, the effects of the variance of these ratings are less well understood. We develop a model where we decompose the variance of consumer ratings into two sources: taste differences about search and experience attributes of a durable good, and quality differences among instances of this good in the form of product failure. We find that (i) optimal price increases and demand decreases in variance caused by taste differences, (ii) optimal price and demand decrease in variance caused by quality differences, and (iii) when holding the average rating as well as the total variance constant, for products with low total variance, both price and demand increase in the relative share of variance caused by taste differences. Counter to intuition, we demonstrate that risk-averse consumers may prefer a higher-priced product with a higher variance in ratings when deciding between two similar products with the same average rating."
            },
            {
                "reference_title": "On-The-Fly Service Construction with Prototypes",
                "reference_link": "publication/324797212_On-The-Fly_Service_Construction_with_Prototypes",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2018",
                "reference_abstract": "In spite of the enormous attention that automated service composition has attracted in the recent past, and the large number of approaches that has been developed, these approaches are rarely put into practice and deployed for real-world problems.\nIn light of the increasing demand for an automated construction of services in various domains, this is a somewhat surprising observation. In fact, in domains such as automated machine learning and cloud gaming, both users and providers of software are highly interested in running services that are created in an on-the-fly manner.\nThis paper makes two contributions. First, we provide a framework that is able to create running services from simple queries specifiable by end-users. Our focus is less on the composition algorithm but more on infrastructural aspects that arise when trying to obtain an executable artifact. Second, to demonstrate the versatility and applicability of the framework, we instantiate it for three highly heterogeneous domains: automated machine learning, cloud gaming, and information integration. By putting automated composition into practice, our approach thus goes beyond existing, entirely model-based service composition techniques. Implementations of the framework and its instantiations are public."
            },
            {
                "reference_title": "Automated software and service composition: A survey and evaluating review",
                "reference_link": "publication/324442759_Automated_software_and_service_composition_A_survey_and_evaluating_review",
                "reference_type": "Chapter",
                "reference_date": "Jan 2016",
                "reference_abstract": null
            },
            {
                "reference_title": "Microservices: Granularity vs. Performance",
                "reference_link": "publication/321637408_Microservices_Granularity_vs_Performance",
                "reference_type": "Conference Paper",
                "reference_date": "Dec 2017",
                "reference_abstract": "Microservice Architectures (MA) have the potential to increase the agility of software development. In an era where businesses require software applications to evolve to support emerging software requirements, particularly for Internet of Things (IoT) applications, we examine the issue of microservice granularity and explore its effect upon application latency. Two approaches to microservice deployment are simulated; the first with microservices in a single container, and the second with microservices partitioned across separate containers. We observed a negligible increase in service latency for the multiple container deployment over a single container."
            },
            {
                "reference_title": "Understanding the determinants of online review helpfulness: A meta-analytic investigation",
                "reference_link": "publication/318020506_Understanding_the_determinants_of_online_review_helpfulness_A_meta-analytic_investigation",
                "reference_type": "Article",
                "reference_date": "Oct 2017",
                "reference_abstract": "Online consumer reviews can help customers reduce uncertainty and risks faced in online shopping. However, the studies examining the determinants of perceived review helpfulness produce mixed findings. We review extant research about the determinant factors of perceived online review helpfulness. All review related determinants (i.e., review depth, review readability, linear review rating, quadratic review rating, review age) and two reviewer related determinants (i.e., reviewer information disclosure and reviewer expertise) are found to have inconsistent conclusions on how they affect perceived review helpfulness. We conduct a meta-analysis to examine those determinant factors in order to reconcile the contradictory findings about their influence on perceived review helpfulness. The meta-analysis results affirm that review depth, review age, reviewer information disclosure, and reviewer expertise have positive influences on review helpfulness. Review readability and review rating are found to have no significant influence on review helpfulness. Moreover, we find that helpfulness measurement, online review platform, and product type are the three factors that cause mixed findings in extant research."
            },
            {
                "reference_title": "Stimulating Online Reviews by Combining Financial Incentives and Social Norms",
                "reference_link": "publication/318009022_Stimulating_Online_Reviews_by_Combining_Financial_Incentives_and_Social_Norms",
                "reference_type": "Article",
                "reference_date": "Jan 2016",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Unveiling large-scale commuting patterns based on mobile phone cellular network data",
        "date": "December 2020",
        "doi": "10.1016/j.jtrangeo.2020.102871",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (58)",
        "abstract": "In this study, with Estonia as an example,we established an approach based on Hidden Markov Model to extract large-scale commuting patterns at different geographical levels using a massive amount of mobile phone cellular network data, which is referred to as Call detail record (CDR). The proposed model is designed for reconstructing and transforming the trajectories extracted from the CDR data. This step allowed us to perform origin-destination matrix extraction among different geographical levels, which helped in depicting the commuting patterns. Besides, we introduced different techniques for analyzing the commuting at the urban level. Our results unveiled that there is great potential behind mobile data of the cellular networks after transforming it into meaningful mobility patterns. That can easily be used for understanding urban dynamics, large-scale daily commuting and mobility. The aggressive development and growth of ubiquitous mobile sensing have generated valuable data that can be used with our approach for providing answers and solutions to the growing problems of transportation, urbanization and sustainability.",
        "reference": [
            {
                "reference_title": "OD-Matrix Extraction based on Trajectory Reconstruction from Mobile Data",
                "reference_link": "publication/336769593_OD-Matrix_Extraction_based_on_Trajectory_Reconstruction_from_Mobile_Data",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2019",
                "reference_abstract": "In this work, we propose a novel approach to reconstruct the individual's trajectories that has been extracted from Call Details Records (CDR) and Visitor Location Registry (VLR) data. Our methodology is based on the second-order Markov model for reconstructing the hidden history of the extracted sparse trajectories that reflects different routes visits by mobile users. \nAfter reconstructing the complete cell-to-cell trajectories of users, the OD matrix of the major cities of Estonia is derived and evaluated by the number of train passengers in the country. Our dataset contains more than 600 millions anonymized CDR and VLR events from approximately 3 hundred thousands users in Estonia."
            },
            {
                "reference_title": "Understanding Human Mobility Patterns in a Developing Country Using Mobile Phone Data",
                "reference_link": "publication/330117766_Understanding_Human_Mobility_Patterns_in_a_Developing_Country_Using_Mobile_Phone_Data",
                "reference_type": "Article",
                "reference_date": "Jan 2019",
                "reference_abstract": "This study demonstrates the use of mobile phone data to derive country-wide mobility patterns. We identified significant locations of users such as home, work, and other based on a combined measure of frequency, duration, time, and day of mobile phone interactions. Consecutive mobile phone records of users are used to identify stay and pass-by locations. A stay location is where users spend a significant amount of their time measured through their mobile phone usage. Trips are constructed for each user between two consecutive stay locations in a day and then categorized by purpose and time of the day. Three measures of entropy are used to further understand the regularity of user\u2019s spatiotemporal mobility patterns. The results show that user\u2019s in a high entropy cluster has high percentage of non-home based trips (77%), and user\u2019s in a low entropy cluster has high percentage of commuting trips (49%), indicating high regularity. A set of doubly constrained trip distribution models is estimated. To measure travel cost, the concept of a centroid point that assumes the origins and destinations of all trips are concentrated at an arbitrary location such as the centroid of a zone is replaced by multiple origins and destinations represented by cell tower locations. Note that a cell tower location can only be used as trips origin/destination location when a stay is detected. The travel cost measured between cell tower locations has resulted in shorter trip distances and the model estimation shows less sensitivity to the distance-decay effect."
            },
            {
                "reference_title": "From Mobility Analysis to Mobility Hubs Discovery: A Concept Based on Using CDR Data of the Mobile Networks",
                "reference_link": "publication/328837467_From_Mobility_Analysis_to_Mobility_Hubs_Discovery_A_Concept_Based_on_Using_CDR_Data_of_the_Mobile_Networks",
                "reference_type": "Conference Paper",
                "reference_date": "Nov 2018",
                "reference_abstract": "Understanding human mobility in dynamic environ- ments, such as our cities, is a crucial step toward the development of our economy, our environment, our transportation, as well as our well-being. Therefore, there is a need to rely on a sensor or a source of information, such as mobile phone data, that can help us to sense these urban dynamics. From this viewpoint, we are focusing on demonstrating and presenting our concept idea that relies on a new method based on switching Kalman filter that enables the extraction of mobility patterns and through its analysis discovering mobility hubs where the urban mobility converges or diverges using mobile data, especially CDR data."
            },
            {
                "reference_title": "Collecting travel diaries: Current state of the art, best practices, and future research directions",
                "reference_link": "publication/328617637_Collecting_travel_diaries_Current_state_of_the_art_best_practices_and_future_research_directions",
                "reference_type": "Article",
                "reference_date": "Jan 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "A trip to work: Estimation of origin and destination of commuting patterns in the main metropolitan regions of Haiti using CDR",
                "reference_link": "publication/323912690_A_trip_to_work_Estimation_of_origin_and_destination_of_commuting_patterns_in_the_main_metropolitan_regions_of_Haiti_using_CDR",
                "reference_type": "Article",
                "reference_date": "Mar 2018",
                "reference_abstract": "The rapid, unplanned urbanisation in Haiti creates a series of urban mobility challenges which can contribute to job market fragmentation and decrease the quality of life in the city. Data on population and job distributions, and on home-work commuting patterns in major urban centres are scarce. The most recent census took place in 2003 and events such as the 2010 earthquake have caused major redistributions of the population. In this data scarce context, our work takes advantage of nationwide de-identified Call Detail Records (CDR) from the main mobile operator in the country to investigate night and daytime populations densities and commuting patterns. We use a non-supervised learning algorithm to identify meaningful locations for individuals. These locations are then labelled according to a scoring criteria. The labelled locations are distributed in a grid with cells measuring 500 \u00d7 500 m in order to aggregate the individual level data and to create origin-destination matrices of weighted connections between home and work locations. The results suggest that labor markets are fragmented in Haiti. The two main urban centres, Port-au-Prince and Cap-Ha\u00eftien suffer from low employment accessibility as measured by the percentage of the population that travels beyond their identified home cluster (1 km radius) during the day. The data from the origin-destination matrices suggest that only 42 and 40 percent of the population are considered to be commuters in Port-au-Prince and Cap-Ha\u00eftien respectively."
            },
            {
                "reference_title": "Exploring human mobility patterns using geo-tagged social media data at the group level",
                "reference_link": "publication/322440460_Exploring_human_mobility_patterns_using_geo-tagged_social_media_data_at_the_group_level",
                "reference_type": "Article",
                "reference_date": "Jan 2018",
                "reference_abstract": "Exploring human mobility using social media data is an active research area related to many disciplines such as geography, urban planning and public health. We attempt to understand human mobility at the community level by analysing community mobility patterns across a significant time-span based on geo-tagged social media data. We take a typical college community, The Chinese University of Geosciences Wuhan (CUG Wuhan) community, as our research object. We identify more than 3757 CUG members with 81,059 geo-tagged Weibo messages in the period 2014\u20132015. We measure CUG group mobility patterns by analysing the spatio-temporal distribution of these messages and the activity patterns. We find that: (i) the students\u2019 \u2018active area\u2019 in relation to their distance from CUG obeys a power-law distribution; (ii) heavy \u2018check-in\u2019 Weibo users do not, in fact, tend to be more active (in the physical world) than light \u2018check-in\u2019 Weibo users; (iii) mobility differences reflect gender differences."
            },
            {
                "reference_title": "Spatio-Temporal Mobility Analysis for Community Detection in the Mobile Networks Using CDR Data",
                "reference_link": "publication/320734301_Spatio-Temporal_Mobility_Analysis_for_Community_Detection_in_the_Mobile_Networks_Using_CDR_Data",
                "reference_type": "Conference Paper",
                "reference_date": "Nov 2017",
                "reference_abstract": "Building or detecting community association is a key attribute in understanding relationships in social networks and their behavior. By detecting groups, we can address many issues related to population displacement and intelligent transportation systems. In this paper, we are presenting a method based on\nanalyzing and exploring spatio-temporal mobility patterns from call detail records of mobile users to discover communities. Our proposed approach relies on deriving the events\u2019 occurrence in a cells coverage in time and space to match and categorize the users based on their mobility patterns extracted from call detail records\ndata. The final results are the outcome of real experiments that demonstrate interesting and encouraging findings by using mobility patterns as a means to detect communities."
            },
            {
                "reference_title": "From edit distance to augmented space-time-weighted edit distance: Detecting and clustering patterns of human activities in Puget Sound region",
                "reference_link": "publication/333542905_From_edit_distance_to_augmented_space-time-weighted_edit_distance_Detecting_and_clustering_patterns_of_human_activities_in_Puget_Sound_region",
                "reference_type": "Article",
                "reference_date": "Jun 2019",
                "reference_abstract": "Considering and measuring the similarity of human activities remains challenging. Existing studies of similarity measures based on traditional edit distance (ED), specifically on activity patterns, do not reflect the spatiotemporal characteristics in the measurement model. Additionally, interdependence between activities is ignored in existing multidimensional sequence alignment methods. To address the gap, we initially extend the traditional edit distance to a space-time-weighted edit distance (STW-ED). Specifically, differences in distance and time between activities are considered cost functions in the operation cost calculation (insertion, deletion, and substitution). We advance STW-ED to an augmented space-time-weighted edit distance method (ASTW-ED)that integrates an optimum-trajectory-based multidimensional sequence alignment method (OT-MDSAM)with STW-ED, treating the nonspatiotemporal dimensions as augment factors. In addition, ontology is considered for the similarity measure for nonspatiotemporal dimensions. To show the feasibility of our proposed approach, we conduct an empirical study based on an activity-based travel survey in the Puget Sound Region. Eight clusters (homemakers, regular workers with a colorful life, regular workers with a monotonous life, part-time workers, recreation travelers, senior travelers, no-job travelers, and night owl adventurers)are identified based on ASTW-ED and ontology. To cluster the similarity matrix derived from the introduced methods, the affinity propagation (AP)clustering method is employed because it is free of prior knowledge for clustering and can produce exemplars of the clusters. The empirical study indicates that, relative to existing methods for multidimensional activity similarity measurement and clustering, ASTW-ED performs better in terms of within-group homogeneity and between-group heterogeneity of clusters. In addition, the results reveal that ontology can improve clustering performance if it is considered for nonspatiotemporal dimensions provide better understanding of human behavior for urban governance.."
            },
            {
                "reference_title": "5G for the Connected World",
                "reference_link": "publication/331627037_5G_for_the_Connected_World",
                "reference_type": "Book",
                "reference_date": "Apr 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Cellpath Routing and Route Traffic Flow Estimation Based on Cellular Network Data",
                "reference_link": "publication/321279318_Cellpath_Routing_and_Route_Traffic_Flow_Estimation_Based_on_Cellular_Network_Data",
                "reference_type": "Article",
                "reference_date": "Nov 2017",
                "reference_abstract": "The signaling data in cellular networks provide means for analyzing the use of transportation systems. We propose methods that aim to reconstruct the used route through a transportation network from call detail records (CDRs) which are spatially and temporally sparse. The route estimation methods are compared based on the individual routes estimated. We also investigate the effect of different route estimation methods when employed in a complete network assignment for a larger city. Using an available CDR dataset for Dakar, Senegal, we show that the choice of the route estimation method can have a significant impact on resulting link flows."
            }
        ]
    },
    {
        "title": "Evaluation of an augmented reality platform for austere surgical telementoring: a randomized controlled crossover study in cricothyroidotomies",
        "date": "December 2020",
        "doi": "10.1038/s41746-020-0284-9",
        "conferance": null,
        "citations_count": "Citations (1)",
        "reference_count": "References (43)",
        "abstract": "Telementoring platforms can help transfer surgical expertise remotely. However, most telementoring platforms are not designed to assist in austere, pre-hospital settings. This paper evaluates the system for telementoring with augmented reality (STAR), a portable and self-contained telementoring platform based on an augmented reality head-mounted display (ARHMD). The system is designed to assist in austere scenarios: a stabilized first-person view of the operating field is sent to a remote expert, who creates surgical instructions that a local first responder wearing the ARHMD can visualize as three-dimensional models projected onto the patient\u2019s body. Our hypothesis evaluated whether remote guidance with STAR could lead to performing a surgical procedure better, as opposed to remote audio-only guidance. Remote expert surgeons guided first responders through training cricothyroidotomies in a simulated austere scenario, and on-site surgeons evaluated the participants using standardized evaluation tools. The evaluation comprehended completion time and technique performance of specific cricothyroidotomy steps. The analyses were also performed considering the participants\u2019 years of experience as first responders, and their experience performing cricothyroidotomies. A linear mixed model analysis showed that using STAR was associated with higher procedural and non-procedural scores, and overall better performance. Additionally, a binary logistic regression analysis showed that using STAR was associated to safer and more successful executions of cricothyroidotomies. This work demonstrates that remote mentors can use STAR to provide first responders with guidance and surgical knowledge, and represents a first step towards the adoption of ARHMDs to convey clinical expertise remotely in austere scenarios.",
        "reference": [
            {
                "reference_title": "A First-Person Mentee Second-Person Mentor AR Interface for Surgical Telementoring",
                "reference_link": "publication/332759693_A_First-Person_Mentee_Second-Person_Mentor_AR_Interface_for_Surgical_Telementoring",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Surgical Telementoring Without Encumbrance: A Comparative Study of See-through Augmented Reality-based Approaches",
                "reference_link": "publication/324619420_Surgical_Telementoring_Without_Encumbrance_A_Comparative_Study_of_See-through_Augmented_Reality-based_Approaches",
                "reference_type": "Article",
                "reference_date": "Apr 2018",
                "reference_abstract": "Objective: \nThis study investigates the benefits of a surgical telementoring system based on an augmented reality head-mounted display (ARHMD) that overlays surgical instructions directly onto the surgeon's view of the operating field, without workspace obstruction.\n\nSummary background data: \nIn conventional telestrator-based telementoring, the surgeon views annotations of the surgical field by shifting focus to a nearby monitor, which substantially increases cognitive load. As an alternative, tablets have been used between the surgeon and the patient to display instructions; however, tablets impose additional obstructions of surgeon's motions.\n\nMethods: \nTwenty medical students performed anatomical marking (Task1) and abdominal incision (Task2) on a patient simulator, in 1 of 2 telementoring conditions: ARHMD and telestrator. The dependent variables were placement error, number of focus shifts, and completion time. Furthermore, workspace efficiency was quantified as the number and duration of potential surgeon-tablet collisions avoided by the ARHMD.\n\nResults: \nThe ARHMD condition yielded smaller placement errors (Task1: 45%, P < 0.001; Task2: 14%, P = 0.01), fewer focus shifts (Task1: 93%, P < 0.001; Task2: 88%, P = 0.0039), and longer completion times (Task1: 31%, P < 0.001; Task2: 24%, P = 0.013). Furthermore, the ARHMD avoided potential tablet collisions (4.8 for 3.2 seconds in Task1; 3.8 for 1.3 seconds in Task2).\n\nConclusion: \nThe ARHMD system promises to improve accuracy and to eliminate focus shifts in surgical telementoring. Because ARHMD participants were able to refine their execution of instructions, task completion time increased. Unlike a tablet system, the ARHMD does not require modifying natural motions to avoid collisions."
            },
            {
                "reference_title": "How About the Mentor? Effective Workspace Visualization in AR Telementoring",
                "reference_link": "publication/342055854_How_About_the_Mentor_Effective_Workspace_Visualization_in_AR_Telementoring",
                "reference_type": "Conference Paper",
                "reference_date": "Mar 2020",
                "reference_abstract": null
            },
            {
                "reference_title": "How About the Mentor? Effective Workspace Visualization in AR Telementoring",
                "reference_link": "publication/341318614_How_About_the_Mentor_Effective_Workspace_Visualization_in_AR_Telementoring",
                "reference_type": "Conference Paper",
                "reference_date": "Mar 2020",
                "reference_abstract": null
            },
            {
                "reference_title": "Perceptual Limits of Optical See-Through Visors for Augmented Reality Guidance of Manual Tasks",
                "reference_link": "publication/332928276_Perceptual_Limits_of_Optical_See-Through_Visors_for_Augmented_Reality_Guidance_of_Manual_Tasks",
                "reference_type": "Article",
                "reference_date": "May 2019",
                "reference_abstract": "The focal length of available optical see-through (OST) head-mounted displays (HMDs) is at least 2 m therefore, during manual tasks, the user eye cannot keep in focus both the virtual and real content at the same time. Another perceptual limitation is related to the vergence-accommodation conflict (VAC), this latter being present in binocular vision only. This paper investigates the effect of incorrect focus cues on the user performance, visual comfort and workload during the execution of augmented reality (AR) guided manual task with one of the most advanced OST HMD, the Microsoft HoloLens.\n\nMethods: \nAn experimental study was designed to investigate the performance of 20 subjects in a connect-the-dots task, with and without the use of AR. The following tests were planned: AR guided monocular and binocular; Naked-eye monocular and binocular. Each trial was analyzed to evaluate the accuracy in connecting dots; NASA Task Load Index and Likert questionnaires were used to assess the workload and the visual comfort.\n\nResults: \nNo statistically significant differences were found in the workload, and in the perceived comfort between the AR guided binocular and monocular test. User performances were significantly better during the Naked eye tests. No statistically significant differences in performances were found in the monocular and binocular tests. The maximum error in AR tests was 5.9 mm.\n\nConclusion: \nEven if there is a growing interest in using commercial OST-HMD, for guiding high-precision manual tasks, attention should be paid to the limitations of the available technology not designed for the peripersonal space."
            },
            {
                "reference_title": "The Utilization of Video Technology in Surgical Education: A Systematic Review",
                "reference_link": "publication/331440960_The_Utilization_of_Video_Technology_in_Surgical_Education_A_Systematic_Review",
                "reference_type": "Article",
                "reference_date": "Mar 2019",
                "reference_abstract": "Background: \nThe use of surgical video has great potential to enhance surgical education, but there exists limited information about how to effectively use surgical videos. We performed a systematic review of video technology in surgical training and provided evidence-based recommendations for its effective use.\n\nMaterials and methods: \nA systematic review of literature on surgical video in residency education was conducted. All articles meeting inclusion criteria were evaluated for technical characteristics pertaining to video usage. Included studies were critically appraised using a quality-scoring system. Recommendations were provided for the effective implementation of video in surgical education based on associations with improved training outcomes.\n\nResults: \nTwenty articles met inclusion criteria. In these studies, the source of video acquisition was primarily laparoscopy (40.0% of papers), and the main perspective of video was endoscopy (45.0%). Features of videos included supplementation with other educational tools (55.0%), schematic diagrams or images (50.0%), audio (40.0%), and narration (25.0%). Videos were primarily viewed preoperatively (60.0%) or postoperatively (50.0%). The intended viewer for videos was usually residents (70.0%) but also included attendings/faculty (30.0%). When compared with a nonvideo training group, video training was associated with improved resident knowledge (100%), improved operative performance (81.3%), and greater participant satisfaction (100%).\n\nConclusions: \nBased on this review, we recommend that surgical training programs incorporate schematics and imaging into video, supplement video with other education tools, and utilize audio in video. For video review, we recommend that residents review video preoperatively and postoperatively for learning and that attendings review video postoperatively for assessment."
            },
            {
                "reference_title": "Rubrum Coelis: The Contribution of Real-Time Telementoring in Acute Trauma Scenarios\u2014A Randomized Controlled Trial",
                "reference_link": "publication/330813013_Rubrum_Coelis_The_Contribution_of_Real-Time_Telementoring_in_Acute_Trauma_Scenarios-A_Randomized_Controlled_Trial",
                "reference_type": "Article",
                "reference_date": "Feb 2019",
                "reference_abstract": "Background: Most deaths in military trauma occur soon after wounding, and demand immediate on scene interventions. Although hemorrhage predominates as the cause of potentially preventable death, airway obstruction and tension pneumothorax are also frequent. First responders caring for casualties in operational settings often have limited clinical experience.Introduction: We hypothesized that communications technologies allowing for real-time communications with a senior medically experienced provider might assist in the efficacy of first responding to catastrophic trauma.Methods: Thirty-three basic life saving (BLS) medics were randomized into two groups: either receiving telementoring support (TMS, n = 17) or no telementoring support (NTMS, n = 16) during the diagnosis and resuscitation of a simulated critical battlefield casualty. In addition to basic life support, all medics were required to perform a procedure needle thoracentesis (not performed by BLS medics in Israel) for the first time. TMS was performed by physicians through an internet link. Performance was assessed during the simulation and later on review of videos.Results: The TMS group was significantly more successful in diagnosing (82.35% vs. 56.25%, p = 0.003) and treating pneumothorax (52.94% vs. 37.5%, p = 0.035). However, needle thoracentesis time was slightly longer for the TMS group versus the NTMS group (1:24 \u00b1 1:00 vs. 0:49 \u00b1 0:21 minu, respectively (p = 0.016). Complete treatment time was 12:56 \u00b1 2:58 min for the TMS group, versus 9:33 \u00b1 3:17 min for the NTMS group (p = 0.003).Conclusions: Remote telementoring of basic life support performed by military medics significantly improved the medics' ability to perform an unfamiliar lifesaving procedure at the cost of prolonging time needed to provide care. Future studies must refine the indications and contraindications for using telemedical support."
            },
            {
                "reference_title": "Telementoring of Surgeons: A Systematic Review",
                "reference_link": "publication/329137984_Telementoring_of_Surgeons_A_Systematic_Review",
                "reference_type": "Article",
                "reference_date": "Nov 2018",
                "reference_abstract": "Background: \nTelementoring is a technique that has shown potential as a surgical training aid. Previous studies have suggested that telementoring is a safe training modality. This review aimed to review both the technological capabilities of reported telementoring systems as well as its potential benefits as a mentoring modality.\n\nMethods: \nA systematic review of the literature, up to July 2017, was carried out in accordance with PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. Study quality was assessed using the Oxford Levels of Evidence proforma. Data were extracted regarding technical capabilities, bandwidth, latency, and costs. Additionally, the primary aim and key results were extracted from each study and analyzed.\n\nResults: \nA total of 66 studies were identified for inclusion. In all, 48% of studies were conducted in general surgery; 22 (33%), 24 (36%), and 20 (30%) of studies reported telementoring that occurred within the same hospital, outside the hospital, and outside the country, respectively. Sixty-four (98%) of studies employed video and audio and 38 (58%) used telestration. Twelve separate studies directly compared telementoring against on-site mentoring. Seven (58%) showed no difference in outcomes between telementoring and on-site mentoring. No study found telementoring to result in poorer postoperative outcomes.\n\nConclusions: \nThe results of this review suggest that telementoring has a similar safety and efficacy profile as on-site mentoring. Future analysis to determine the potential benefits and pitfalls to surgical education through telementoring are required to determine the exact role it shall play in the future. Technological advances to improve remote connectivity would also aid the uptake of telementoring on a larger scale."
            },
            {
                "reference_title": "Tele-mentored damage-control and emergency trauma surgery: A feasibility study using live-tissue models",
                "reference_link": "publication/322911729_Tele-mentored_damage-control_and_emergency_trauma_surgery_A_feasibility_study_using_live-tissue_models",
                "reference_type": "Article",
                "reference_date": "Feb 2018",
                "reference_abstract": "Background: \nDamage-control and emergency surgical procedures in trauma have the potential to save lives. They may occasionally not be performed due to clinician inexperience or lack of comfort and knowledge.\n\nMethods: \nCanadian Armed Forces (CAF) non-surgeon Medical Officers (MOs) participated in a live tissue training exercise. They received tele-mentoring assistance using a secure video-conferencing application on a smartphone/tablet platform. Feasibility of tele-mentored surgery was studied by measuring their effectiveness at completing a set series of tasks in this pilot study. Additionally, their comfort and willingness to perform studied procedures was gauged using pre- and post-study surveys.\n\nResults: \nWith no pre-procedural teaching, participants were able to complete surgical airway, chest tube insertion and resuscitative thoracotomy with 100% effectiveness with no noted complications. Comfort level and willingness to perform these procedures were improved with tele-mentoring. Participants felt that tele-mentored surgery would benefit their performance of resuscitative thoracotomy most.\n\nConclusion: \nThe use of tele-mentored surgery to assist non-surgeon clinicians in the performance of damage-control and emergency surgical procedures is feasible. More study is required to validate its effectiveness."
            },
            {
                "reference_title": "Am I a Baller? Basketball Performance Assessment from First-Person Videos",
                "reference_link": "publication/322058020_Am_I_a_Baller_Basketball_Performance_Assessment_from_First-Person_Videos",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2017",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "A Common Optimization Framework for Multi-Robot Exploration and Coverage in 3D Environments",
        "date": "December 2020",
        "doi": "10.1007/s10846-020-01255-4",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (38)",
        "abstract": "This paper studies the problems of static coverage and autonomous exploration of unknown three-dimensional environments with a team of cooperating aerial vehicles. Although these tasks are usually considered separately in the literature, we propose a common framework where both problems are formulated as the maximization of online acquired information via the definition of single-robot optimization functions, which differs only slightly in the two cases to take into account the static and dynamic nature of coverage and exploration respectively. A common derivative-free approach based on a stochastic approximation of these functions and their successive optimization is proposed, resulting in a fast and decentralized solution. The locality of this methodology limits however this solution to have local optimality guarantees and specific additional layers are proposed for the two problems to improve the final performance. Specifically, a Voronoi-based initialization step is added for the coverage problem and a combination with a frontier-based approach is proposed for the exploration case. The resulting algorithms are finally tested in simulations and compared with possible alternatives.",
        "reference": [
            {
                "reference_title": "An autonomous exploration algorithm using environment-robot interacted traversability analysis",
                "reference_link": "publication/337655581_An_autonomous_exploration_algorithm_using_environment-robot_interacted_traversability_analysis",
                "reference_type": "Conference Paper",
                "reference_date": "Nov 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Collaborative visual area coverage using aerial agents equipped with PTZ-cameras under localization uncertainty",
                "reference_link": "publication/334279927_Collaborative_visual_area_coverage_using_aerial_agents_equipped_with_PTZ-cameras_under_localization_uncertainty",
                "reference_type": "Conference Paper",
                "reference_date": "Jun 2019",
                "reference_abstract": "Mobile Aerial Agents (MAAs) using localization-sensors suffer from their inherent localization uncertainty. These MAAs are equipped with Pan-Tilt-Zoom (PTZ) cameras and thru their 3D-altitude and camera parameters control survey the area underneath them. For a larger convex area, the MAAs transmit to their neighbors their PTZ and altitude parameters. Using a Voronoi-free area tessellation framework and a gradient scheme, a collaborative control framework is provided to maximize the covered area. Simulation studies are offered to investigate the effectiveness of the suggested scheme."
            },
            {
                "reference_title": "Cooperative Frontier-Based Exploration Strategy for Multi-Robot System",
                "reference_link": "publication/326949052_Cooperative_Frontier-Based_Exploration_Strategy_for_Multi-Robot_System",
                "reference_type": "Conference Paper",
                "reference_date": "Jun 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Multirobot Exploration of Communication-Restricted Environments: A Survey",
                "reference_link": "publication/322691000_Multirobot_Exploration_of_Communication-Restricted_Environments_A_Survey",
                "reference_type": "Article",
                "reference_date": "Nov 2017",
                "reference_abstract": "Exploration of initially unknown environments is an online task in which autonomous mobile robots coordinate themselves to efficiently discover free spaces and obstacles. Several efforts have been devoted to study coordinated multirobot exploration assuming that communication is possible between any two locations. The problem of developing multirobot systems for effective exploration in the presence of communication constraints, despite its remarkable practical relevance, is comparably much less studied. The authors provide a taxonomy of the field of communication-restricted multirobot exploration, survey recent work in this field, and outline some promising research directions."
            },
            {
                "reference_title": "Strategies for coordinated multirobot exploration with recurrent connectivity constraints",
                "reference_link": "publication/318683927_Strategies_for_coordinated_multirobot_exploration_with_recurrent_connectivity_constraints",
                "reference_type": "Article",
                "reference_date": "Apr 2018",
                "reference_abstract": "During several applications, such as search and rescue, robots must discover new information about the environment and, at the same time, share operational knowledge with a base station through an ad hoc network. In this paper, we design exploration strategies that allow robots to coordinate with teammates to form such a network in order to satisfy recurrent connectivity constraints\u2014that is, data must be shared with the base station when making new observations at the assigned locations. Current approaches lack in flexibility due to the assumptions made about the communication model. Furthermore, they are sometimes inefficient because of the synchronous way they work: new plans are issued only once all robots have reached their goals. This paper introduces two novel asynchronous strategies that work with arbitrary communication models. In this paper, \u2018asynchronous\u2019 means that it is possible to issue new plans to subgroups of robots, when they are ready to receive them. First, we propose a single-stage strategy based on Integer Linear Programming for selecting and assigning robots to locations. Second, we design a two-stage strategy to improve computational efficiency, by separating the problem of locations\u2019 selection from that of robot-location assignments. Extensive testing both in simulation and with real robots show that the proposed strategies provide good situation awareness at the base station while efficiently exploring the environment."
            },
            {
                "reference_title": "Help from the Sky: Leveraging UAVs for Disaster Management",
                "reference_link": "publication/312254141_Help_from_the_Sky_Leveraging_UAVs_for_Disaster_Management",
                "reference_type": "Article",
                "reference_date": "Jan 2017",
                "reference_abstract": "This article presents a vision for future unmanned aerial vehicles (UAV)-assisted disaster management, considering the holistic functions of disaster prediction, assessment, and response. Here, UAVs not only survey the affected area but also assist in establishing vital wireless communication links between the survivors and nearest available cellular infrastructure. A perspective of different classes of geophysical, climate-induced, and meteorological disasters based on the extent of interaction between the UAV and terrestrially deployed wireless sensors is presented in this work, with suitable network architectures designed for each of these cases. The authors outline unique research challenges and possible solutions for maintaining connected aerial meshes for handoff between UAVs and for systems-specific, security- and energy-related issues. This article is part of a special issue on drones."
            },
            {
                "reference_title": "Combining Stochastic Optimization and Frontiers for Aerial Multi-Robot Exploration of 3D Terrains",
                "reference_link": "publication/338942139_Combining_Stochastic_Optimization_and_Frontiers_for_Aerial_Multi-Robot_Exploration_of_3D_Terrains",
                "reference_type": "Conference Paper",
                "reference_date": "Nov 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Visual Coverage Control for Teams of Quadcopters via Control Barrier Functions",
                "reference_link": "publication/335144653_Visual_Coverage_Control_for_Teams_of_Quadcopters_via_Control_Barrier_Functions",
                "reference_type": "Conference Paper",
                "reference_date": "May 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Collaborative Visual Area Coverage",
                "reference_link": "publication/311492443_Collaborative_Visual_Area_Coverage",
                "reference_type": "Article",
                "reference_date": "Dec 2016",
                "reference_abstract": "This article examines the problem of visual area coverage by a network of Mobile Aerial Agents (MAAs). Each MAA is assumed to be equipped with a downwards facing camera with a conical field of view which covers all points within a circle on the ground. The diameter of that circle is proportional to the altitude of the MAA, whereas the quality of the covered area decreases with the altitude. A distributed control law that maximizes a joint coverage-quality criterion by adjusting the MAAs' spatial coordinates is developed. The effectiveness of the proposed control scheme is evaluated through simulation studies."
            },
            {
                "reference_title": "Least squares quantization in PCM",
                "reference_link": "publication/309715704_Least_squares_quantization_in_PCM",
                "reference_type": "Article",
                "reference_date": "Jan 2006",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "So, what exactly is a qualitative calculus?",
        "date": "December 2020",
        "doi": "10.1016/j.artint.2020.103385",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (33)",
        "abstract": "The paradigm of algebraic constraint-based reasoning, embodied in the notion of a qualitative calculus, is studied within two alternative frameworks. One framework defines a qualitative calculus as \u201ca non-associative relation algebra (NA) with a qualitative representation\u201d, the other as \u201can algebra generated by jointly exhaustive and pairwise disjoint (JEPD) relations\u201d. These frameworks provide complementary perspectives: the first is intensional (axiom-based), whereas the second one is extensional (based on semantic structures). However, each definition admits calculi that lie beyond the scope of the other. Thus, a qualitatively representable NA may be incomplete or non-atomic, whereas an algebra generated by JEPD relations may have non-involutive converse and no identity element. The divergence of definitions creates a confusion around the notion of a qualitative calculus and makes the \u201cwhat\u201d question posed by Ligozat and Renz actual once again. Here we define the relation-type qualitative calculus unifying the intensional and extensional approaches. By introducing the notions of weak identity, inference completeness and Q-homomorphism, we give equivalent definitions of qualitative calculi both intensionally and extensionally. We show that \u201calgebras generated by JEPD relations\u201d and \u201cqualitatively representable NAs\u201d are embedded into the class of relation-type qualitative algebras.",
        "reference": [
            {
                "reference_title": "Algebraic foundations for qualitative calculi and networks",
                "reference_link": "publication/304590233_Algebraic_foundations_for_qualitative_calculi_and_networks",
                "reference_type": "Article",
                "reference_date": "Jun 2016",
                "reference_abstract": "A qualitative representation $\\phi$ is like an ordinary representation of a relation algebra, but instead of requiring $(a; b)^\\phi = a^\\phi | b^\\phi$, as we do for ordinary representations, we only require that $c^\\phi\\supseteq a^\\phi | b^\\phi \\iff c\\geq a ; b$, for each $c$ in the algebra. A constraint network is qualitatively satisfiable if its nodes can be mapped to elements of a qualitative representation, preserving the constraints. If a constraint network is satisfiable then it is clearly qualitatively satisfiable, but the converse can fail. However, for a wide range of relation algebras including the point algebra, the Allen Interval Algebra, RCC8 and many others, a network is satisfiable if and only if it is qualitatively satisfiable. Unlike ordinary composition, the weak composition arising from qualitative representations need not be associative, so we can generalise by considering network satisfaction problems over non-associative algebras. We prove that computationally, qualitative representations have many advantages over ordinary representations: whereas many finite relation algebras have only infinite representations, every finite qualitatively representable algebra has a finite qualitative representation; the representability problem for (the atom structures of) finite non-associative algebras is NP-complete; the network satisfaction problem over a finite qualitatively representable algebra is always in NP; the validity of equations over qualitative representations is co-NP-complete. On the other hand we prove that there is no finite axiomatisation of the class of qualitatively representable algebras."
            },
            {
                "reference_title": "An Algebra of Qualitative Taxonomical Relations for Ontology Alignments",
                "reference_link": "publication/281472549_An_Algebra_of_Qualitative_Taxonomical_Relations_for_Ontology_Alignments",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2015",
                "reference_abstract": "Algebras of relations were shown useful in managing ontology alignments. They make it possible to aggregate alignments disjunctively or conjunctively and to propagate alignments within a network of ontologies. The previously considered algebra of relations contains taxonomical relations between classes. However, compositional inference using this algebra is sound only if we assume that classes which occur in alignments have nonempty extensions. Moreover, this algebra covers relations only between classes. Here we introduce a new algebra of relations, which, first, solves the limitation of the previous one, and second, incorporates all qualitative taxonomical relations that occur between individuals and concepts, including the relations \u201cis a\u201d and \u201cis not\u201d. We prove that this algebra is coherent with respect to the simple semantics of alignments."
            },
            {
                "reference_title": "Some Varieties Containing Relation Algebras",
                "reference_link": "publication/247590685_Some_Varieties_Containing_Relation_Algebras",
                "reference_type": "Article",
                "reference_date": "Aug 1982",
                "reference_abstract": "ABSTRACT. Three varieties of algebras are introduced which extend the variety RA of\nrelation algebras. They are obtained from RA by weakening the associative law for\nrelative product, and are consequently called nonassociative, weakly-associative and\nsemiassociative relation algebras, or NA, WA, and SA, respectively. Each of these\nvarieties arises naturally in solving various problems concerning relation algebras.\nWe show, for example, that WA is the only one of these varieties which is closed\nunder the formation of complex algebras of atom structures of algebras, and that\nWA is the closure of the variety of representable RA's under relativization. The\npaper also contains a study of the elementary theories of these varieties, various\nrepresentation theorems, and numerous examples."
            },
            {
                "reference_title": "Graduate Texts in Mathematics",
                "reference_link": "publication/321497762_Graduate_Texts_in_Mathematics",
                "reference_type": "Article",
                "reference_date": "Jan 1998",
                "reference_abstract": "The roots of Borel sets go back to the work of Baire [8]. He was trying to come to grips with the abstract notion of a function introduced by Dirich\u00ad let and Riemann. According to them, a function was to be an arbitrary correspondence between objects without giving any method or procedure by which the correspondence could be established. Since all the specific functions that one studied were determined by simple analytic expressions, Baire delineated those functions that can be constructed starting from con\u00ad tinuous functions and iterating the operation 0/ pointwise limit on a se\u00ad quence 0/ functions. These functions are now known as Baire functions. Lebesgue [65] and Borel [19] continued this work. In [19], Borel sets were defined for the first time. In his paper, Lebesgue made a systematic study of Baire functions and introduced many tools and techniques that are used even today. Among other results, he showed that Borel functions coincide with Baire functions. The study of Borel sets got an impetus from an error in Lebesgue's paper, which was spotted by Souslin. Lebesgue was trying to prove the following: Suppose / : )R2 -- R is a Baire function such that for every x, the equation /(x,y) = 0 has a. unique solution. Then y as a function 0/ x defined by the above equation is Baire."
            },
            {
                "reference_title": "Algebraic Calculi for Weighted Ontology Alignments",
                "reference_link": "publication/313066157_Algebraic_Calculi_for_Weighted_Ontology_Alignments",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2016",
                "reference_abstract": "Alignments between ontologies usually come with numerical attributes expressing the confidence of each correspondence. Semantics supporting such confidences must generalise the semantics of alignments without confidence. There exists a semantics which satisfies this but introduces a discontinuity between weighted and non-weighted interpretations. Moreover, it does not provide a calculus for reasoning with weighted ontology alignments. This paper introduces a calculus for such alignments. It is given by an infinite relation-type algebra, the elements of which are weighted taxonomic relations. In addition, it approximates the non-weighted case in a continuous manner."
            },
            {
                "reference_title": "A Survey of Qualitative Spatial and Temporal Calculi -- Algebraic and Computational Properties",
                "reference_link": "publication/303750072_A_Survey_of_Qualitative_Spatial_and_Temporal_Calculi_--_Algebraic_and_Computational_Properties",
                "reference_type": "Article",
                "reference_date": "Jun 2016",
                "reference_abstract": "Qualitative Spatial and Temporal Reasoning (QSTR) is concerned with symbolic knowledge representation, typically over infinite domains. The motivations for employing QSTR techniques range from exploiting computational properties that allow efficient reasoning to capture human cognitive concepts in a computational framework. The notion of a qualitative calculus is one of the most prominent QSTR formalisms. This article presents the first overview of all qualitative calculi developed to date and their computational properties, together with generalized definitions of the fundamental concepts and methods, which now encompass all existing calculi. Moreover, we provide a classification of calculi according to their algebraic properties."
            },
            {
                "reference_title": "Maintaining Knowledge about Temporal Intervals",
                "reference_link": "publication/283925025_Maintaining_Knowledge_about_Temporal_Intervals",
                "reference_type": "Article",
                "reference_date": "Jan 2013",
                "reference_abstract": "We have described a system for reasoning about temporal intervals that is both expressive and computationally effective. The representation captures the temporal hierarchy implicit in many domains by using a hierarchy of reference intervals, which precisely control the amount of deduction performed automatically by the system. This approach is partially partially useful in domains where temporal information is imprecise and relative, and techniques such as dating are not possible. \u00a9 1990 Morgan Kaufmann Publishers, Inc. Published by Elsevier Inc. All rights reserved."
            },
            {
                "reference_title": "Boolean Algebras with Operators. Part I",
                "reference_link": "publication/243779915_Boolean_Algebras_with_Operators_Part_I",
                "reference_type": "Article",
                "reference_date": "Oct 1951",
                "reference_abstract": null
            },
            {
                "reference_title": "Composing Cardinal Direction Relations",
                "reference_link": "publication/222657759_Composing_Cardinal_Direction_Relations",
                "reference_type": "Article",
                "reference_date": "Jul 2001",
                "reference_abstract": "We study the recent proposal of Goyal and Egenhofer who presented a model for qualitative spatial reasoning about cardinal directions. Our approach is formal and complements the presentation of Goyal and Egenhofer. We focus our efforts on the composition operator for two cardinal direction relations. We consider two interpretations of the composition operator: consistency-based and existential composition. We point out that the only published method to compute the consistency-based composition does not always work correctly. Then, we consider progressively more expressive classes of cardinal direction relations and give consistency-based composition algorithms for these classes. Our theoretical framework allows us to prove formally that our algorithms are correct. When we consider existential composition, we demonstrate that the binary relation resulting from the composition of two cardinal direction relations cannot be expressed using the relations defined by Goyal and Egenhofer. Finally, we discuss some extensions to the basic model and consider the composition problem for these extensions."
            },
            {
                "reference_title": "Atom structures of cylindric algebras and relation algebras",
                "reference_link": "publication/222501574_Atom_structures_of_cylindric_algebras_and_relation_algebras",
                "reference_type": "Article",
                "reference_date": "Dec 1997",
                "reference_abstract": "For any finite n \u2a7e 3 there are two atomic n-dimensional cylindric algebras with the same atom structure, with one representable, the other, not.Hence, the complex algebra of the atom structure of a representable atomic cylindric algebra is not always representable, so that the class RCAn of representable n-dimensional cylindric algebras is not closed under completions. Further, it follows by an argument of Venema that RCAn is not axiomatisable by Sahlqvist equations, and hence nor by equations where negation can only occur in constant terms.Similar results hold for relation algebras."
            }
        ]
    },
    {
        "title": "Optimisation of deep neural networks for identification of epileptic abnormalities from electroencephalogram signals",
        "date": "December 2020",
        "doi": "10.1016/j.heliyon.2020.e05694",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (25)",
        "abstract": "An electroencephalogram (EEG) measures and records the electrical activity of the brain. It provides valuable information that can be used to identify epileptic abnormalities. However, the visual identification of such abnormalities from EEG signals by expert neurologists is time consuming. Therefore, several researchers have proposed using deep neural networks (DNNs) to automate the identification of these abnormalities. Their studies have examined the use of different numbers of layers, different numbers of parameters, and various operation types arranged in different architectures. This paper presents the shallowest 11-layer DNN architecture capable of classifying three classes of EEG signals: normal, preictal, and seizure. When the proposed architecture was applied to the standard University of Bonn EEG signal dataset, it achieved accuracy, specificity, and sensitivity values of 99.43%, 99.57%, and 99.10%, respectively. It not only had a better performance than the state of the art DNN architectures, but also had shallower layers with fewer parameters. This allowed it to more quickly identify epileptic abnormalities. Experiments were also conducted where the length of the EEG signals was reduced to 65% (2,662 samples with a period of 15.26 s), which in turn minimised the total parameters of the proposed architecture so that it was comparable to the smallest state-of-the-art architecture and decreased the lag time for identification. Even in these experiments, it was capable of producing equal performance measures, with the execution time reduced to only 69% of that when employing the full length of EEG signals.",
        "reference": [
            {
                "reference_title": "MMSFL-OWFB: A novel class of orthogonal wavelet filters for epileptic seizure detection",
                "reference_link": "publication/326294123_MMSFL-OWFB_A_novel_class_of_orthogonal_wavelet_filters_for_epileptic_seizure_detection",
                "reference_type": "Article",
                "reference_date": "Jul 2018",
                "reference_abstract": "The optimal filters with minimal bandwidth are highly desirable in many applications such as communication and biomedical signal processing. In this study, we design optimally frequency localized orthogonal wavelet filters and evaluate their performance using electroencephalogram (EEG) signals for automated detection of epileptic seizure. The paper presents a novel method for designing optimal orthogonal wavelet filter banks (OWFB) with the objective of minimizing their frequency spreads. The designed wavelet filter also possesses the desired degree of regularity. The regularity condition has been imposed analytically so as to satisfy the constraint accurately. We propose a novel semi-definite programming (SDP) formulation which does not involve any parametrization. The solution of the SDP yields optimal orthogonal wavelet filter for the given length of the filter. We have developed an automated diagnosis system that identifies epileptic seizure EEG signals using the features obtained from the designed minimally mean squared frequency localized (MMSFL) OWFB. We have tested the performance of the proposed model using two independent EEG databases in order to ensure the consistency and robustness of the model. Interestingly, the proposed MMSFL-OWFB feature-based model exhibits ceiling level of performance, with classification accuracy \u2265 99% in classifying seizure (ictal) and seizure-free (non-ictal) EEG signals for both databases. Our developed system can be employed in hospitals and community cares to aid the epileptologists in the accurate diagnosis of seizures."
            },
            {
                "reference_title": "Deep convolutional neural network for the automated detection and diagnosis of seizure using EEG signals",
                "reference_link": "publication/320072603_Deep_convolutional_neural_network_for_the_automated_detection_and_diagnosis_of_seizure_using_EEG_signals",
                "reference_type": "Article",
                "reference_date": "Sep 2017",
                "reference_abstract": "An encephalogram (EEG) is a commonly used ancillary test to aide in the diagnosis of epilepsy. The EEG signal contains information about the electrical activity of the brain. Traditionally, neurologists employ direct visual inspection to identify epileptiform abnormalities. This technique can be time-consuming, limited by technical artifact, provides variable results secondary to reader expertise level, and is limited in identifying abnormalities. Therefore, it is essential to develop a computer-aided diagnosis (CAD) system to automatically distinguish the class of these EEG signals using machine learning techniques. This is the first study to employ the convolutional neural network (CNN) for analysis of EEG signals. In this work, a 13-layer deep convolutional neural network (CNN) algorithm is implemented to detect normal, preictal, and seizure classes. The proposed technique achieved an accuracy, specificity, and sensitivity of 88.67%, 90.00% and 95.00%, respectively."
            },
            {
                "reference_title": "Tunable-Q wavelet transform based multiscale entropy measure for automated classification of epileptic EEG signals",
                "reference_link": "publication/315782576_Tunable-Q_wavelet_transform_based_multiscale_entropy_measure_for_automated_classification_of_epileptic_EEG_signals",
                "reference_type": "Article",
                "reference_date": "Apr 2017",
                "reference_abstract": "This paper analyzes the underlying complexity and non-linearity of electroencephalogram (EEG) signals by computing a novel multi-scale entropy measure for the classification of seizure, seizure-free and normal EEG signals. The quality factor (Q) based multi-scale entropy measure is proposed to compute the entropy of the EEG signal in different frequency-bands of interest. The Q -based entropy (QEn) is computed by decomposing the signal with the tunable-Q wavelet transform (TQWT) into the number of sub-bands and estimating K-nearest neighbor (K-NN) entropies from various sub-bands cumulatively. The optimal selection of Q and the redundancy parameter (R) of TQWT showed better robustness for entropy computation in the presence of high- and low-frequency components. The extracted features are fed to the support vector machine (SVM) classifier with the wrapper-based feature selection method. The proposed method has achieved accuracy of 100% in classifying normal (eyes-open and eyes-closed) and seizure EEG signals, 99.5% in classifying seizure-free EEG signals (from the hippocampal formation of the opposite hemisphere of the brain) from seizure EEG signals and 98% in classifying seizure-free EEG signals (from the epileptogenic zone) from seizure EEG signals, respectively, using the SVM classifier. We have also achieved classification accuracies of 99% and 98.6% in classifying seizure versus non-seizure EEG signals and the individual three classes, namely normal, seizure-free and seizure EEG signals, respectively. The performance measure of the proposed multi-scale entropy has been found to be comparable with the existing state of the art epileptic EEG signals classification methods studied using the same database."
            },
            {
                "reference_title": "A new approach to characterize epileptic seizures using analytic time-frequency flexible wavelet transform and fractal dimension",
                "reference_link": "publication/315582822_A_new_approach_to_characterize_epileptic_seizures_using_analytic_time-frequency_flexible_wavelet_transform_and_fractal_dimension",
                "reference_type": "Article",
                "reference_date": "Mar 2017",
                "reference_abstract": "The identification of seizure activities in non-stationary electroencephalography (EEG) is a challenging task. The seizure detection by human inspection of EEG signals is prone to errors, inaccurate as well as time-consuming. Several attempts have been made to develop automatic systems so as to assist neurophysiologists in identifying epileptic seizures accurately. The proposed study brings forth a novel automatic approach to detect epileptic seizures using analytic time-frequency flexible wavelet transform (ATFFWT) and fractal dimension (FD). The ATFFWT has inherent attractive features such as, shift-invariance property, tunable oscillatory attribute and flexible time-frequency covering favorable for the analysis of non-stationary and transient signals. We have used ATFFWT to decompose EEG signals into the desired subbands. Following the ATFFWT decomposition, we calculate FD for each subband. Finally, FDs of all subbands have been fed to the least-squares support vector machine (LS-SVM) classifier. The 10-fold cross validation has been used to obtain stable and reliable performance and to avoid the over fitting of the model. In this study, we investigate various different classification problems (CPs) pertaining to different classes of EEG signals, including the following popular CPs: (i) ictal versus normal (ii) ictal versus inter-ictal (iii) ictal versus non-ictal. The proposed model is found to be outperforming all existing models in terms of classification sensitivity (CSE) as it achieves perfect 100% sensitivity for seven CPs investigated by us. The prominent attribute of the proposed system is that though the model employs only one set of discriminating features (FD) for all CPs, it yields promising classification accuracy. Since, the proposed model attains the perfect classification performance it appears that a system is in place to assist clinicians to diagnose seizures accurately in less time. Further, the proposed system seems useful and attractive, especially, in the rural areas of developing countries where there is a shortage of experienced clinicians and expensive machines like functional magnetic resonance imaging (fMRI)."
            },
            {
                "reference_title": "A Multivariate Approach for Patient-Specific EEG Seizure Detection Using Empirical Wavelet Transform",
                "reference_link": "publication/312043417_A_Multivariate_Approach_for_Patient-Specific_EEG_Seizure_Detection_Using_Empirical_Wavelet_Transform",
                "reference_type": "Article",
                "reference_date": "Jan 2017",
                "reference_abstract": "Objective: This paper investigates the multivariate oscillatory nature of electroencephalogram (EEG) signals in\nadaptive frequency scales for epileptic seizure detection. \n\nMethods:\nThe empirical wavelet transform (EWT) has been explored for the multivariate signals in order to determine the joint instantaneous amplitudes and frequencies in signal adaptive frequency scales. The proposed multivariate extension of EWT has been studied on multivariate multi-component synthetic signal, as well as on multivariate EEG signals of CHB-MIT scalp EEG database. In a moving window based analysis, two\nseconds long multivariate EEG signal epochs containing five automatically selected channels have been decomposed and three features have been extracted from each one second part of the two second long joint instantaneous amplitudes of multivariate EEG signals. The extracted features from each oscillatory level\nhave been processed using a proposed feature processing step and joint features have been computed in order to achieve better discrimination of seizure and seizure-free EEG signal epochs.\n\nResults: The proposed detection method has been evaluated over 177 hours of EEG records using six well known classifiers. We have achieved average sensitivity, specificity and accuracy values as 97.91% and 99.57% and 99.41% respectively, using ten-fold cross-validation method, which are significantly higher than the existing state of art methods studied on this database. \n\nConclusion: Efficient detection of epileptic seizure is possible when seizure events appear for long duration in hours long EEG recordings. Significance: The proposed method develops time-frequency plane for multivariate signals and builds patient specific models for EEG seizure detection."
            },
            {
                "reference_title": "A Study on Human Activity Recognition Using Accelerometer Data from Smartphones",
                "reference_link": "publication/270979872_A_Study_on_Human_Activity_Recognition_Using_Accelerometer_Data_from_Smartphones",
                "reference_type": "Article",
                "reference_date": "Dec 2014",
                "reference_abstract": "This paper describes how to recognize certain types of human physical activities using acceleration data generated by a user's cell phone. We propose a recognition system in which a new digital low-pass filter is designed in order to isolate the component of gravity acceleration from that of body acceleration in the raw data. The system was trained and tested in an experiment with multiple human subjects in real-world conditions. Several classifiers were tested using various statistical features. High-frequency and low-frequency components of the data were taken into account. We selected five classifiers each offering good performance for recognizing our set of activities and investigated how to combine them into an optimal set of classifiers. We found that using the average of probabilities as the fusion method could reach an overall accuracy rate of 91.15%."
            },
            {
                "reference_title": "A Survey of the Usages of Deep Learning for Natural Language Processing",
                "reference_link": "publication/340820178_A_Survey_of_the_Usages_of_Deep_Learning_for_Natural_Language_Processing",
                "reference_type": "Article",
                "reference_date": "Apr 2020",
                "reference_abstract": "Over the last several years, the field of natural language processing has been propelled forward by an explosion in the use of deep learning models. This article provides a brief introduction to the field and a quick overview of deep learning architectures and methods. It then sifts through the plethora of recent studies and summarizes a large assortment of relevant contributions. Analyzed research areas include several core linguistic processing issues in addition to many applications of computational linguistics. A discussion of the current state of the art is then provided along with recommendations for future research in the field."
            },
            {
                "reference_title": "Auto-detection of Epileptic Seizure events using deep neural network with different feature scaling techniques",
                "reference_link": "publication/336987360_Auto-detection_of_Epileptic_Seizure_events_using_deep_neural_network_with_different_feature_scaling_techniques",
                "reference_type": "Article",
                "reference_date": "Oct 2019",
                "reference_abstract": "Misdiagnosis of epilepsy is more seen in manual analysis of electroencephalogram (EEG) signals for epileptic seizure event detection. Therefore, automated systems for epilepsy detection are required to help neurologists in diagnosing epilepsy. These automated systems act as supporting systems for the neurologists to diagnose epilepsy with good accuracy in less time. In this paper an attempt is made to develop an automated seizure detection method using deep neural network using the dataset collected from Bonn University, Germany. The results of the experiment are compared with the existing machine learning method. Our model gives better results compared to ML methods without the need of feature extraction. It is important to perform normalization of the dataset using feature scaling techniques to obtain good accuracy in the results. In this experiment we also worked on feature scaling of the dataset. At first we tried using StandardScaler and calculated loss using mean squared error. For this we achieved an accuracy of 97.21%, Sensitivity 98.17%, Specificity 94.93%, F1_score 98.48%, MCC 91.96% and ROC 97.55%. Experiment was continued to compare the performance of four different feature scaling techniques and four different loss functions. From the experimental results it was observed that StandardScaler and RobustScaler are equally good and are the best feature scaling techniques. Loss computed using Mean squared error works better in combination with all feature scaling techniques."
            },
            {
                "reference_title": "An Automated System for Epilepsy Detection using EEG Brain Signals based on Deep Learning Approach",
                "reference_link": "publication/322537718_An_Automated_System_for_Epilepsy_Detection_using_EEG_Brain_Signals_based_on_Deep_Learning_Approach",
                "reference_type": "Article",
                "reference_date": "Jan 2018",
                "reference_abstract": "Epilepsy is a neurological disorder and for its detection, encephalography (EEG) is a commonly used clinical approach. Manual inspection of EEG brain signals is a time-consuming and laborious process, which puts heavy burden on neurologists and affects their performance. Several automatic techniques have been proposed using traditional approaches to assist neurologists in detecting binary epilepsy scenarios e.g. seizure vs. non-seizure or normal vs. ictal. These methods do not perform well when classifying ternary case e.g. ictal vs. normal vs. inter-ictal; the maximum accuracy for this case by the state-of-the-art-methods is 97+-1%. To overcome this problem, we propose a system based on deep learning, which is an ensemble of pyramidal one-dimensional convolutional neural network (P-1D-CNN) models. In a CNN model, the bottleneck is the large number of learnable parameters. P-1D-CNN works on the concept of refinement approach and it results in 60% fewer parameters compared to traditional CNN models. Further to overcome the limitations of small amount of data, we proposed augmentation schemes for learning P-1D-CNN model. In almost all the cases concerning epilepsy detection, the proposed system gives an accuracy of 99.1+-0.9% on the University of Bonn dataset."
            },
            {
                "reference_title": "Improved spiking neural networks for EEG classification and epilepsy and seizure detection",
                "reference_link": "publication/262350118_Improved_spiking_neural_networks_for_EEG_classification_and_epilepsy_and_seizure_detection",
                "reference_type": "Article",
                "reference_date": "Aug 2007",
                "reference_abstract": "The goal of this research is to develop an efficient SNN model for epilepsy and epileptic seizure detection using electroencephalograms (EEGs), a complicated pattern recognition problem. Three training algorithms are investigated: SpikeProp (using both incremental and batch processing), QuickProp, and RProp. Since the epilepsy and epileptic seizure detection problem requires a large training dataset the efficacy of these algorithms is investigated by first applying them to the XOR and Fisher iris benchmark problems. Three measures of performance are investigated: number of convergence epochs, computational efficiency, and classification accuracy. Extensive parametric analysis is performed to identify heuristic rules and optimum parameter values that increase the computational efficiency and classification accuracy. The result is a remarkable increase in computational efficiency. For the XOR problem, the computational efficiency of SpikeProp, QuickProp, and RProp is increased by a factor of 588, 82, and 75, respectively, compared with the results reported in the literature. EEGs from three different subject groups are analyzed: (a) healthy subjects, (b) epileptic subjects during a seizure-free interval, and (c) epileptic subjects during a seizure. It is concluded that RProp is the best training algorithm because it has the highest classification accuracy among all training algorithms specially for large size training datasets with about the same computational efficiency provided by SpikeProp. The SNN model for EEG classification and epilepsy and seizure detection uses RProp as training algorithm. This model yields a high classification accuracy of 92.5%."
            }
        ]
    },
    {
        "title": "Code Reviewer Recommendations as a Multi-Objective Problem: Balancing Expertise, Availability and Collaborations",
        "date": "December 2020",
        "doi": "10.1007/s10515-020-00275-6",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (39)",
        "abstract": "Modern Code review is one of the most critical tasks in software maintenance and evolution. A rigorous code review leads to fewer bugs and reduced overall maintenance costs. Most existing studies focus on automatically identifying the most qualified reviewers, based on their expertise, to review pull-up requests. However, the management of code reviews is a complex problem in practice due to a project\u2019s limited resources, including the availability of peer reviewers. Furthermore, the history of collaborations between developers and reviewers could affect the quality of the reviews, in positive or negative ways. In this paper, we formulate the recommendation of code reviewers as a multi-objective search problem to balance the conflicting objectives of expertise, availability, and history of collaborations. Our validation confirms the effectiveness of our multi-objective approach on 9 open source projects by making better recommendations, on average, than the state of the art.",
        "reference": [
            {
                "reference_title": "A tutorial on multiobjective optimization: fundamentals and evolutionary methods",
                "reference_link": "publication/325475570_A_tutorial_on_multiobjective_optimization_fundamentals_and_evolutionary_methods",
                "reference_type": "Article",
                "reference_date": "Sep 2018",
                "reference_abstract": "In almost no other field of computer science, the idea of using bio-inspired search paradigms has been so useful as in solving multiobjective optimization problems. The idea of using a population of search agents that collectively approximate the Pareto front resonates well with processes in natural evolution, immune systems, and swarm intelligence. Methods such as NSGA-II, SPEA2, SMS-EMOA, MOPSO, and MOEA/D became standard solvers when it comes to solving multiobjective optimization problems. This tutorial will review some of the most important fundamentals in multiobjective optimization and then introduce representative algorithms, illustrate their working principles, and discuss their application scope. In addition, the tutorial will discuss statistical performance assessment. Finally, it highlights recent important trends and closely related research fields. The tutorial is intended for readers, who want to acquire basic knowledge on the mathematical foundations of multiobjective optimization and state-of-the-art methods in evolutionary multiobjective optimization. The aim is to provide a starting point for researching in this active area, and it should also help the advanced reader to identify open research topics."
            },
            {
                "reference_title": "MORE: A multi\u2010objective refactoring recommendation approach to introducing design patterns and fixing code smells",
                "reference_link": "publication/314265561_MORE_A_multi-objective_refactoring_recommendation_approach_to_introducing_design_patterns_and_fixing_code_smells",
                "reference_type": "Article",
                "reference_date": "Mar 2017",
                "reference_abstract": "Refactoring is widely recognized as a crucial technique applied when evolving object-oriented software systems. If applied well, refactoring can improve different aspects of software quality including readability, maintainability, and extendibility. However, despite its importance and benefits, recent studies report that automated refactoring tools are underused much of the time by software developers. This paper introduces an automated approach for refactoring recommendation, called MORE, driven by 3 objectives: (1) to improve design quality (as defined by software quality metrics), (2) to fix code smells, and (3) to introduce design patterns. To this end, we adopt the recent nondominated sorting genetic algorithm, NSGA-III, to find the best trade-off between these 3 objectives. We evaluated the efficacy of our approach using a benchmark of 7 medium and large open-source systems, 7 commonly occurring code smells (god class, feature envy, data class, spaghetti code, shotgun surgery, lazy class, and long parameter list), and 4 common design pattern types (visitor, factory method, singleton, and strategy). Our approach is empirically evaluated through a quantitative and qualitative study to compare it against 3 different state-of-the art approaches, 2 popular multiobjective search algorithms, and random search. The statistical analysis of the results confirms the efficacy of our approach in improving the quality of the studied systems while successfully fixing 84% of code smells and introducing an average of 6 design patterns. In addition, the qualitative evaluation shows that most of the suggested refactorings (an average of 69%) are considered by developers to be relevant and meaningful."
            },
            {
                "reference_title": "Search-Based Software Library Recommendation Using Multi-Objective Optimization",
                "reference_link": "publication/311165020_Search-Based_Software_Library_Recommendation_Using_Multi-Objective_Optimization",
                "reference_type": "Article",
                "reference_date": "Nov 2016",
                "reference_abstract": "Context: Software library reuse has significantly increased the productivity of software developers, reduced time-to-market and improved software quality and reusability. However, with the growing number of reusable software libraries in code repositories, finding and adopting a relevant software library becomes a fastidious and complex task for developers. Objective: In this paper, we propose a novel approach called LibFinder to prevent missed reuse opportunities during software maintenance and evolution. The goal is to provide a decision support for developers to easily find \u201cuseful\u201d third-party libraries to the implementation of their software systems. Method: To this end, we used the non-dominated sorting genetic algorithm (NSGA-II), a multi-objective search-based algorithm, to find a trade-off between three objectives : 1) maximizing co-usage between a candidate library and the actual libraries used by a given system, 2) maximizing the semantic similarity between a candidate library and the source code of the system, and 3) minimizing the number of recommended libraries. Results: We evaluated our approach on 6,083 different libraries from Maven Central super repository that were used by 32,760 client systems obtained from Github super repository. Our results show that our approach outperforms three other existing search techniques and a state-of-the art approach, not based on heuristic search, and succeeds in recommending useful libraries at an accuracy score of 92%, precision of 51% and recall of 68%, while finding the best trade-off between the three considered objectives. Furthermore, we evaluate the usefulness of our approach in practice through an empirical study on two industrial Java systems with developers. Results show that the top 10 recommended libraries was rated by the original developers with an average of 3.25 out of 5. Conclusion: This study suggests that (1) library usage history collected from different client systems and (2) library semantics/content embodied in library identifiers should be balanced together for an efficient library recommendation technique."
            },
            {
                "reference_title": "Recommending relevant classes for bug reports using multi-objective search",
                "reference_link": "publication/307090396_Recommending_relevant_classes_for_bug_reports_using_multi-objective_search",
                "reference_type": "Conference Paper",
                "reference_date": "Aug 2016",
                "reference_abstract": "Developers may follow a tedious process to find the cause of a bug based on code reviews and reproducing the abnormal behavior. In this paper, we propose an automated approach to finding and ranking potential classes with the respect to the probability of containing a bug based on a bug report description. Our approach finds a good balance between minimizing the number of recommended classes and maximizing the relevance of the proposed solution using a multi-objective optimization algorithm. The relevance of the recommended classes (solution) is estimated based on the use of the history of changes and bug-fixing, and the lexical similarity between the bug report description and the API documentation. We evaluated our system on 6 open source Java projects, using the version of the project before fixing the bug of many bug reports. The experimental results show that the search-based approach significantly outperforms three state-of-the-art methods in recommending relevant files for bug reports. In particular, our multi-objective approach is able to successfully locate the true buggy methods within the top 10 recommendations for over 87% of the bug reports."
            },
            {
                "reference_title": "Multi-Objective Optimization In Theory and Practice II: Metaheuristic Algorithms",
                "reference_link": "publication/332064678_Multi-Objective_Optimization_In_Theory_and_Practice_II_Metaheuristic_Algorithms",
                "reference_type": "Book",
                "reference_date": "Mar 2019",
                "reference_abstract": "This second part of a study on multi-objective optimization (MOO) concerns the use of metaheuristic methods in challenging practical situations. These difficulties include the size of the real-world application, the nonlinearities, and discontinuities of the Pareto-optimal front we may have. Metaheuristic algorithms include evolutionary computation, such as an evolution strategy, evolution programming, and genetic algorithm. Thus, a particle swarm optimization algorithm is inspired by the social behavior of birds in flocks in search of food. Their extension to MOO problems required new concepts, such as Pareto domination ranking and fitness sharing. The goal is to maintain diversity in the population of solutions through successive generations. Collective strategies are possible within a population. Co-evolutionary models involve different populations or species. Solving MOO problems may use a decomposition into a sequence of subproblems. The hybrid evolutionary algorithm is another efficient method in which different metaheuristics or heuristics combine. This book is an attempt to handle the most significant aspects of the real-life complexities in decision-making. This book is typically user-oriented with theoretical and practical aspects. This book reviews and evaluates multi-objective programming models using several software packages. This book includes detailed examples, figures, test functions, and small-size applications from the literature. It is extended to other practical challenges as with many-objective (more than three objectives) optimization problems, and the requirement of parallel computations. This study presents and compares a variety of fifty test functions for which Pareto-fronts are determined. This book uses commercial and free packages like Mathematica \u00ae and SciLab, respectively."
            },
            {
                "reference_title": "Search-Based Peer Reviewers Recommendation in Modern Code Review",
                "reference_link": "publication/309378722_Search-Based_Peer_Reviewers_Recommendation_in_Modern_Code_Review",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2016",
                "reference_abstract": "Code review is of primary importance in modern software development. It is widely recognized that peer review is an efficient and effective practice for improving software quality and reducing defect proneness. For successful review process, peer reviewers should have a deep experience and knowledge with the code being reviewed, and familiar to work and collaborate together. However, one of the main challenging tasks in modern code review is to find the most appropriate reviewers for submitted code changes. So far, reviewers assignment is still a manual, costly and time-consuming task. In this paper, we introduce a search-based approach, namely RevRec, to provide decision-making support for code change submitters and/or reviewers assigners to identify most appropriate peer reviewers for their code changes. RevRec aims at finding reviewers to be assigned for a code change based on their expertise and collaboration in past reviews using genetic algorithm (GA). We evaluated our approach on a benchmark of three open-source software systems, Android, OpenStack, and Qt. Results indicate that RevRec accurately recommends code reviewers with up to 59% of precision and 74% of recall. Our experiments provide evidence that leveraging reviewers expertise from their prior reviews and the socio-technical aspects of the team work and collaboration is relevant in improving the performance of peer reviewers recommendation in modern code review."
            },
            {
                "reference_title": "Who should review this change?: Putting text and file location analyses together for more accurate recommendations",
                "reference_link": "publication/308734549_Who_should_review_this_change_Putting_text_and_file_location_analyses_together_for_more_accurate_recommendations",
                "reference_type": "Conference Paper",
                "reference_date": "Sep 2015",
                "reference_abstract": null
            },
            {
                "reference_title": "Investigating code review quality: Do people and participation matter?",
                "reference_link": "publication/308730155_Investigating_code_review_quality_Do_people_and_participation_matter",
                "reference_type": "Conference Paper",
                "reference_date": "Sep 2015",
                "reference_abstract": null
            },
            {
                "reference_title": "Bi-level Identification of Web Service Defects",
                "reference_link": "publication/308338218_Bi-level_Identification_of_Web_Service_Defects",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2016",
                "reference_abstract": "Successful Web services must evolve to remain relevant (e.g. requirements update, bugs fix, etc.), but this process of evolution increases complexity and can cause the Web service interface design to decay and lead to significantly reduced usability and popularity of the services. Maintaining a high level of design quality is extremely expensive due to monetary and time pressures that force programmers to neglect improving the quality of their interfaces. A more fundamental reason is that there is little support to automatically identify design defects at the Web service interface level and reduce the high calibration effort to determine manually the threshold value for each quality metric to identify design defects. In this paper, we propose to treat the generation of interface design defects detection rules as a bi-level optimization problem. To this end, the upper level problem generates a set of detection rules, as combination of quality metrics, which maximizes the coverage of a base of defects examples extracted from several Web services and artificial defects generated by the lower level. The lower level maximizes the number of generated artificial defects that cannot be detected by the rules produced by the upper level. The statistical analysis of our experiments over 30 runs on a benchmark of 415 Web services shows that 8 types of Web service defects were detected with an average of more than 93 % of precision and 98 % recall. The results confirm the outperformance of our bi-level proposal compared to state-of-art Web service design defects detection techniques and the survey performed by potential users and programmers also shows the relevance of the detected defects."
            },
            {
                "reference_title": "Process Aspects and Social Dynamics of Contemporary Code Review: Insights from Open Source Development as well as Industrial Practice at Microsoft",
                "reference_link": "publication/303847498_Process_Aspects_and_Social_Dynamics_of_Contemporary_Code_Review_Insights_from_Open_Source_Development_as_well_as_Industrial_Practice_at_Microsoft",
                "reference_type": "Article",
                "reference_date": "Jan 2016",
                "reference_abstract": "Many open source and commercial developers practice contemporary code review, a lightweight, informal, tool-based code review process. To better understand this process and its benefits, we gathered information about code review practices via surveys of open source software developers and developers from Microsoft. The results of our analysis suggest that developers spend approximately 10-15 percent of their time in code reviews, with the amount of effort increasing with experience. Developers consider code review important, stating that in addition to finding defects, code reviews offer other benefits, including knowledge sharing, community building, and maintaining code quality. The quality of the code submitted for review helps reviewers form impressions about their teammates, which can influence future collaborations. We found a large amount of similarity between the Microsoft and OSS respondents. One interesting difference is that while OSS respondents view code review as an important method of impression formation, Microsoft respondents found knowledge dissemination to be more important. Finally, we found little difference between distributed and co-located Microsoft teams. Our findings identify the following key areas that warrant focused research: 1) exploring the non-technical benefits of code reviews, 2) helping developers in articulating review comments, and 3) assisting reviewers' program comprehension during code reviews."
            }
        ]
    },
    {
        "title": "CIDO, a community-based ontology for coronavirus disease knowledge and data integration, sharing, and analysis",
        "date": "December 2020",
        "doi": "10.1038/s41597-020-0523-6",
        "conferance": null,
        "citations_count": "Citations (7)",
        "reference_count": "References (29)",
        "abstract": "The Coronavirus Infectious Disease Ontology (CIDO) is a community-based ontology that supports coronavirus disease knowledge and data standardization, integration, sharing, and analysis.",
        "reference": [
            {
                "reference_title": "The Infectious Disease Ontology in the Age of COVID-19",
                "reference_link": "publication/340952581_The_Infectious_Disease_Ontology_in_the_Age_of_COVID-19",
                "reference_type": "Preprint",
                "reference_date": "May 2020",
                "reference_abstract": "Background\nEfforts to respond effectively to public health emergencies, such as we are now experiencing with COVID-19, require data sharing across multiple disciplines, and this is hindered by the fact that relevant information is often collected using discipline-specific terminologies and coding systems and stored in heterogenous databases. Ontologies provide a powerful data sharing and integration tool. In practice, however, this method is often undermined by uncoordinated ontology development. Following the principles of the Open Biomedical Ontologies Foundry, the Infectious Disease Ontology (IDO) represents one step towards overcoming such silo problems.ResultsIDO is a suite of interoperable ontology modules that aims to provide coverage of all aspects of the infectious disease domain, including biomedical research, clinical care, and public health. IDO Core is designed to be a disease and pathogen neutral ontology, covering just those types of entities and relations that are relevant to infectious diseases generally. IDO Core is then extended by a collection of ontology modules focusing on specific diseases and pathogens. In this paper we present applications of IDO Core together with an overview of all IDO extension ontologies and the methodology on the basis of which they are built. We also survey recent developments involving IDO, including: IDO Virus (VIDO); the Coronavirus Infectious Disease Ontology (CIDO); and an extension of CIDO focusing on COVID-19 (IDO-COVID-19). We discuss how these ontologies might assist in information-driven efforts to deal with the ongoing COVID-19 pandemic, to accelerate data discovery in the early stages of future pandemics, and to promote reproducibility of infectious disease research.Conclusions\nAs we face the continued threat of novel pathogens in the future, IDO provides a simple recipe for building new pathogen-specific ontologies in a way that allows data about novel diseases to be easily compared, along multiple dimensions, with already curated data from earlier diseases. IDO\u2019s tightly coordinated suite of ontologies modules provides a powerful method of data integration and sharing that will allow physicians, researchers, and public health organizations to respond rapidly and efficiently both to the current and future public health crises."
            },
            {
                "reference_title": "COVID-19 coronavirus vaccine design using reverse vaccinology and machine learning",
                "reference_link": "publication/340089618_COVID-19_coronavirus_vaccine_design_using_reverse_vaccinology_and_machine_learning",
                "reference_type": "Preprint",
                "reference_date": "Mar 2020",
                "reference_abstract": "To ultimately combat the emerging COVID-19 pandemic, it is desired to develop an effective and safe vaccine against this highly contagious disease caused by the SARS-CoV-2 coronavirus. Our literature and clinical trial survey showed that the whole virus, as well as the spike (S) protein, nucleocapsid (N) protein, and membrane protein, have been tested for vaccine development against SARS and MERS. We further used the Vaxign reverse vaccinology tool and the newly developed Vaxign-ML machine learning tool to predict COVID-19 vaccine candidates. The N protein was found to be conserved in the more pathogenic strains (SARS/MERS/COVID-19), but not in the other human coronaviruses that mostly cause mild symptoms. By investigating the entire proteome of SARS-CoV-2, six proteins, including the S protein and five non-structural proteins (nsp3, 3CL-pro, and nsp8-10) were predicted to be adhesins, which are crucial to the viral adhering and host invasion. The S, nsp3, and nsp8 proteins were also predicted by Vaxign-ML to induce high protective antigenicity. Besides the commonly used S protein, the nsp3 protein has not been tested in any coronavirus vaccine studies and was selected for further investigation. The nsp3 was found to be more conserved among SARS-CoV-2, SARS-CoV, and MERS-CoV than among 15 coronaviruses infecting human and other animals. The protein was also predicted to contain promiscuous MHC-I and MHC-II T-cell epitopes, and linear B-cell epitopes localized in specific locations and functional domains of the protein. Our predicted vaccine targets provide new strategies for effective and safe COVID-19 vaccine development."
            },
            {
                "reference_title": "Comparative Analysis of Eleven Healthcare-Associated Outbreaks of Middle East Respiratory Syndrome Coronavirus (Mers-Cov) from 2015 to 2017",
                "reference_link": "publication/333088919_Comparative_Analysis_of_Eleven_Healthcare-Associated_Outbreaks_of_Middle_East_Respiratory_Syndrome_Coronavirus_Mers-Cov_from_2015_to_2017",
                "reference_type": "Article",
                "reference_date": "May 2019",
                "reference_abstract": "Since its emergence in 2012, 2,260 cases and 803 deaths due to Middle East respiratory syndrome coronavirus (MERS-CoV) have been reported to the World Health Organization. Most cases were due to transmission in healthcare settings, sometimes causing large outbreaks. We analyzed epidemiologic and clinical data of laboratory-confirmed MERS-CoV cases from eleven healthcare-associated outbreaks in the Kingdom of Saudi Arabia and the Republic of Korea between 2015\u20132017. We quantified key epidemiological differences between outbreaks. Twenty-five percent (n = 105/422) of MERS cases who acquired infection in a hospital setting were healthcare personnel. In multivariate analyses, age \u226565 (OR 4.8, 95%CI: 2.6\u20138.7) and the presence of underlying comorbidities (OR: 2.7, 95% CI: 1.3\u20135.7) were associated with increased mortality whereas working as healthcare personnel was protective (OR 0.07, 95% CI: 0.01\u20130.34). At the start of these outbreaks, the reproduction number ranged from 1.0 to 5.7; it dropped below 1 within 2 to 6 weeks. This study provides a comprehensive characterization of MERS HCA-outbreaks. Our results highlight heterogeneities in the epidemiological profile of healthcare-associated outbreaks. The limitations of our study stress the urgent need for standardized data collection for high-threat respiratory pathogens, such as MERS-CoV."
            },
            {
                "reference_title": "The eXtensible ontology development (XOD) principles and tool implementation to support ontology interoperability",
                "reference_link": "publication/322441127_The_eXtensible_ontology_development_XOD_principles_and_tool_implementation_to_support_ontology_interoperability",
                "reference_type": "Article",
                "reference_date": "Jan 2018",
                "reference_abstract": "Ontologies are critical to data/metadata and knowledge standardization, sharing, and analysis. With hundreds of biological and biomedical ontologies developed, it has become critical to ensure ontology interoperability and the usage of interoperable ontologies for standardized data representation and integration. The suite of web-based Ontoanimal tools (e.g., Ontofox, Ontorat, and Ontobee) support different aspects of extensible ontology development. By summarizing the common features of Ontoanimal and other similar tools, we identified and proposed an \"eXtensible Ontology Development\" (XOD) strategy and its associated four principles. These XOD principles reuse existing terms and semantic relations from reliable ontologies, develop and apply well-established ontology design patterns (ODPs), and involve community efforts to support new ontology development, promoting standardized and interoperable data and knowledge representation and integration. The adoption of the XOD strategy, together with robust XOD tool development, will greatly support ontology interoperability and robust ontology applications to support data to be Findable, Accessible, Interoperable and Reusable (i.e., FAIR)."
            },
            {
                "reference_title": "Therapeutic indications and other use-case-driven updates in the drug ontology: Anti-malarials, anti-hypertensives, opioid analgesics, and a large term request",
                "reference_link": "publication/314206633_Therapeutic_indications_and_other_use-case-driven_updates_in_the_drug_ontology_Anti-malarials_anti-hypertensives_opioid_analgesics_and_a_large_term_request",
                "reference_type": "Article",
                "reference_date": "Mar 2017",
                "reference_abstract": "Background\nThe Drug Ontology (DrOn) is an OWL2-based representation of drug products and their ingredients, mechanisms of action, strengths, and dose forms. We originally created DrOn for use cases in comparative effectiveness research, primarily to identify historically complete sets of United States National Drug Codes (NDCs) that represent packaged drug products, by the ingredient(s), mechanism(s) of action, and so on contained in those products. Although we had designed DrOn from the outset to carefully distinguish those entities that have a therapeutic indication from those entities that have a molecular mechanism of action, we had not previously represented in DrOn any particular therapeutic indication.\n\nResults\nIn this work, we add therapeutic indications for three research use cases: resistant hypertension, malaria, and opioid abuse research. We also added mechanisms of action for opioid analgesics and added 108 classes representing drug products in response to a large term request from the Program for Resistance, Immunology, Surveillance and Modeling of Malaria in Uganda (PRISM) project. The net result is a new version of DrOn, current to May 2016, that represents three major therapeutic classes of drugs and six new mechanisms of action.\n\nConclusions\nA therapeutic indication of a drug product is represented as a therapeutic function in DrOn. Adverse effects of drug products, as well as other therapeutic uses for which the drug product was not designed are dispositions. Our work provides a framework for representing additional therapeutic indications, adverse effects, and uses of drug products beyond their design. Our work also validated our past modeling decisions for specific types of mechanisms of action, namely effects mediated via receptor and/or enzyme binding. DrOn is available at: http://purl.obolibrary.org/obo/dron.owl. A smaller version without NDCs is available at: http://purl.obolibrary.org/obo/dron/dron-lite.owl"
            },
            {
                "reference_title": "The Ontology for Biomedical Investigations",
                "reference_link": "publication/301737190_The_Ontology_for_Biomedical_Investigations",
                "reference_type": "Article",
                "reference_date": "Apr 2016",
                "reference_abstract": "The Ontology for Biomedical Investigations (OBI) is an ontology that provides terms with precisely defined meanings to describe all aspects of how investigations in the biological and medical domains are conducted. OBI re-uses ontologies that provide a representation of biomedical knowledge from the Open Biological and Biomedical Ontologies (OBO) project and adds the ability to describe how this knowledge was derived. We here describe the state of OBI and several applications that are using it, such as adding semantic expressivity to existing databases, building data entry forms, and enabling interoperability between knowledge resources. OBI covers all phases of the investigation process, such as planning, execution and reporting. It represents information and material entities that participate in these processes, as well as roles and functions. Prior to OBI, it was not possible to use a single internally consistent resource that could be applied to multiple types of experiments for these applications. OBI has made this possible by creating terms for entities involved in biological and medical investigations and by importing parts of other biomedical ontologies such as GO, Chemical Entities of Biological Interest (ChEBI) and Phenotype Attribute and Trait Ontology (PATO) without altering their meaning. OBI is being used in a wide range of projects covering genomics, multi-omics, immunology, and catalogs of services. OBI has also spawned other ontologies (Information Artifact Ontology) and methods for importing parts of ontologies (Minimum information to reference an external ontology term (MIREOT)). The OBI project is an open cross-disciplinary collaborative effort, encompassing multiple research communities from around the globe. To date, OBI has created 2366 classes and 40 relations along with textual and formal definitions. The OBI Consortium maintains a web resource (http://obi-ontology.org) providing details on the people, policies, and issues being addressed in association with OBI. The current release of OBI is available at http://purl.obolibrary.org/obo/obi.owl."
            },
            {
                "reference_title": "The FAIR Guiding Principles for scientific data management and stewardship",
                "reference_link": "publication/298345883_The_FAIR_Guiding_Principles_for_scientific_data_management_and_stewardship",
                "reference_type": "Article",
                "reference_date": "Mar 2016",
                "reference_abstract": "There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders\u2014representing academia, industry, funding agencies, and scholarly publishers\u2014have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community."
            },
            {
                "reference_title": "Ontological and Bioinformatic Analysis of Anti-Coronavirus Drugs and Their Implication for Drug Repurposing against COVID-19",
                "reference_link": "publication/340287014_Ontological_and_Bioinformatic_Analysis_of_Anti-Coronavirus_Drugs_and_Their_Implication_for_Drug_Repurposing_against_COVID-19",
                "reference_type": "Preprint",
                "reference_date": "Mar 2020",
                "reference_abstract": "Coronavirus-infected diseases have posed great threats to human health. In past years, highly infectious coronavirus-induced diseases, including COVID-19, SARS, and MERS, have resulted in world-wide severe infections. Our literature annotations identified 72 chemical drugs and 27 antibodies effective against at least one human coronavirus infection in vitro or in vivo. Many of these drugs inhibit viral entry to cells and viral replication inside cells or modulate host immune responses. Many antimicrobial drugs, including antimalarial (e.g., chloroquine and mefloquine) and antifungal (e.g., terconazole and rapamycin) drugs as well as antibiotics (e.g., teicoplanin and azithromycin) were associated with anti-coronavirus activity. A few drugs, including remdesivir, chloroquine phosphate, favipiravir, and tocilizumab, have already been reported to be effective in treating COVID-19. After mapping our identified drugs to three ontologies ChEBI, NDF-RT, and DrON, many features such as roles and mechanisms of action (MoAs) of these drugs were identified and categorized. For example, out of 35 drugs with MoA annotations in NDF-RT, 34 have MoAs of different types of inhibitors and antagonists. Two clustering analyses, one based on ChEBI-based semantic similarity, the other based on drug chemical similarity, were performed to cluster over 60 drugs to new categories. Moreover, PCA analysis of anti-coronavirus drugs found differences in physicochemical properties between those inhibiting viral entry and viral replication. A total of 137 host genes were identified as the targets of 47 anti-coronavirus drugs, resulting in a network of 370 interactions among these drugs and targets. Chlorpromazine, dasatinib, and anisomycin are the hubs of the drug-target network with the highest number of connected target proteins. Many enriched pathways such as calcium signaling and neuroactive ligand-receptor interaction pathways were identified. These findings may be used to facilitate drug repurposing against COVID-19."
            },
            {
                "reference_title": "Classification, Ontology, and Precision Medicine",
                "reference_link": "publication/328226795_Classification_Ontology_and_Precision_Medicine",
                "reference_type": "Article",
                "reference_date": "Oct 2018",
                "reference_abstract": "A goal of precision medicine1 is to stratify patients in order to improve diagnosis and medical treatment. Translational investigators are bringing to bear ever greater amounts of heterogeneous clinical data and scientific information to create classification strategies that enable the matching of intervention to underlying mechanisms of disease in subgroups of patients. Ontologies are systematic representations of knowledge that can be used to integrate and analyze large amounts of heterogeneous data, allowing precise classification of a patient. In this review, we describe ontologies and their use in computational reasoning to support precise classification of patients for diagnosis, care management, and translational research."
            },
            {
                "reference_title": "NCBI GEO: archive for functional genomics data sets - 10years on",
                "reference_link": "publication/285320397_NCBI_GEO_archive_for_functional_genomics_data_sets_-_10years_on",
                "reference_type": "Article",
                "reference_date": "Jan 2012",
                "reference_abstract": "The Gene Expression Omnibus (GEO, http://www.ncbi.nlm.nih.gov/geo/) is an international public repository for high-throughput microarray and next-generation sequence functional genomic data sets submitted by the research community. The resource supports archiving of raw data, processed data and metadata which are indexed, cross-linked and searchable. All data are freely available for download in a variety of formats. GEO also provides several web-based tools and strategies to assist users to query, analyse and visualize data. This article reports current status and recent database developments, including the release of GEO2R, an R-based web application that helps users analyse GEO data."
            }
        ]
    },
    {
        "title": "Coordinating Transitions: Exploring the STEM Institution from the Standpoint of Freshman and Transfer Undergraduate Women",
        "date": "December 2020",
        "doi": "10.1007/s41979-020-00036-w",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (83)",
        "abstract": "Through the lens of feminist standpoint theory, we explore if and how the processes, procedures, and discourses that coordinate the everyday work of transitioning from K12 or a community college created challenges for women students in computer science and physics. Data collection and analysis focused on describing the day-to-day work of transitioning into a computer science or physics major, identifying what coordinated the work (policy, procedure, or discourse), and if and how participants experienced challenges during their transition into their majors. Women students majoring in computer science or physics and transitioning as a freshman or transfer participated in 1\u20133 interviews throughout the study. Participants reported challenges with the transition process into STEM education (e.g. credits from previous institutions not transferring, adjusting to the academic rigor of STEM courses, sense of belonging). The findings also suggested the participants found ways to navigate these challenges through privilege and confidence in their own voice to gain access to information that may not be readily available to other women.",
        "reference": [
            {
                "reference_title": "Problematizing teaching and learning mathematics as \u201cgiven\u201d in STEM education",
                "reference_link": "publication/339008949_Problematizing_teaching_and_learning_mathematics_as_given_in_STEM_education",
                "reference_type": "Article",
                "reference_date": "Dec 2019",
                "reference_abstract": "Mathematics is fundamental for many professions, especially science, technology, and engineering. Yet, mathematics is often perceived as difficult and many students leave disciplines in science, technology, engineering, and mathematics (STEM) as a result, closing doors to scientific, engineering, and technological careers. In this editorial, we argue that how mathematics is traditionally viewed as \u201cgiven\u201d or \u201cfixed\u201d for students\u2019 expected acquisition alienates many students and needs to be problematized. We propose an alternative approach to changes in mathematics education and show how the alternative also applies to STEM education."
            },
            {
                "reference_title": "It's time to recognize how men's careers benefit from sexually harassing women in academia",
                "reference_link": "publication/333610186_It's_time_to_recognize_how_men's_careers_benefit_from_sexually_harassing_women_in_academia",
                "reference_type": "Article",
                "reference_date": "Jun 2019",
                "reference_abstract": "Human Geography"
            },
            {
                "reference_title": "Sexual harassment reported by undergraduate female physicists",
                "reference_link": "publication/332585906_Sexual_harassment_reported_by_undergraduate_female_physicists",
                "reference_type": "Article",
                "reference_date": "Apr 2019",
                "reference_abstract": "Sexual harassment occurs more frequently in male-dominated fields and physics is a more male-dominated field than most other science, technology, engineering, and mathematics (STEM) fields. Thus, it is important to examine the occurrence and impact of sexual harassment on women in physics. A survey of undergraduate women, who attended a conference for undergraduate women in physics, revealed that approximately three quarters (74.3%; 338/455) of survey respondents experienced at least one type of sexual harassment. This sample was recruited from a large fraction of undergraduate women in physics in the United States. We find that certain types of sexual harassment predict a negative sense of belonging and exacerbate the imposter phenomenon. The types of sexual harassment that predict these outcomes, both forms of gender harassment, while seemingly less severe types of harassment, have been found to have substantially negative personal and professional consequences. These findings are important since prior work has found that sense of belonging and the imposter phenomenon are related to students\u2019 persistence in STEM fields. Our results have implications for understanding and improving persistence in physics by informing the community about the occurrence of sexual harassment and its effects so that we can begin to work towards reducing its occurrence and mitigating its effects."
            },
            {
                "reference_title": "Socioeconomic gaps in science achievement",
                "reference_link": "publication/328201137_Socioeconomic_gaps_in_science_achievement",
                "reference_type": "Article",
                "reference_date": "Oct 2018",
                "reference_abstract": "Background\nIn contrast to the extensive research on socioeconomic gaps in reading and math achievement, little attention has been given to socioeconomic disparities in science skills, particularly during the early years of schooling. This emphasis on later years may be problematic because large socioeconomic disparities emerge in the early years, thus it is crucial to document the size of disparities in science achievement and begin unpacking the range of factors that contribute to these disparities. Additionally, it is crucial to know which components of socioeconomic status are more strongly linked to children\u2019s science skills so that resources can be more effectively targeted to address disparities. Using nationally representative data from the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (N = 9250), this study examines disparities in science achievement across elementary and middle school related to parental income and parental education separating their effects from each other and from a range of confounding factors. Additionally, it considers whether characteristics of children, families, and schools are pathways through which socioeconomic disparities emerge.\n\nResults\nResults show moderate gaps in science achievement related to both household income and parental education. The primary pathways through which parental education and family income influenced science achievement was through mathematics and reading achievement. For parental education gaps, smaller indirect effects also operated through access to informal science learning opportunities both inside and outside of the home environment.\n\nConclusion\nFirst, this study highlights the importance of considering the contributions of multiple measures of socioeconomic status, instead of a composite. Second, it shows that socioeconomic disparities in science achievement emerge early and that programs and policies aimed at addressing these gaps may need to target children during the early elementary and preschool years. Third, our findings suggest that elementary instructional approaches that simultaneously address science instruction with reading and/or mathematics instruction will likely be especially important for improving overall science outcomes."
            },
            {
                "reference_title": "Race and gender differences in how sense of belonging influences decisions to major in STEM",
                "reference_link": "publication/324609272_Race_and_gender_differences_in_how_sense_of_belonging_influences_decisions_to_major_in_STEM",
                "reference_type": "Article",
                "reference_date": "Apr 2018",
                "reference_abstract": "Background\nWomen and students of color are widely underrepresented in most STEM fields. In order to investigate this underrepresentation, we interviewed 201 college seniors, primarily women and people of color, who either majored in STEM or started but dropped a STEM major. Here we discuss one section of the longer interview that focused on students\u2019 sense of belonging, which has been found to be related to retention. In our analysis, we examine the intersections of race and gender with students\u2019 sense of belonging, a topic largely absent from the current literature.\n\nResults\nWe found that white men were most likely to report a sense of belonging whereas women of color were the least likely. Further, we found that representation within one\u2019s STEM sub-discipline, namely biology versus the physical sciences, impacts sense of belonging for women. Four key factors were found to contribute to sense of belonging for all students interviewed: interpersonal relationships, perceived competence, personal interest, and science identity.\n\nConclusions\nOur findings indicate that students who remain in STEM majors report a greater sense of belonging than those who leave STEM. Additionally, we found that students from underrepresented groups are less likely to feel they belong. These findings highlight structural and cultural features of universities, as well as STEM curricula and pedagogy, that continue to privilege white males."
            },
            {
                "reference_title": "Institutionalised whiteness, racial microaggressions and black bodies out of place in Higher Education",
                "reference_link": "publication/333347612_Institutionalised_whiteness_racial_microaggressions_and_black_bodies_out_of_place_in_Higher_Education",
                "reference_type": "Article",
                "reference_date": "May 2019",
                "reference_abstract": "On the morning of Friday 3 February 2017, Femi Nylander \u2013 a Black Oxford alumnus \u2013 walked through the grounds of Oxford University\u2019s Harris Manchester College. Later that morning a CCTV image of Femi was circulated to staff and students who were urged to \u2018maintain vigilance\u2019.\nWhilst \u2018post-racial\u2019 ideology insists on framing such incidents as isolated aberrations bereft of wider structural and institutional context, in this article I draw upon the theoretical concepts of racial microaggressions and bodies out of place in order to disrupt this hegemonic interpretation.\nAdopting the Critical Race Theory (CRT) method of counter-narrative, I centralise the voices of student campaigns as sites of legitimate experiential knowledge. These campaigns reveal a web of whiteness that undergirds Higher Education. It is this web, I argue, that ensnares Femi on the day in question. Thus, Femi\u2019s experience cannot be understood in abstraction from structural white supremacy and institutionalised whiteness."
            },
            {
                "reference_title": "When Making the Grade Isn\u2019t Enough: The Gendered Nature of Premed Science Course Attrition",
                "reference_link": "publication/332285901_When_Making_the_Grade_Isn't_Enough_The_Gendered_Nature_of_Premed_Science_Course_Attrition",
                "reference_type": "Article",
                "reference_date": "Apr 2019",
                "reference_abstract": "Women take qualifying exams and enter medical school at substantially lower levels than predicted by their interest in medical degrees at the end of high school. We examined how science course experiences contribute to gendered attrition in premed using a multicohort data set of 8,253 undergraduates taking the traditional premed sequence of introductory science courses at a public research university between 2008 and 2016. Gendered attrition was not based in academic performance, was specific to high-performing women, and yet was grounded in competency beliefs. The result is that high-performing women often graduate with lower paying, lower status degrees. Motivational interventions in premed science courses will be critical for retaining high-performing women in premed, an important outcome with implications for equity and women\u2019s health."
            },
            {
                "reference_title": "Illuminating Low-income Pregnant and Parenting Student Mothers\u2019 Experiences with Community College",
                "reference_link": "publication/332047489_Illuminating_Low-income_Pregnant_and_Parenting_Student_Mothers'_Experiences_with_Community_College",
                "reference_type": "Article",
                "reference_date": "Mar 2019",
                "reference_abstract": "U.S. community colleges are considered historical sites of educational access and opportunity for social mobility for nontraditional students. Theoretically framed in Acker\u2019s theory of gendered organizations, this qualitative study explores low-income pregnant and parenting student mothers\u2019 experiences with community colleges, spaces that are designed for an abstract, \u201cideal student.\u201d Gendered analyses of in-depth interviews from 17 low-income pregnant and parenting student mothers across three community colleges in the Northeastern U.S. reveal compromised classroom and campus wide experiences that negatively impact processes of education and social growth. Findings illustrate the complexity of experiences for this marginalized student group who \u201cdo school\u201d in institutional spaces that fail to meet their needs as students with multiple roles. Recommendations for institutional supports are offered."
            },
            {
                "reference_title": "Where and what are the barriers to progression for female students and academics in UK Higher Education?",
                "reference_link": "publication/328934843_Where_and_what_are_the_barriers_to_progression_for_female_students_and_academics_in_UK_Higher_Education",
                "reference_type": "Article",
                "reference_date": "Nov 2018",
                "reference_abstract": "Gender equality is a high priority for UK universities with research showing that female academics do not progress at the same rate as their male counterparts. Research has been carried out to analyse data across the academic career pathway, from undergraduate student to professorial level, using Uni A as a case study. This research has taken a deductive approach, testing the Leaky Pipeline Theory in Social Science departments, making use of survey data and using qualitative data to interpret quantitative findings. Results showed that there were four stages in the academic career pathway in which women exit at a greater rate than men. This has been shown not to result from a bias in shortlisting or interview, but due to a lack of female applications for progression compared to male applications. To improve this position, UK universities must facilitate part-time working in senior academic positions, recognise talent and empower women to apply for promotion."
            },
            {
                "reference_title": "Gendered Student Ideals in STEM in Higher Education",
                "reference_link": "publication/322108189_Gendered_Student_Ideals_in_STEM_in_Higher_Education",
                "reference_type": "Article",
                "reference_date": "Dec 2017",
                "reference_abstract": "Using the framework of feminist standpoint theory, this study explored the everyday work of undergraduate STEM students to identify STEM institutional cultural norms and standards that organize and inform the organization of everyday work for undergraduate women majoring in math and physics. Data collection and analysis focused on how the interface between undergraduate women and STEM education was organized as a matter of everyday encounters between students, faculty, and administration through their experiences inside and outside the classroom. Undergraduate participants reported challenges meeting some of the characteristics of successful math and physics students (e.g., taking risks, asking questions, putting school first) and preferred a collectivistic environment. These characteristics are evidence of a masculine STEM institution, which also creates a masculine ideal that women students are expected to meet and exacerbates their discomfort in the STEM environment."
            }
        ]
    },
    {
        "title": "The Asymptotics of the Clustering Transition for Random Constraint Satisfaction Problems",
        "date": "December 2020",
        "doi": "10.1007/s10955-020-02635-8",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (34)",
        "abstract": "Random constraint satisfaction problems exhibit several phase transitions when their density of constraints is varied. One of these threshold phenomena, known as the clustering or dynamic transition, corresponds to a transition for an information theoretic problem called tree reconstruction. In this article we study this threshold for two CSPs, namely the bicoloring of k-uniform hypergraphs with a density \u03b1 of constraints, and the q-coloring of random graphs with average degree c. We show that in the large k, q limit the clustering transition occurs for \u03b1=2k-1k(lnk+lnlnk+\u03b3d+o(1)), c=q(lnq+lnlnq+\u03b3d+o(1)), where \u03b3d is the same constant for both models. We characterize \u03b3d via a functional equation, solve the latter numerically to estimate \u03b3d\u22480.871, and obtain an analytic lowerbound \u03b3d\u22651+ln(2(2-1))\u22480.812. Our analysis unveils a subtle interplay of the clustering transition with the rigidity (naive reconstruction) threshold that occurs on the same asymptotic scale at \u03b3r=1.",
        "reference": [
            {
                "reference_title": "The hard-core model on random graphs revisited",
                "reference_link": "publication/239524502_The_hard-core_model_on_random_graphs_revisited",
                "reference_type": "Article",
                "reference_date": "Jun 2013",
                "reference_abstract": "We revisit the classical hard-core model, also known as independent set and\ndual to vertex cover problem, where one puts particles with a first-neighbor\nhard-core repulsion on the vertices of a random graph. Although the case of\nrandom graphs with small and very large average degrees respectively are quite\nwell understood, they yield qualitatively different results and our aim here is\nto reconciliate these two cases. We revisit results that can be obtained using\nthe (heuristic) cavity method and show that it provides a closed-form\nconjecture for the exact density of the densest packing on random regular\ngraphs with degree K>=20, and that for K>16 the nature of the phase transition\nis the same as for large K. This also shows that the hard-code model is the\nsimplest mean-field lattice model for structural glasses and jamming."
            },
            {
                "reference_title": "Nonnormal Small Jump Approximation of Infinitely Divisible Distributions",
                "reference_link": "publication/236263987_Nonnormal_Small_Jump_Approximation_of_Infinitely_Divisible_Distributions",
                "reference_type": "Article",
                "reference_date": "Apr 2013",
                "reference_abstract": "We consider a type of nonnormal approximation of infinitely divisible\ndistributions that incorporates compound Poisson, Gamma, and normal\ndistributions. The approximation relies on achieving higher orders of cumulant\nmatching, to obtain higher rates of approximation error decay. The parameters\nof the approximation are easy to fix. The computational complexity of random\nsampling of the approximating distribution in many cases is of the same order\nas normal approximation. Error bounds in terms of total variance distance are\nderived. Both the univariate and the multivariate cases of the approximation\nare considered."
            },
            {
                "reference_title": "Biased landscapes for random constraint satisfaction problems",
                "reference_link": "publication/331359976_Biased_landscapes_for_random_constraint_satisfaction_problems",
                "reference_type": "Article",
                "reference_date": "Feb 2019",
                "reference_abstract": "The typical complexity of constraint satisfaction problems (CSPs) can be investigated by means of random ensembles of instances. The latter exhibit many threshold phenomena besides their satisfiability phase transition, in particular a clustering or dynamic phase transition (related to the tree reconstruction problem) at which their typical solutions shatter into disconnected components. In this paper we study the evolution of this phenomenon under a bias that breaks the uniformity among solutions of one CSP instance, concentrating on the bicoloring of k-uniform random hypergraphs. We show that for small k the clustering transition can be delayed in this way to higher density of constraints, and that this strategy has a positive impact on the performances of simulated annealing algorithms. We characterize the modest gain that can be expected in the large k limit from the simple implementation of the biasing idea studied here. This paper contains also a contribution of a more methodological nature, made of a review and extension of the methods to determine numerically the discontinuous dynamic transition threshold."
            },
            {
                "reference_title": "An Introduction to Probability Theory and its Applications.",
                "reference_link": "publication/324932789_An_Introduction_to_Probability_Theory_and_its_Applications",
                "reference_type": "Article",
                "reference_date": "Jan 1951",
                "reference_abstract": null
            },
            {
                "reference_title": "Phase transitions in the $q$-coloring of random hypergraphs",
                "reference_link": "publication/318315872_Phase_transitions_in_the_q-coloring_of_random_hypergraphs",
                "reference_type": "Article",
                "reference_date": "Jul 2017",
                "reference_abstract": "We study in this paper the structure of solutions in the random hypergraph coloring problem and the phase transitions they undergo when the density of constraints is varied. Hypergraph coloring is a constraint satisfaction problem where each constraint includes $K$ variables that must be assigned one out of $q$ colors in such a way that there are no monochromatic constraints, i.e. there are at least two distinct colors in the set of variables belonging to every constraint. This problem generalizes naturally coloring of random graphs ($K=2$) and bicoloring of random hypergraphs ($q=2$), both of which were extensively studied in past works. The study of random hypergraph coloring gives us access to a case where both the size $q$ of the domain of the variables and the arity $K$ of the constraints can be varied at will. Our work provides explicit values and predictions for a number of phase transitions that were discovered in other constraint satisfaction problems but never evaluated before in hypergraph coloring. Among other cases we revisit the hypergraph bicoloring problem ($q=2$) where we find that for $K=3$ and $K=4$ the colorability threshold is not given by the one-step-replica-symmetry-breaking analysis as the latter is unstable towards more levels of replica symmetry breaking. We also unveil and discuss the coexistence of two different 1RSB solutions in the case of $q=2$, $K \\ge 4$. Finally we present asymptotic expansions for the density of constraints at which various phase transitions occur, in the limit where $q$ and/or $K$ diverge."
            },
            {
                "reference_title": "Proof of the Satisfiability Conjecture for Large k",
                "reference_link": "publication/267760132_Proof_of_the_Satisfiability_Conjecture_for_Large_k",
                "reference_type": "Article",
                "reference_date": "Nov 2014",
                "reference_abstract": "We establish the satisfiability threshold for random k-SAT for all k >= k_0.\nThat is, there exists a limiting density alpha_s(k) such that a random k-SAT\nformula of clause density alpha is with high probability satisfiable for alpha\n< alpha_s(k), and unsatisfiable for alpha > alpha_s(k). The satisfiability\nthreshold alpha_s(k) is given explicitly by the one-step replica symmetry\nbreaking prediction from statistical physics. We believe that our methods may\napply to a range of random CSPs in the 1RSB universality class."
            },
            {
                "reference_title": "Statistical Physics of Spin Glasses and Information Processing\u2014An Introduction",
                "reference_link": "publication/266517844_Statistical_Physics_of_Spin_Glasses_and_Information_Processing-An_Introduction",
                "reference_type": "Article",
                "reference_date": "Jul 2001",
                "reference_abstract": null
            },
            {
                "reference_title": "The freezing threshold for k-colourings of a random graph",
                "reference_link": "publication/239761530_The_freezing_threshold_for_k-colourings_of_a_random_graph",
                "reference_type": "Article",
                "reference_date": "May 2012",
                "reference_abstract": "We rigorously determine the exact freezing threshold, rkf, for k-colourings of a random graph. We prove that for random graphs with density above rkf, almost every colouring is such that a linear number of variables are frozen, meaning that their colours cannot be changed by a sequence of alterations whereby we change the colours of o(n) vertices at a time, always obtaining another proper colouring. When the density is below rkf, then almost every colouring has at most o(n) frozen variables. This confirms hypotheses made using the non-rigorous cavity method. It has been hypothesized that the freezing threshold is the cause of the \"algorithmic barrier\", the long observed phenomenon that when the edge-density of a random graph exceeds hf k ln k(1+ok(1)), no algorithms are known to find k-colourings, despite the fact that this density is only half the k-colourability threshold.\nWe also show that rkf is the threshold of a strong form of reconstruction for k-colourings of the Galton-Watson tree, and of the graphical model."
            },
            {
                "reference_title": "Ratio of The Tail of An Infinitely Divisible Distribution on The Line to That of Its Levy Measure",
                "reference_link": "publication/228436980_Ratio_of_The_Tail_of_An_Infinitely_Divisible_Distribution_on_The_Line_to_That_of_Its_Levy_Measure",
                "reference_type": "Article",
                "reference_date": "Jan 2010",
                "reference_abstract": "A necessary and sufficient condition for the tail of an infinitely divisible distribution on the real line to be estimated by the tail of its Levy measure is found. The lower limit and the upper limit of the ratio of the right tail (mu) over bar (r) of an infinitely divisible distribution mu to the right tail (nu) over bar (r) of its Levy measure nu as r -> infinity are estimated from above and below by reviving Teugels's classical method. The exponential class and the dominated varying class are studied in detail."
            },
            {
                "reference_title": "Computers And Intractability: A Guide to the Theory of NP-Completeness",
                "reference_link": "publication/228057735_Computers_And_Intractability_A_Guide_to_the_Theory_of_NP-Completeness",
                "reference_type": "Chapter",
                "reference_date": "Jan 1979",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Invariants for Time-Series Constraints",
        "date": "December 2020",
        "doi": "10.1007/s10601-020-09308-z",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (40)",
        "abstract": "Many constraints restricting the result of some computations over an integer sequence can be compactly represented by counter automata. We improve the propagation of the conjunction of such constraints on the same sequence by synthesising a database of linear and non-linear invariants using their counter-automaton representation. The obtained invariants are formulae parameterised by the sequence length and proven to be true for any long enough sequence. To assess the quality of such linear invariants, we developed a method to verify whether a generated linear invariant is a facet of the convex hull of the feasible points. This method, as well as the proof of non-linear invariants, are based on the systematic generation of constant-size deterministic finite automata that accept all integer sequences whose result verifies some simple condition. We apply such methodology to a set of 44 time-series constraints and obtain 1400 linear invariants from which 70% are facet defining, and 600 non-linear invariants, which were tested on short-term electricity production problems.",
        "reference": [
            {
                "reference_title": "Deriving Generic Bounds for Time-Series Constraints Based on Regular Expressions Characteristics",
                "reference_link": "publication/315457683_Deriving_Generic_Bounds_for_Time-Series_Constraints_Based_on_Regular_Expressions_Characteristics",
                "reference_type": "Article",
                "reference_date": "Jan 2018",
                "reference_abstract": "We introduce the concept of regular expression characteristics as a unified way to concisely express bounds on time-series constraints. This allows us not only to define time-series constraints in a compositional way, but also to deal with their combinatorial aspect in a compositional way, without developing ad-hoc bounds for each time-series constraint separately."
            },
            {
                "reference_title": "Describing and Generating Solutions for the EDF Unit Commitment Problem with the ModelSeeker",
                "reference_link": "publication/280852131_Describing_and_Generating_Solutions_for_the_EDF_Unit_Commitment_Problem_with_the_ModelSeeker",
                "reference_type": "Conference Paper",
                "reference_date": "Sep 2013",
                "reference_abstract": "We present an application to extract and solve constraint models from sample solutions of the Unit Commitment Problem of EDF, which computes the power output for each power plant in France as a 48 hour time series. Our aim is to describe and automatically generate the plant-specific model constraints common to the optimal solutions obtained over multiple days. The proposed system generates specific domains for each variable (i.e., time slot), binary constraints between consecutive time slots, and global constraints with functional dependencies over the entire time series. We employ time series clustering techniques for finding stronger constraints and we identify plant-specific time intervals, for which we add additional global constraints. A custom search routine and the generated models allow us to produce solutions corresponding to many overlapping global constraints. Our tool is based on the ModelSeeker [4], but specializes and extends that system for this specific application domain. Results indicate that useful models can be generated with this process."
            },
            {
                "reference_title": "Functional description of sequence constraints and synthesis of combinatorial objects",
                "reference_link": "publication/329863118_Functional_description_of_sequence_constraints_and_synthesis_of_combinatorial_objects",
                "reference_type": "Thesis",
                "reference_date": "Sep 2018",
                "reference_abstract": "Contrary to the standard approach consisting in introducing ad hoc constraints and designing dedicated algorithms for handling their combinatorial aspect, this thesis takes another point of view. On the one hand, it focusses on describing a family of sequence constraints in a compositional way by multiple layers of functions. On the other hand, it addresses the combinatorial aspect of both a single constraint and a conjunction of such constraints by synthesising compositional combinatorial objects, namely bounds, linear inequalities, non-linear constraints and finite automata. These objects are obtained in a systematic way and are not instance-specific: they are parameterised by one or several constraints, by the number of variables in a considered sequence of variables, and by the initial domains of the variables. When synthesising such objects we draw full benefit both from the declarative view of such constraints, based on regular expressions, and from the operational view, based on finite transducers and register automata.There are many advantages of synthesising combinatorial objects rather than designing dedicated algorithms: 1) parameterised formulae can be applied in the context of several resolution techniques such as constraint programming or linear programming, whereas algorithms are typically tailored to a specific technique; 2) combinatorial objects can be combined together to provide better performance in practice; 3) finally, the quantities computed by some formulae cannot just be used in an optimisation setting, but also in the context of data mining."
            },
            {
                "reference_title": "Automatic Generation of Descriptions of Time-Series Constraints",
                "reference_link": "publication/325635486_Automatic_Generation_of_Descriptions_of_Time-Series_Constraints",
                "reference_type": "Conference Paper",
                "reference_date": "Nov 2017",
                "reference_abstract": null
            },
            {
                "reference_title": "Generating Linear Invariants for a Conjunction of Automata Constraints",
                "reference_link": "publication/319231359_Generating_Linear_Invariants_for_a_Conjunction_of_Automata_Constraints",
                "reference_type": "Conference Paper",
                "reference_date": "Aug 2017",
                "reference_abstract": "We propose a systematic approach for generating linear implied constraints that link the values returned by several automata with accumulators after consuming the same input sequence. The method handles automata whose accumulators are increased by (or reset to) some non-negative integer value on each transition. We evaluate the impact of the generated linear invariants on conjunctions of two families of time-series constraints."
            },
            {
                "reference_title": "Learning Disjunctions of Predicates",
                "reference_link": "publication/317650166_Learning_Disjunctions_of_Predicates",
                "reference_type": "Article",
                "reference_date": "Jun 2017",
                "reference_abstract": "Let $F$ be a set of boolean functions. We present an algorithm for learning $F_\\vee := \\{\\vee_{f\\in S} f \\mid S \\subseteq F\\}$ from membership queries. Our algorithm asks at most $|F| \\cdot OPT(F_\\vee)$ membership queries where $OPT(F_\\vee)$ is the minimum worst case number of membership queries for learning $F_\\vee$. When $F$ is a set of halfspaces over a constant dimension space or a set of variable inequalities, our algorithm runs in polynomial time. The problem we address has practical importance in the field of program synthesis, where the goal is to synthesize a program that meets some requirements. Program synthesis has become popular especially in settings aiming to help end users. In such settings, the requirements are not provided upfront and the synthesizer can only learn them by posing membership queries to the end user. Our work enables such synthesizers to learn the exact requirements while bounding the number of membership queries."
            },
            {
                "reference_title": "An efficient algorithm for determining the convex hull of a finite planar set",
                "reference_link": "publication/313645872_An_efficient_algorithm_for_determining_the_convex_hull_of_a_finite_planar_set",
                "reference_type": "Article",
                "reference_date": "Jan 1972",
                "reference_abstract": null
            },
            {
                "reference_title": "Systematic Derivation of Bounds and Glue Constraints for Time-Series Constraints",
                "reference_link": "publication/306392208_Systematic_Derivation_of_Bounds_and_Glue_Constraints_for_Time-Series_Constraints",
                "reference_type": "Conference Paper",
                "reference_date": "Sep 2016",
                "reference_abstract": "Integer time series are often subject to constraints on the aggregation of the integer features of all occurrences of some pattern within the series. For example, the number of inflexions may be constrained, or the sum of the peak maxima, or the minimum of the peak widths. It is currently unknown how to maintain domain consistency efficiently on such constraints. We propose parametric ways of systematically deriving glue constraints, which are a particular kind of implied constraints, as well as aggregation bounds that can be added to the decomposition of time-series constraints [5]. We evaluate the beneficial propagation impact of the derived implied constraints and bounds, both alone and together."
            },
            {
                "reference_title": "Time-Series Constraints: Improvements and Application in CP and MIP Contexts",
                "reference_link": "publication/302973578_Time-Series_Constraints_Improvements_and_Application_in_CP_and_MIP_Contexts",
                "reference_type": "Conference Paper",
                "reference_date": "May 2016",
                "reference_abstract": "A checker for a constraint on a variable sequence can often be compactly specified by an automaton, possibly with accumulators, that consumes the sequence of values taken by the variables; such an automaton can also be used to decompose its specified constraint into a conjunction of logical constraints. The inference achieved by this decomposition in a CP solver can be boosted by automatically generated implied constraints on the accumulators, provided the latter are updated in the automaton transitions by linear expressions. Automata with non-linear accumulator updates can be automatically synthesised for a large family of time-series constraints. In this paper, we describe and evaluate extensions to those techniques. First, we improve the automaton synthesis to generate automata with fewer accumulators. Second, we decompose a constraint specified by an automaton with accumulators into a conjunction of linear inequalities, for use by a MIP solver. Third, we generalise the implied constraint generation to cover the entire family of time-series constraints. The newly synthesised automata for time-series constraints outperform the old ones, for both the CP and MIP decompositions, and the generated implied constraints boost the inference, again for both the CP and MIP decompositions. We evaluate CP and MIP solvers on a prototypical application modelled using time-series constraints."
            },
            {
                "reference_title": "Using finite transducers for describing and synthesising structural time-series constraints",
                "reference_link": "publication/281299324_Using_finite_transducers_for_describing_and_synthesising_structural_time-series_constraints",
                "reference_type": "Article",
                "reference_date": "Aug 2015",
                "reference_abstract": "We describe a large family of constraints for structural time series by means of function composition. These constraints are on aggregations of features of patterns that occur in a time series, such as the number of its peaks, or the range of its steepest ascent. The patterns and features are usually linked to physical properties of the time series generator, which are important to capture in a constraint model of the system, i.e. a conjunction of constraints that produces similar time series. We formalise the patterns using finite transducers, whose output alphabet corresponds to semantic values that precisely describe the steps for identifying the occurrences of a pattern. Based on that description, we automatically synthesise automata with accumulators, as well as constraint checkers. The description scheme not only unifies the structure of the existing 30 time-series constraints in the Global Constraint Catalogue, but also leads to over 600 new constraints, with more than 100,000 lines of synthesised code."
            }
        ]
    },
    {
        "title": "Using Game Theory in Public Domains: The Potential and Limitations of Security Games",
        "date": "December 2020",
        "doi": "10.2478/nispa-2020-0024",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (42)",
        "abstract": "Since its origins, when it was mainly connected to the field of economics, game theory has brought important theoretic insights into many domains. Besides biology, philosophy or computer science, its findings have been applied to various fields of public policy. One specific area of public policy is that of security. Within the last two decades we have been witnesses to a significant increase in efforts to model security issues using tools of game theory and to derive political implications. The paper deals with the model of a Stackelberg security game and its real-world applications in security domains. The main aim and purpose of the paper is to provide a survey of selected cases of real-world deployed applications of the game-theoretic Stackelberg model in the area of public security and, based on the literature analysis, to discuss the potential and limitations of the model for policy- and decision-makers that are dealing with security measures on various governmental levels. Existing cases clearly indicate that the model can contribute to a better design and implementation of the security policy and help implement a better allocation of resources and thus potentially improve the effectiveness of security measures. On the other hand, the paper also discusses some limitations and potential future adjustments of the model together with points for further research.",
        "reference": [
            {
                "reference_title": "On repeated stackelberg security game with the cooperative human behavior model for wildlife protection",
                "reference_link": "publication/328266911_On_repeated_stackelberg_security_game_with_the_cooperative_human_behavior_model_for_wildlife_protection",
                "reference_type": "Article",
                "reference_date": "Mar 2019",
                "reference_abstract": "Inspired by successful deployments of Stackelberg Security Game in real life, researchers are working hard to optimize the game models to make them more practical. Recent security game work on wildlife protection makes a step forward by taking the possible cooperation among attackers into consideration. However, it models attackers to have complete rationality, which is not always possible in practice given they are human beings. We aim to tackle attackers\u2019 bounded rationality in the complicated, cooperation-enabled and multi-round security game for wildlife protection. Specifically, we construct a repeated Stackelberg game, and propose a novel adaptive human behavior model for attackers based on it. Despite generating defender\u2019s optimal strategy requires to solve a non-linear and non-convex optimization problem, we are able to propose an efficient algorithm that approximately solve this problem. We perform extensive real-life experiments, and results from over 25,000 game plays show our solution effectively helps the defender to deal with attackers who might cooperate."
            },
            {
                "reference_title": "Stackelberg Security Games: Models, Applications and Computational Aspects",
                "reference_link": "publication/308937195_Stackelberg_Security_Games_Models_Applications_and_Computational_Aspects",
                "reference_type": "Article",
                "reference_date": "Sep 2016",
                "reference_abstract": "Stackelberg games are non-symmetric games where one player or specified group of players have the privilege position and make decision before the other players. Such games are used in telecommunication and computational systems for supporting administrative decisions. Recently Stackleberg games became useful also in the systems where security issues are the crucial decision criteria. In this paper authors briefly survey the most popular Stackelberg security game models and provide the analysis of the model properties illustrated in the realistic use cases."
            },
            {
                "reference_title": "TRUSTS: Scheduling Randomized Patrols for Fare Inspection in Transit Systems using Game Theory",
                "reference_link": "publication/266501833_TRUSTS_Scheduling_Randomized_Patrols_for_Fare_Inspection_in_Transit_Systems_using_Game_Theory",
                "reference_type": "Article",
                "reference_date": "Dec 2012",
                "reference_abstract": "In proof-of-payment transit systems, passengers are legally required to purchase tickets before entering but are not phys-ically forced to do so. Instead, patrol units move about the transit system, inspecting the tickets of passengers, who face fines if caught fare evading. The deterrence of fare evasion depends on the unpredictability and effectiveness of the pa-trols. In this paper, we present TRUSTS, an application for schedul-ing randomized patrols for fare inspection in transit systems. TRUSTS models the problem of computing patrol strate-gies as a leader-follower Stackelberg game where the ob-jective is to deter fare evasion and hence maximize rev-enue. This problem differs from previously studied Stackel-berg settings in that the leader strategies must satisfy massive temporal and spatial constraints; moreover, unlike in these counterterrorism-motivated Stackelberg applications, a large fraction of the ridership might realistically consider fare eva-sion, and so the number of followers is potentially huge. A third key novelty in our work is deliberate simplification of leader strategies to make patrols easier to be executed. We present an efficient algorithm for computing such patrol strategies and present experimental results using real-world ridership data from the Los Angeles Metro Rail system. The Los Angeles County Sheriff's department is currently carry-ing out trials of TRUSTS."
            },
            {
                "reference_title": "Defense Strategy Selection Method for Stackelberg Security Game Based on Incomplete Information",
                "reference_link": "publication/335779277_Defense_Strategy_Selection_Method_for_Stackelberg_Security_Game_Based_on_Incomplete_Information",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2019",
                "reference_abstract": "Network attack and defense confrontation is essentially a game process between unequal subjects on both sides of attack and defense. There are some defects in the prior hypothesis that the subject status of both sides of the attack and defense game is equal and the active defense ability is insufficient in the research of the existing network defense strategy. In this paper, game theory unequal thought and defender active release incentive technology into network defense strategy generation model construction process, and a Stackelberg Security Game defense strategy generation method based on incomplete information is proposed. By establishing the network model, the strong equilibrium strategy algorithm of Stackelberg Security Game with incomplete information is used to generate the optimal defense strategy of the network. The defender can actively release the incentive strategy without reducing the accuracy of defense strategy generation. The experimental results provide a method for the selection of optimal defense strategy, and can strengthen the security of the system."
            },
            {
                "reference_title": "Trends and applications in stackelberg security games",
                "reference_link": "publication/332109013_Trends_and_applications_in_stackelberg_security_games",
                "reference_type": "Chapter",
                "reference_date": "Aug 2018",
                "reference_abstract": "Security is a critical concern around the world, whether it is the challenge of protecting ports, airports, and other critical infrastructure; interdicting the illegal flow of drugs, weapons, and money; protecting endangered wildlife, forests, and fisheries; or suppressing urban crime or security in cyberspace. Unfortunately, limited security resources prevent full security coverage at all \u00d7 instead, we must optimize the use of limited security resources. To that end, we founded a new \"security games\" framework that has led to building of decision aids for security agencies around the world. Security games are a novel area of research that is based on computational and behavioral game theory while also incorporating elements of AI planning under uncertainty and machine learning. Today security-games-based decision aids for infrastructure security are deployed in the US and internationally; examples include deployments at ports and ferry traffic with the US Coast Guard, for security of air traffic with the US Federal Air Marshals, and for security of university campuses, airports, and metro trains with police agencies in the US and other countries. Moreover, recent work on \"green security games\" has led our decision aids to be deployed, assisting NGOs in protection of wildlife; and \"opportunistic crime security games\" have focused on suppressing urban crime. In cyber-security domain, the interaction between the defender and adversary is quite complicated with high degree of incomplete information and uncertainty. Recently, applications of game theory to provide quantitative and analytical tools to network administrators through defensive algorithm development and adversary behavior prediction to protect cyber infrastructures has also received significant attention. This chapter provides an overview of use-inspired research in security games including algorithms for scaling up security games to real-world sized problems, handling multiple types of uncertainty, and dealing with bounded rationality and bounded surveillance of human adversaries. \u00a9 Springer International Publishing AG, part of Springer Nature 2018."
            },
            {
                "reference_title": "Stackelberg Security Games: Looking Beyond a Decade of Success",
                "reference_link": "publication/326208363_Stackelberg_Security_Games_Looking_Beyond_a_Decade_of_Success",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2018",
                "reference_abstract": "The Stackelberg Security Game (SSG) model has been immensely influential in security research since it was introduced roughly a decade ago. Furthermore, deployed SSG-based applications are one of most successful examples of game theory applications in the real world. We present a broad survey of recent technical advances in SSG and related literature, and then look to the future by highlighting the new potential applications and open research problems in SSG."
            },
            {
                "reference_title": "Attacker Deterrence and Perceived Risk in a Stackelberg Security Game",
                "reference_link": "publication/295078705_Attacker_Deterrence_and_Perceived_Risk_in_a_Stackelberg_Security_Game",
                "reference_type": "Article",
                "reference_date": "Feb 2016",
                "reference_abstract": "In Stackelberg security games, a defender must allocate scarce resources to defend against a potential attacker. The optimal defense involves the randomization of scarce security resources, yet how attackers perceive the risk given randomized defense is not well understood. We conducted an experiment where attackers chose whether to attack or not attack targets protected by randomized defense schemes, the key treatment variable being whether the defender picks one target at random to guard or imperfectly guards all targets. The two schemes are expected-payoff equivalent, and when provided separately we found no effect of having one scheme or the other. Yet, when both are present, we found that subjects had a preference for the fixed scheme, a preference that cannot be reduced to differences in beliefs. Overall, our results suggest that understanding how individuals perceive risk is vital to understand the behavior of attackers."
            },
            {
                "reference_title": "PROTECT - A Deployed Game-Theoretic System for Strategic Security Allocation for the United States Coast Guard",
                "reference_link": "publication/288806493_PROTECT_-_A_Deployed_Game-Theoretic_System_for_Strategic_Security_Allocation_for_the_United_States_Coast_Guard",
                "reference_type": "Article",
                "reference_date": "Dec 2012",
                "reference_abstract": "While three deployed applications of game theory for security have recently been reported, we as a community of agents and AI researchers remain in the early stages of these deployments; there is a continuing need to understand the core principles for innovative security applications of game theory. Toward that end, this article presents PROTECT, a game-theoretic system deployed by the United States Coast Guard (USCG) in the Port of Boston for scheduling its patrols. USCG has termed the deployment of PROTECT in Boston a success; PROTECT is currently being tested in the Port of New York, with the potential for nationwide deployment. PROTECT is premised on an attackerdefender Stackelberg game model and offers five key innovations. First, this system is a departure from the assumption of perfect adversary rationality noted in previous work, relying instead on a quantal response (QR) model of the adversary's behavior - to the best of our knowledge, this is the first real-world deployment of the QR model. Second, to improve PROTECT's efficiency, we generate a compact representation of the defender's strategy space, exploiting equivalence and dominance. Third, we show how to practically model a real maritime patrolling problem as a Stackelberg game. Fourth, our experimental results illustrate that PROTECT's QR model more robustly handles real-world uncertainties than a perfect rationality model. Finally, in evaluating PROTECT, this article for the first time provides real-world data: comparison of human-generated versus PROTECT security schedules, and results from an Adversarial Perspective Team's (human mock attackers) analysis. Copyright \u00a9 2012, Association for the Advancement of Artificial Intelligence."
            },
            {
                "reference_title": "A Stackelberg security game with random strategies based on the extraproximal theoretic approach",
                "reference_link": "publication/266377424_A_Stackelberg_security_game_with_random_strategies_based_on_the_extraproximal_theoretic_approach",
                "reference_type": "Article",
                "reference_date": "Jan 2015",
                "reference_abstract": "In this paper we present a novel approach for representing a real-world attacker\u2013defender Stackelberg security game-theoretic model based on the extraproximal method. We focus on a class of ergodic controlled finite Markov chain games. The extraproximal problem formulation is considered as a nonlinear programming problem with respect to stationary distributions. The Lagrange principle and Tikhonov\u05f3s regularization method are employed to ensure the convergence of the cost functions. We transform the problem into a system of equations in a proximal format, and a two-step (prediction and basic) iterated procedure is applied to solve the formulated problem. In particular, the extraproximal method is employed for computing mixed strategies, providing a strong optimization formulation to compute the Stackelberg/Nash equilibrium. Mixed strategies are especially found when the resources available for both the defender and the attacker are limited. In this sense, each equation in this system is an optimization problem for which the minimum is found using a quadratic programming approach. The model supports a defender and N attackers. In order to address the dynamic execution uncertainty in security patrolling, we provide a game-theoretic based method for scheduling randomized patrols. Simulation results provide a validations of our approach."
            },
            {
                "reference_title": "Homeland Security: The Difference between a Vision and a Wish",
                "reference_link": "publication/263183100_Homeland_Security_The_Difference_between_a_Vision_and_a_Wish",
                "reference_type": "Article",
                "reference_date": "Jan 2002",
                "reference_abstract": "The U.S. government is considering how it should reorganize for homeland security. Consequently, attention has focused on the new Office of Homeland Security (OHS). Much debate focuses on two issues related to OHS: (1) whether the OHS should be a separate executive agency; and (2) whether the OHS director has sufficient authority to direct changes in policies and resource allocation of other departments and agencies. The authors believe the emphasis on these areas of interest is misplaced as measures of the eventual success of the OHS. Rather than focusing on these political issues, this article outlines several questions about how the OHS might approach its complex mission and highlights some organizational and bureaucratic realities that are likely to survive the debate over placement of the OHS within the executive branch and the authorities of the OHS director. This article concludes with a discussion of some organizational tools that the OHS or any coordinating office will require to fulfill its mandate."
            }
        ]
    },
    {
        "title": "Range searching in multidimensional databases using navigation metadata",
        "date": "December 2020",
        "doi": "10.1016/j.amc.2020.125510",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (18)",
        "abstract": "This work presents a new range searching algorithm for multidimensional databases. The proposed methodology is based on the idea of generating a navigation metadata structure, complementary to the database, that eases the navigation between the elements of the database. This metadata structure can be adapted to different problems and it is generated in a one time pre-procesing effort for each database. This work contains a complete description of the algorithm, including a study of its searching performance under different conditions compared with a brute force approach.",
        "reference": [
            {
                "reference_title": "The n-dimensional k-vector and its application to orthogonal range searching",
                "reference_link": "publication/338580664_The_n-dimensional_k-vector_and_its_application_to_orthogonal_range_searching",
                "reference_type": "Article",
                "reference_date": "May 2020",
                "reference_abstract": "This work focuses on the definition and study of the n-dimensional k-vector, an algorithm devised to perform orthogonal range searching in static databases with multiple dimensions. The methodology first finds the order in which to search the dimensions, and then, performs the search using a modified projection method. In order to determine the dimension order, the algorithm uses the k-vector, a range searching technique for one dimension that identifies the number of elements contained in the searching range. Then, using this information, the algorithm predicts and selects the best approach to deal with each dimension. The algorithm has a worst case complexity of O(nd(k/n)^(2/d), where k is the number of elements retrieved, n is the number of elements in the database, and d is the number of dimensions of the database. This work includes a detailed description of the methodology as well as a study of the algorithm performance."
            },
            {
                "reference_title": "R Trees: A Dynamic Index Structure for Spatial Searching",
                "reference_link": "publication/221213205_R_Trees_A_Dynamic_Index_Structure_for_Spatial_Searching",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 1984",
                "reference_abstract": "In order to handle spatial data efficiently, as required in computer aided design and geo-data applications, a database system needs an index mechanism that will help it retrieve data items quickly according to their spatial locations However, traditional indexing methods are not well suited to data objects of non-zero size located m multi-dimensional spaces In this paper we describe a dynamic index structure called an R-tree which meets this need, and give algorithms for searching and updating it. We present the results of a series of tests which indicate that the structure performs well, and conclude that it is useful for current database systems in spatial applications"
            },
            {
                "reference_title": "Comparison of two different tree algorithms",
                "reference_link": "publication/282699568_Comparison_of_two_different_tree_algorithms",
                "reference_type": "Article",
                "reference_date": "Jun 1990",
                "reference_abstract": "The efficiency of two different algorithms of hierarchical force calculation is discussed. Both algorithms utilize the tree structure to reduce the cost of the force calculation from O(N2) to O(N log N). The only difference lies in the method of the construction of the tree. One algorithm uses the oct-tree, which is the recursive division of a cube into eight subcubes. The other method makes the tree by repeatedly replacing a mutually nearest pair in the system by a super-particle. Numerical experiments showed that the cost of the force calculation using these two schemes is quite similar for the same relative accuracy of the obtained force. The construction of the mutual-nearest-neighbor tree is more expensive than the construction of the oct-tree roughly by a factor of 10. On the conventional mainframes this difference is not important because the cost of the tree construction is only a small fraction of the total calculation cost. On vector processors, the oct-tree scheme is currently faster because the tree construction is relatively more expensive on the vector processors."
            },
            {
                "reference_title": "On the (quote)dimensionality curse(quote) and the (quote)self-similarity blessing(quote)",
                "reference_link": "publication/243121119_On_the_quotedimensionality_cursequote_and_the_quoteself-similarity_blessingquote",
                "reference_type": "Article",
                "reference_date": "Jan 2001",
                "reference_abstract": null
            },
            {
                "reference_title": "Organization and maintenance of large ordered indices",
                "reference_link": "publication/242463515_Organization_and_maintenance_of_large_ordered_indices",
                "reference_type": "Article",
                "reference_date": "Jan 1972",
                "reference_abstract": null
            },
            {
                "reference_title": "Efficient tree codes on SIMD computer architectures",
                "reference_link": "publication/238804461_Efficient_tree_codes_on_SIMD_computer_architectures",
                "reference_type": "Article",
                "reference_date": "Nov 1996",
                "reference_abstract": "This paper describes changes made to a previous implementation of an N-body tree code developed for a fine-grained, SIMD computer architecture. These changes include (1) switching from a balanced binary tree to a balanced oct tree, (2) addition of quadrupole corrections, and (3) having the particles search the tree in groups rather than individually. An algorithm for limiting errors is also discussed. In aggregate, these changes have led to a performance increase of over a factor of 10 compared to the previous code. For problems several times larger than the processor array, the code now achieves performance levels of \u223c 1 Gflop on the Maspar MP-2 or roughly 20% of the quoted peak performance of this machine. This percentage is competitive with other parallel implementations of tree codes on MIMD architectures. This is significant, considering the low relative cost of SIMD architectures."
            },
            {
                "reference_title": "Approximate range searching",
                "reference_link": "publication/222839576_Approximate_range_searching",
                "reference_type": "Article",
                "reference_date": "Dec 2000",
                "reference_abstract": "The range searching problem is a fundamental problem in computational geometry, with numerous important applications. Most research has focused on solving this problem exactly, but lower bounds show that if linear space is assumed, the problem cannot be solved in polylogarithmic time, except for the case of orthogonal ranges. In this paper we show that if one is willing to allow approximate ranges, then it is possible to do much better. In particular, given a bounded range Q of diameter w and \u03b5>0, an approximate range query treats the range as a fuzzy object, meaning that points lying within distance \u03b5w of the boundary of Q either may or may not be counted. We show that in any fixed dimension d, a set of n points in can be preprocessed in time and space, such that approximate queries can be answered in time. The only assumption we make about ranges is that the intersection of a range and a d-dimensional cube can be answered in constant time (depending on dimension). For convex ranges, we tighten this to time. We also present a lower bound for approximate range searching based on partition trees of , which implies optimality for convex ranges (assuming fixed dimensions). Finally, we give empirical evidence showing that allowing small relative errors can significantly improve query execution times."
            },
            {
                "reference_title": "A modified tree code: Don't laugh; It runs",
                "reference_link": "publication/222795675_A_modified_tree_code_Don't_laugh_It_runs",
                "reference_type": "Article",
                "reference_date": "Mar 1990",
                "reference_abstract": "I describe a modification of the Barnes-Hut tree algorithm together with a series of numerical tests of this method. The basic idea is to improve the performance of the code on heavily vector-oriented machines such as the Cyber 205 by exploiting the fact that nearby particles tend to have very similar interaction lists. By building an interaction list good everywhere within a cell containing a modest number of particles and reusing this interaction list for each particle in the cell in turn, the balance of computation can be shifted from recursive descent to force summation. Instead of vectorizing tree descent, this scheme simply avoids it in favor of force summation, which is quite easy to vectorize. A welcome side-effect of this modification is that the force calculation, which now treats a larger fraction of the local interactions exactly, is significantly more accurate than the unmodified method."
            },
            {
                "reference_title": "New Data Structures for Orthogonal Range Queries",
                "reference_link": "publication/220617304_New_Data_Structures_for_Orthogonal_Range_Queries",
                "reference_type": "Article",
                "reference_date": "Feb 1985",
                "reference_abstract": "Consider a set of N records corresponding to points in k-dimensional space (k greater than equivalent to 2). This paper introduces one new data structure which uses memory O(N log**k** minus **1 N) for supporting orthogonal range queries with worst-case complexity O(log**k** minus **1 N) and several modifications of this proposal for a dynamic environment. These results are especially useful when k equals 2."
            },
            {
                "reference_title": "Lower Bounds for Orthogonal Range Searching: I. The Reporting Case",
                "reference_link": "publication/220431093_Lower_Bounds_for_Orthogonal_Range_Searching_I_The_Reporting_Case",
                "reference_type": "Article",
                "reference_date": "Apr 1990",
                "reference_abstract": "We establish lower bounds on the complexity of orthogonal range reporting in the static case. Given a collection of n points in d-space and a box [a1, b1] X \u2026 X [ad, bd], report every point whose ith coordinate lies in [ai, bi], for each i = l, \u2026 , d. The collection of points is fixed once and for all and can be preprocessed. The box, on the other hand, constitutes a query that must be answered online. It is shown that on a pointer machine a query time of O(k + polylog(n)), where k is the number of points to be reported, can only be achieved at the expense of &OHgr;(n(log n/log log n)d-1) storage. Interestingly, these bounds are optimal in the pointer machine model, but they can be improved (ever so slightly) on a random access machine. In a companion paper, we address the related problem of adding up weights assigned to the points in the query box."
            }
        ]
    },
    {
        "title": "Network of evolvable neural units can learn synaptic learning rules and spiking dynamics",
        "date": "December 2020",
        "doi": "10.1038/s42256-020-00267-x",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (63)",
        "abstract": "Although deep neural networks have seen great success in recent years through various changes in overall architectures and optimization strategies, their fundamental underlying design remains largely unchanged. Computational neuroscience may provide more biologically realistic models of neural processing mechanisms, but they are still high-level abstractions of empirical behaviour. Here we propose an evolvable neural unit (ENU) that can evolve individual somatic and synaptic compartment models of neurons in a scalable manner. We demonstrate that ENUs can evolve to mimic integrate-and-fire neurons and synaptic spike-timing-dependent plasticity. Furthermore, by constructing a network where an ENU takes the place of each synapse and neuron, we evolve an agent capable of learning to solve a T-maze environment task. This network independently discovers spiking dynamics and reinforcement-type learning rules, opening up a new path towards biologically inspired artificial intelligence.",
        "reference": [
            {
                "reference_title": "Designing neural networks through neuroevolution",
                "reference_link": "publication/330203191_Designing_neural_networks_through_neuroevolution",
                "reference_type": "Article",
                "reference_date": "Jan 2019",
                "reference_abstract": "Deep neural networks have become very successful at certain machine learning tasks partly due to the widely adopted method of training called backpropagation. An alternative way to optimize neural networks is by using evolutionary algorithms, which, fuelled by the increase in computing power, offers a new range of capabilities and modes of learning."
            },
            {
                "reference_title": "Evolving Simple Models of Diverse Intrinsic Dynamics in Hippocampal Neuron Types",
                "reference_link": "publication/323733667_Evolving_Simple_Models_of_Diverse_Intrinsic_Dynamics_in_Hippocampal_Neuron_Types",
                "reference_type": "Article",
                "reference_date": "Mar 2018",
                "reference_abstract": "The diversity of intrinsic dynamics observed in neurons may enhance the computations implemented in the circuit by enriching network-level emergent properties such as synchronization and phase locking. Large-scale spiking network models of entire brain regions offer a platform to test theories of neural computation and cognitive function, providing useful insights on information processing in the nervous system. However, a systematic in-depth investigation requires network simulations to capture the biological intrinsic diversity of individual neurons at a sufficient level of accuracy. The computationally efficient Izhikevich model can reproduce a wide range of neuronal behaviors qualitatively. Previous studies using optimization techniques, however, were less successful in quantitatively matching experimentally recorded voltage traces. In this article, we present an automated pipeline based on evolutionary algorithms to quantitatively reproduce features of various classes of neuronal spike patterns using the Izhikevich model. Employing experimental data from Hippocampome.org, a comprehensive knowledgebase of neuron types in the rodent hippocampus, we demonstrate that our approach reliably fit Izhikevich models to nine distinct classes of experimentally recorded spike patterns, including delayed spiking, spiking with adaptation, stuttering, and bursting. Importantly, by leveraging the parameter-exploration capabilities of evolutionary algorithms, and by representing qualitative spike pattern class definitions in the error landscape, our approach creates several suitable models for each neuron type, exhibiting appropriate feature variabilities among neurons. Moreover, we demonstrate the flexibility of our methodology by creating multi-compartment Izhikevich models for each neuron type in addition to single-point versions. Although the results presented here focus on hippocampal neuron types, the same strategy is broadly applicable to any neural systems."
            },
            {
                "reference_title": "Neuromodulated-Spike-Timing-Dependent Pasticity, and Theory of Three-Factor Learning Rules",
                "reference_link": "publication/291387276_Neuromodulated-Spike-Timing-Dependent_Pasticity_and_Theory_of_Three-Factor_Learning_Rules",
                "reference_type": "Article",
                "reference_date": "Jan 2016",
                "reference_abstract": "Classical Hebbian learning puts the emphasis on joint pre- and postsynaptic activity, but neglects the potential role of neuromodulators. Since neuromodulators convey information about novelty or reward, the influence of neuromodulatorson synaptic plasticity is useful not just for action learning in classical conditioning, but also to decide 'when' to create new memories in response to a flow of sensory stimuli. In this review, we focus on timing requirements for pre- and postsynaptic activity in conjunction with one or several phasic neuromodulatory signals. While the emphasis of the text is on conceptual models and mathematical theories, we also discuss some experimental evidence for neuromodulation of Spike-Timing-Dependent Plasticity. We highlight the importance of synaptic mechanisms in bridging the temporal gap between sensory stimulation and neuromodulatory signals, and develop a framework for a class of neo-Hebbian three-factor learning rules that depend on presynaptic activity, postsynaptic variables as well as the influence of neuromodulators."
            },
            {
                "reference_title": "Deep Learning",
                "reference_link": "publication/277411157_Deep_Learning",
                "reference_type": "Article",
                "reference_date": "May 2015",
                "reference_abstract": "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech."
            },
            {
                "reference_title": "ES is more than just a traditional finite-difference approximator",
                "reference_link": "publication/326168056_ES_is_more_than_just_a_traditional_finite-difference_approximator",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2018",
                "reference_abstract": "An evolution strategy (ES) variant based on a simplification of a natural evolution strategy recently attracted attention because it performs surprisingly well in challenging deep reinforcement learning domains. It searches for neural network parameters by generating perturbations to the current set of parameters, checking their performance, and moving in the aggregate direction of higher reward. Because it resembles a traditional finite-difference approximation of the reward gradient, it can naturally be confused with one. However, this ES optimizes for a different gradient than just reward: It optimizes for the average reward of the entire population, thereby seeking parameters that are robust to perturbation. This difference can channel ES into distinct areas of the search space relative to gradient descent, and also consequently to networks with distinct properties. This unique robustness-seeking property, and its consequences for optimization, are demonstrated in several domains. They include humanoid locomotion, where networks from policy gradient-based reinforcement learning are significantly less robust to parameter perturbation than ES-based policies solving the same task. While the implications of such robustness and robustness-seeking remain open to further study this work's main contribution is to highlight such differences and their potential importance."
            },
            {
                "reference_title": "Long short-term memory and Learning-to-learn in networks of spiking neurons",
                "reference_link": "publication/324055230_Long_short-term_memory_and_Learning-to-learn_in_networks_of_spiking_neurons",
                "reference_type": "Article",
                "reference_date": "Mar 2018",
                "reference_abstract": "Networks of spiking neurons (SNNs) are frequently studied as models for networks of neurons in the brain, but also as paradigm for novel energy efficient computing hardware. In principle they are especially suitable for computations in the temporal domain, such as speech processing, because their computations are carried out via events in time and space. But so far they have been lacking the capability to preserve information for longer time spans during a computation, until it is updated or needed - like a register of a digital computer. This function is provided to artificial neural networks through Long Short-Term Memory (LSTM) units. We show here that SNNs attain similar capabilities if one includes adapting neurons in the network. Adaptation denotes an increase of the firing threshold of a neuron after preceding firing. A substantial fraction of neurons in the neocortex of rodents and humans has been found to be adapting. It turns out that if adapting neurons are integrated in a suitable manner into the architecture of SNNs, the performance of these enhanced SNNs, which we call LSNNs, for computation in the temporal domain approaches that of artificial neural networks with LSTM-units. In addition, the computing and learning capabilities of LSNNs can be substantially enhanced through learning-to-learn (L2L) methods from machine learning, that have so far been applied primarily to LSTM networks and apparently never to SSNs. This preliminary report on arXiv will be replaced by a more detailed version in about a month."
            },
            {
                "reference_title": "Neuroscience-Inspired Artificial Intelligence",
                "reference_link": "publication/318551830_Neuroscience-Inspired_Artificial_Intelligence",
                "reference_type": "Article",
                "reference_date": "Jul 2017",
                "reference_abstract": "The fields of neuroscience and artificial intelligence (AI) have a long and intertwined history. In more recent times, however, communication and collaboration between the two fields has become less commonplace. In this article, we argue that better understanding biological brains could play a vital role in building intelligent machines. We survey historical interactions between the AI and neuroscience fields and emphasize current advances in AI that have been inspired by the study of neural computation in humans and other animals. We conclude by highlighting shared themes that may be key for advancing future research in both fields."
            },
            {
                "reference_title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
                "reference_link": "publication/314943017_Evolution_Strategies_as_a_Scalable_Alternative_to_Reinforcement_Learning",
                "reference_type": "Article",
                "reference_date": "Mar 2017",
                "reference_abstract": "We explore the use of Evolution Strategies, a class of black box optimization algorithms, as an alternative to popular RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using hundreds to thousands of parallel workers, ES can solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training time. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation."
            },
            {
                "reference_title": "An Evolutionary Framework for Replicating Neurophysiological Data with Spiking Neural Networks",
                "reference_link": "publication/307507499_An_Evolutionary_Framework_for_Replicating_Neurophysiological_Data_with_Spiking_Neural_Networks",
                "reference_type": "Conference Paper",
                "reference_date": "Sep 2016",
                "reference_abstract": "Here we present a framework for the automatic tuning of spiking neural networks (SNNs) that utilizes an evolutionary algorithm featuring indirect encoding to achieve a drastic reduction in the dimensionality of the parameter space, combined with a GPU-accelerated SNN simulator that results in a considerable decrease in the time needed for fitness evaluation, despite the need for both a training and a testing phase. We tuned the parameters governing a learning rule called spike-timing-dependent plasticity (STDP), which was used to alter the synaptic weights of the network. We validated this framework by applying it to a case study in which synthetic neuronal firing rates were matched to electrophysiologically recorded neuronal firing rates in order to evolve network functionality. Our framework was not only able to match their firing rates, but also captured functional and behavioral aspects of the biological neuronal population, in roughly 50 generations."
            },
            {
                "reference_title": "The columnar organization of the neocortex",
                "reference_link": "publication/283617640_The_columnar_organization_of_the_neocortex",
                "reference_type": "Article",
                "reference_date": "Apr 1997",
                "reference_abstract": "The modular organization of nervous systems is a widely documented principle of design for both vertebrate and invertebrate brains of which the columnar organization of the neocortex is an example. The classical cytoarchitectural areas of the neocortex are composed of smaller units, local neural circuits repeated iteratively within each area. Modules may vary in cell type and number, in internal and external connectivity, and in mode of neuronal processing between different large entities; within any single large entity they have a basic similarity of internal design and operation. Modules are most commonly grouped into entities by sets of dominating external connections. This unifying factor is most obvious for the heterotypical sensory and motor areas of the neocortex. Columnar defining factors in homotypical areas are generated, in part, within the cortex itself. The set of all modules composing such an entity may be fractionated into different modular subsets by different extrinsic connections. Linkages between them and subsets in other large entities form distributed systems. The neighborhood relations between connected subsets of modules in different entities result in nested distributed systems that serve distributed functions. A cortical area defined in classical cytoarchitectural terms may belong to more than one and sometimes to several distributed systems. Columns in cytoarchitectural areas located at some distance from one another, but with some common properties, may be linked by long-range, intracortical connections."
            }
        ]
    },
    {
        "title": "Recent Methods from Computer Vision for Perception in Highly Automated Driving (Keynote at ACM Computer Science in Cars Symposium 2020)",
        "date": "December 2020",
        "doi": "10.13140/RG.2.2.19349.35048",
        "conferance": "Conference: ACM Comuter Science in Cars Symposium (CSCS) 2020",
        "citations_count": null,
        "reference_count": "References (8)",
        "abstract": "Prof. Dr.-Ing. Tim Fingscheidt holds the Chair of Signal Processing and Machine Learning at Technische Universit\u00e4t Braunschweig. He heads the TU Braunschweig Deep Learning Lab (TUBS.dll), having research interest in machine learning for Speech, Vision, and Predictive Maintenance. In his keynote, he will present selected results from the field of Computer Vision for perception in highly automated driving. The talk will draw from his Vision team\u2019s recent works and will cover some of the following methods: Domain mismatch adaptation, continual learning, adversarial attacks and defenses, multi-task learning and monocular depth estimation.",
        "reference": [
            {
                "reference_title": "Improved Noise and Attack Robustness for Semantic Segmentation by Using Multi-Task Training with Self-Supervised Depth Estimation",
                "reference_link": "publication/342987652_Improved_Noise_and_Attack_Robustness_for_Semantic_Segmentation_by_Using_Multi-Task_Training_with_Self-Supervised_Depth_Estimation",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2020",
                "reference_abstract": "While current approaches for neural network training often aim at improving performance, less focus is put on training methods aiming at robustness towards varying noise conditions or directed attacks by adversarial examples. In this paper, we propose to improve robustness by a multi-task training, which extends supervised semantic segmentation by a self-supervised monocular depth estimation on unlabeled videos. This additional task is only performed during training to improve the semantic segmentation model's robustness at test time under several input perturbations. Moreover, we even find that our joint training approach also improves the performance of the model on the original (supervised) semantic segmentation task. Our evaluation exhibits a particular novelty in that it allows to mutually compare the effect of input noises and adversarial attacks on the robustness of the semantic segmentation. We show the effectiveness of our method on the Cityscapes dataset, where our multi-task training approach consistently outperforms the single-task semantic segmentation baseline in terms of both robustness vs. noise and in terms of adversarial attacks, without the need for depth labels in training."
            },
            {
                "reference_title": "Digging Into Self-Supervised Monocular Depth Estimation",
                "reference_link": "publication/339774433_Digging_Into_Self-Supervised_Monocular_Depth_Estimation",
                "reference_type": "Conference Paper",
                "reference_date": "Nov 2019",
                "reference_abstract": "Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods. Research on self-supervised monocular training usually explores increasingly complex architectures, loss functions, and image formation models, all of which have recently helped to close the gap with fully-supervised methods. We show that a surprisingly simple model, and associated design choices, lead to superior predictions. In particular, we propose (i) a minimum reprojection loss, designed to robustly handle occlusions, (ii) a full-resolution multi-scale sampling method that reduces visual artifacts, and (iii) an auto-masking loss to ignore training pixels that violate camera motion assumptions. We demonstrate the effectiveness of each component in isolation, and show high quality, state-of-the-art results on the KITTI benchmark."
            },
            {
                "reference_title": "Unsupervised Learning of Depth and Ego-Motion from Video",
                "reference_link": "publication/320968448_Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2017",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jun 2020",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Aug 2018",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jun 2020",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Aug 2020",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2020",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Tight Chang's-lemma-type bounds for Boolean functions",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (23)",
        "abstract": "Chang's lemma (Duke Mathematical Journal, 2002) is a classical result with applications across several areas in mathematics and computer science. For a Boolean function $f$ that takes values in {-1,1} let $r(f)$ denote its Fourier rank. For each positive threshold $t$, Chang's lemma provides a lower bound on $wt(f):=\\Pr[f(x)=-1]$ in terms of the dimension of the span of its characters with Fourier coefficients of magnitude at least $1/t$. We examine the tightness of Chang's lemma w.r.t. the following three natural settings of the threshold: - the Fourier sparsity of $f$, denoted $k(f)$, - the Fourier max-supp-entropy of $f$, denoted $k'(f)$, defined to be $\\max \\{1/|\\hat{f}(S)| : \\hat{f}(S) \\neq 0\\}$, - the Fourier max-rank-entropy of $f$, denoted $k''(f)$, defined to be the minimum $t$ such that characters whose Fourier coefficients are at least $1/t$ in absolute value span a space of dimension $r(f)$. We prove new lower bounds on $wt(f)$ in terms of these measures. One of our lower bounds subsumes and refines the previously best known upper bound on $r(f)$ in terms of $k(f)$ by Sanyal (ToC, 2019). Another lower bound is based on our improvement of a bound by Chattopadhyay, Hatami, Lovett and Tal (ITCS, 2019) on the sum of the absolute values of the level-$1$ Fourier coefficients. We also show that Chang's lemma for the these choices of the threshold is asymptotically outperformed by our bounds for most settings of the parameters involved. Next, we show that our bounds are tight for a wide range of the parameters involved, by constructing functions (which are modifications of the Addressing function) witnessing their tightness. Finally we construct Boolean functions $f$ for which - our lower bounds asymptotically match $wt(f)$, and - for any choice of the threshold $t$, the lower bound obtained from Chang's lemma is asymptotically smaller than $wt(f)$.",
        "reference": [
            {
                "reference_title": "An Entropic Proof of Chang's Inequality",
                "reference_link": "publication/224904644_An_Entropic_Proof_of_Chang's_Inequality",
                "reference_type": "Article",
                "reference_date": "May 2012",
                "reference_abstract": "Chang's lemma is a useful tool in additive combinatorics and the analysis of\nBoolean functions. Here we give an elementary proof using entropy. The constant\nwe obtain is tight, and we give a slight improvement in the case where the\nvariables are highly biased."
            },
            {
                "reference_title": "Variations on the Sensitivity Conjecture",
                "reference_link": "publication/47637978_Variations_on_the_Sensitivity_Conjecture",
                "reference_type": "Article",
                "reference_date": "Nov 2010",
                "reference_abstract": "We present a selection of known as well as new variants of the Sensitivity Conjecture and point out some weaker versions that are also open. Comment: 16 pages, 1 table"
            },
            {
                "reference_title": "A polynomial bound in Freiman's theorem",
                "reference_link": "publication/38337268_A_polynomial_bound_in_Freiman's_theorem",
                "reference_type": "Article",
                "reference_date": "Jun 2002",
                "reference_abstract": "In this paper the following improvement on Freiman's theorem on set addition is obtained (see Theorems 1 and 2 in Section 1). \u00b6 Let $A\\subset \\mathbb {Z}$ be a finite set such that $|A+A|<\\alpha|A|$ . Then A is contained in a proper d-dimensional progression P, where $d\\leq [\\alpha -1]$ and \\log (|P|/|A|)<C\\alpha\\sp 2(\\log \\alpha)\\sp 3$ . \u00b6 Earlier bounds involved exponential dependence in \u03b1 in the second estimate. Our argument combines I. Ruzsa's method, which we improve in several places, as well as Y. Bilu's proof of Freiman's theorem."
            },
            {
                "reference_title": "Approximate Constraint Satisfaction Requires Large LP Relaxations",
                "reference_link": "publication/310444977_Approximate_Constraint_Satisfaction_Requires_Large_LP_Relaxations",
                "reference_type": "Article",
                "reference_date": "Oct 2016",
                "reference_abstract": "We prove super-polynomial lower bounds on the size of linear programming relaxations for approximation versions of constraint satisfaction problems. We show that for these problems, polynomial-sized linear programs are no more powerful than programs arising from a constant number of rounds of the Sherali-Adams hierarchy. In particular, any polynomial-sized linear program for MAX CUT has an integrality gap of 12 and any such linear program for MAX 3-SAT has an integrality gap of 7/8."
            },
            {
                "reference_title": "A quantitative improvement for Roth's theorem on arithmetic progressions",
                "reference_link": "publication/262568662_A_quantitative_improvement_for_Roth's_theorem_on_arithmetic_progressions",
                "reference_type": "Article",
                "reference_date": "May 2014",
                "reference_abstract": "We improve the quantitative estimate for Roth's theorem on three-term\narithmetic progressions, showing that if $A\\subset\\{1,\\ldots,N\\}$ contains no\nnon-trivial three-term arithmetic progressions then $\\lvert A\\rvert\\ll\nN(\\log\\log N)^4/\\log N$. By the same method we also improve the bounds in the\nanalogous problem over $\\mathbb{F}_q[t]$ and for the problem of finding long\narithmetic progressions in a sumset."
            },
            {
                "reference_title": "Spectral Structure of Sets of Integers",
                "reference_link": "publication/240258049_Spectral_Structure_of_Sets_of_Integers",
                "reference_type": "Article",
                "reference_date": "Jan 2004",
                "reference_abstract": "Let \u00cb be a small subset of a finite abelian group, and let R be the set of points at which its Fourier transform is large. A result of Chang states that R has a great deal of additive structure. We give a statement and a proof of this result and discuss some applications of it.\nFinally, we discuss some related open questions."
            },
            {
                "reference_title": "Arithmetic progressions in sumsets",
                "reference_link": "publication/225739104_Arithmetic_progressions_in_sumsets",
                "reference_type": "Article",
                "reference_date": "Aug 2002",
                "reference_abstract": "We prove several results concerning arithmetic progressions in sets of integers. Suppose, for example, that a \\alpha and b \\beta are positive reals, that N is a large prime and that \n\nC,D \u00cd \\Bbb Z/N\\Bbb Z C,D \\subseteq {\\Bbb Z}/N{\\Bbb Z} have sizes gN \\gamma N and dN \\delta N respectively. Then the sumset C + D contains an AP of length at least ec \u00d6{log} N e^{c \\sqrt{\\rm log} N} , where c > 0 depends only on g \\gamma and d \\delta . In deriving these results we introduce the concept of hereditary non-uniformity (HNU) for subsets of \n\n\\Bbb Z/N\\Bbb Z {\\Bbb Z}/N{\\Bbb Z} , and prove a structural result for sets with this property."
            },
            {
                "reference_title": "Testing Booleanity and the Uncertainty Principle",
                "reference_link": "publication/222106163_Testing_Booleanity_and_the_Uncertainty_Principle",
                "reference_type": "Article",
                "reference_date": "Apr 2012",
                "reference_abstract": "Let f:{-1,1}^n -> R be a real function on the hypercube, given by its\ndiscrete Fourier expansion, or, equivalently, represented as a multilinear\npolynomial. We say that it is Boolean if its image is in {-1,1}.\nWe show that every function on the hypercube with a sparse Fourier expansion\nmust either be Boolean or far from Boolean. In particular, we show that a\nmultilinear polynomial with at most k terms must either be Boolean, or output\nvalues different than -1 or 1 for a fraction of at least 2/(k+2)^2 of its\ndomain.\nIt follows that given black box access to f, together with the guarantee that\nits representation as a multilinear polynomial has at most k terms, one can\ntest Booleanity using O(k^2) queries. We show an Omega(k) queries lower bound\nfor this problem.\nWe also consider the problem of deciding if a function is Boolean, given its\nexplicit representation as a k term multilinear polynomial. The naive approach\nof evaluating it at every input has O(kn2^n) time complexity. For large k (i.e,\nexponential) we present a simple randomized O(kn sqrt(2^n)) algorithm. For\nsmall k we show how the problem can be solved deterministically in O(k^3n).\nOur proofs crucially use Hirschman's entropic version of Heisenberg's\nuncertainty principle."
            },
            {
                "reference_title": "Some Constructions In The Inverse Spectral Theory Of Cyclic Groups.",
                "reference_link": "publication/220357738_Some_Constructions_In_The_Inverse_Spectral_Theory_Of_Cyclic_Groups",
                "reference_type": "Article",
                "reference_date": "Mar 2003",
                "reference_abstract": "The results of this paper concern the 'large spectra' of sets, by which we mean the set of points in F-p(x) at which the Fourier transform of a characteristic function chi(A), A subset of or equal to F-p, P can be large. We show that a recent result of Chang concerning the structure of the large spectrum is best possible. Chang's result has already found a number of applications in combinatorial number theory. We also show that if \\A\\ = [p/2], and if R is the set of points r for which \\(\u03c7) over cap (A)(r)\\ greater than or equal to alphap, then almost nothing can be said about R other than that \\R\\ much less than alpha(-2), a trivial consequence of Parseval's theorem."
            },
            {
                "reference_title": "On Roth's theorem on progressions",
                "reference_link": "publication/47637728_On_Roth's_theorem_on_progressions",
                "reference_type": "Article",
                "reference_date": "Oct 2010",
                "reference_abstract": "We show that if A is a subset of {1,...,N} contains no non-trivial three-term\narithmetic progressions then |A|=O(N/ log^{1-o(1)} N). The approach is somewhat\ndifferent from that used in arXiv:1007.5444."
            }
        ]
    },
    {
        "title": "Systematic Roadmap to Address the Research Issues and Challenges in Advanced Intelligent Video Surveillance with Deep Learning Techniques",
        "date": "December 2020",
        "doi": null,
        "conferance": "Conference: AICTE Sponsored FDP on\u00a0\"Contemporary Research Issues and Challenges in\u00a0Advanced Intelligent Video Surveillance with Deep Learning Techniques\"",
        "citations_count": null,
        "reference_count": null,
        "abstract": "Presented \"Systematic Roadmap to Address the Research Issues and Challenges in Advanced Intelligent Video Surveillance with Deep Learning Techniques\" on 3rd Dec 2020, Thursday 10.30 AM to 12 PM Online in AICTE Sponsored Faculty Development Programme organized by the Department of Computer Science and Engineering, Muthayammal Engineering College(Autonomous), Rasipuram, Salem, tamil Nadu.",
        "reference": []
    },
    {
        "title": "An image encryption scheme based on public key cryptosystem and quantum logistic map",
        "date": "December 2020",
        "doi": "10.1038/s41598-020-78127-2",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (67)",
        "abstract": "Most of existing image encryption schemes are proposed in the spatial domain which easily destroys the correlation between pixels. This paper proposes an image encryption scheme by employing discrete cosine transform (DCT), quantum logistic map and substitution-permutation network (SPN). The DCT is used to transform the images in the frequency domain. Meanwhile, the SPN is used to provide the security properties of confusion and diffusion. The SPN provides fast encryption as compared to the asymmetric based image encryption since operations with low computational complexity are used (e.g., exclusive-or and permutation). Different statistical experiments and security analysis are performed against six grayscale and color images to justify the effectiveness and security of the proposed image encryption scheme.",
        "reference": [
            {
                "reference_title": "A novel chaotic image encryption algorithm based on improved baker map and logistic map",
                "reference_link": "publication/344438778_A_novel_chaotic_image_encryption_algorithm_based_on_improved_baker_map_and_logistic_map",
                "reference_type": "Article",
                "reference_date": "Aug 2019",
                "reference_abstract": "A novel image encryption algorithm based on double chaotic systems is proposed in this paper. On account of the limited chaotic range and vulnerability of a single chaotic map, we use the two-dimensional Baker chaotic map to control the system parameters and the state variable of the logistic chaotic map. After control, the parameter of the logistic map is varying, and the generated logistic sequence is non-stationary. The improved map has been proven to be random and unpredictable by complexity analysis. Furthermore, a novel image encryption algorithm, including shuffling and substituting processes, is proposed based on the improved chaotic maps. Many statistical tests and security analysis indicate that this algorithm has an excellent security performance, and it can be competitive with some other recently proposed image encryption algorithms."
            },
            {
                "reference_title": "Image encryption using quantum 3-D Baker map and generalized gray code coupled with fractional Chen\u2019s chaotic system",
                "reference_link": "publication/342369517_Image_encryption_using_quantum_3-D_Baker_map_and_generalized_gray_code_coupled_with_fractional_Chen's_chaotic_system",
                "reference_type": "Article",
                "reference_date": "Jun 2020",
                "reference_abstract": "In our efforts to construct a secure quantum image encryption algorithm, we first propose a quantum 3-D Baker map to scramble a 3-D quantum representation of an image. To have this 3-D quantum representation, we harness the NEQR model for a \\(2^n \\times 2^n\\) grayscale image. In the second step of the proposed encryption scheme, we implement a substitution routine which starts by implementing the generalized gray code on the permuted image and concludes with selected intra bit-XOR-ing and XOR-ing with the pseudorandom sequence generated by the Fractional Chen\u2019s system. The encryption scheme utilizes the basic quantum gates like C-NOT, Toffoli, and Ripple-carry adder due to their computational efficiency. The theoretical and numerical simulation results show that the algorithm has the potential to be used as an image encryption algorithm on quantum computers."
            },
            {
                "reference_title": "A novel hybrid encryption scheme based on chaotic Lorenz system and logarithmic key generation",
                "reference_link": "publication/342104296_A_novel_hybrid_encryption_scheme_based_on_chaotic_Lorenz_system_and_logarithmic_key_generation",
                "reference_type": "Article",
                "reference_date": "Aug 2020",
                "reference_abstract": "The digital information is transmitted through different communication channels with ease within no time. The sphere of digital world is evolving and become one of the key aspects of our daily lives. In this regard, confidentially to this digital information which are fundamentally transmitted over insecure line of communication is one of the mandatory concerns. This digital information is placed in different databases available online. In this article, our aim is to perform cryptanalysis of Logarithmic based image encryption scheme and investigate its potential key leakages by utilizing public key to recover its private key. Moreover, we have suggested chaos-based Logarithm scheme which is more secure. The Lorenz system is utilized in an anticipated mechanism which is sensitive to initial conditions and chaotic parameters. The proposed scheme is then authenticated by using available security performance benchmarks. To measure the security against different cryptographic attacks, we utilized the statistical analyses which are entropy and the correlation between the pixels, and the differential analyses which are number of changing pixel rate (NPCR) and the unified averaged changed intensity (UACI). In order to resist the brute force attacks, key sensitivity analyses and key space analyses is also performed. The randomness of proposed encryption scheme investigated through entropy and NIST randomness suit tests. An application on digital images is investigated. Also, we have compared our modified confidentiality scheme with existing benchmarks which suggested our encryption technique is quite reasonable for digital multimedia security."
            },
            {
                "reference_title": "Color image compression and encryption scheme based on compressive sensing and double random encryption strategy",
                "reference_link": "publication/341957673_Color_image_compression_and_encryption_scheme_based_on_compressive_sensing_and_double_random_encryption_strategy",
                "reference_type": "Article",
                "reference_date": "Jun 2020",
                "reference_abstract": "Based on compressive sensing and double random encryption strategy, a novel color image compression and encryption scheme is proposed in this paper. The architecture of compression, confusion and diffusion is adopted. Firstly, the red, green and blue components of color plain image are converted to three sparse coefficient matrices by discrete wavelet transform (DWT), and then a double random position permutation (DRPP) is introduced to confuse the coefficient matrices. Subsequently, Logistic-Tent system is utilized to generate the asymptotic deterministic random measurement matrix based on chaotic system and plain image (ADMMCP), which is used to measure the coefficient matrices to obtain measurement value matrices. Moreover, simultaneous double random pixel diffusion between inter-intra components (SDRDIC) is presented to modify the elements of measurement value matrices to obtain the final cipher image. A 4-D hyperchaotic system is applied to produce chaotic sequences for confusion and diffusion, the initial conditions of the used chaotic systems are controlled by the SHA 512 hash value of plain image and external keys, such that the proposed image cryptosystem may withstand known-plaintext and chosen-plaintext attacks. Experimental results and security analyses verify the effectiveness of the proposed cipher."
            },
            {
                "reference_title": "Circuit implementation of 3D chaotic self-exciting single-disk homopolar dynamo and its application in digital image confidentiality",
                "reference_link": "publication/341474888_Circuit_implementation_of_3D_chaotic_self-exciting_single-disk_homopolar_dynamo_and_its_application_in_digital_image_confidentiality",
                "reference_type": "Article",
                "reference_date": "May 2020",
                "reference_abstract": "Confidentiality of secret information is one of the mandatory issue of digitally modernized era of science and technology.\nDue to availability of several online web applications and social media usages nowadays, information is transmitted with ease. The privacy of these digital information can be addressed by using different encryption mechanisms. We have proposed digital information privacy preserving scheme based on three-dimensional chaotic self-exciting single disk homopolar system. Moreover, we have designed a circuit implementations of three-dimensional chaotic self-exciting single disk homopolar dynamical system. The suggested encryption scheme is tested against different security performance analyses which clearly reflects the affectedness of our encryption scheme."
            },
            {
                "reference_title": "An Encryption Scheme Based on Discrete Quantum Map and Continuous Chaotic System",
                "reference_link": "publication/339538800_An_Encryption_Scheme_Based_on_Discrete_Quantum_Map_and_Continuous_Chaotic_System",
                "reference_type": "Article",
                "reference_date": "Apr 2020",
                "reference_abstract": "Chaotic encryption is a growing field for competently shielding visual data. As well as quantum encryption and substitution also play a significant role in applied fields owing to its prospective use in secure communication. This article offers new method for the encryption by utilizing quantum chaotic maps and continuous chaotic dynamical systems. Since chaotic maps are itself suitable for quality encryption coupled with quantum logistic encryption, substitution box stipulates highly secure encryption program. Quantum logistic map is combined with Liu and Rossler chaotic systems with the intention of improvement in efficiency of data randomness. Designed scheme ensures the security of data with the minimum time of encryption. Outcomes acquired from performance evaluation signifies that projected chaotic encryption program reveals high complexity and security."
            },
            {
                "reference_title": "Double-image encryption based on interference and logistic map under the framework of double random phase encoding",
                "reference_link": "publication/345441515_Double-image_encryption_based_on_interference_and_logistic_map_under_the_framework_of_double_random_phase_encoding",
                "reference_type": "Article",
                "reference_date": "Nov 2019",
                "reference_abstract": "A novel double-image encryption approach has been proposed based on optical interference and logistic map. Initially, original images are rearranged into two scrambled components with the use of the random sequence engendered based on the logistic map with the given conditions such as initial value and bifurcate parameter. One component is used as the input image of the framework of double random phase encoding and encrypted into the ciphertext. Another is decomposed into two phase masks, which are placed into the input plane and transform plane of double random phase encoding, respectively. To strength the nonlinearity of the cryptosystem, these phase masks considered as the main secret keys are further encoded by the aid of two random sequences. Because these keys are produced in the encryption process and directly related to original images, the proposed scheme has high resistance against the potential attacks such as chosen plaintext attack. Meanwhile, the parameters such as wavelength, axial distance, and conditions of the logistic map are considered as additional keys, which can enhance the security level further. Most importantly, the annoying silhouette problem existed in the interference-based encryption methods can be avoided effectively. A set of numerical simulations are presented to demonstrate the validity and feasibility of the proposed technique."
            },
            {
                "reference_title": "Coupling chaotic system based on unit transform and its applications in image encryption",
                "reference_link": "publication/345386372_Coupling_chaotic_system_based_on_unit_transform_and_its_applications_in_image_encryption",
                "reference_type": "Article",
                "reference_date": "Jan 2021",
                "reference_abstract": "Chaotic maps are very important for establishing chaos-based image encryption systems. This paper introduces a coupling chaotic system based on a certain unit transform, which can combine any two 1D chaotic maps to generate a new one with better performance in cryptography applications. The chaotic behavior analysis has verified this coupling system\u2019s effectiveness and progress. In particular, we give a specific strategy about selecting an appropriate unit transform function to enhance the chaotic behaviors of generated maps. Besides, a new chaos-based pseudo-random number generator, shorted as CBPRNG, is designed to improve the randomness of chaotic sequences. Moreover, based on CBPRNG, a novel digital image encryption algorithm is introduced, where we design a two-way multi-round transformation network to encrypt the higher bits and lower bits of input images separately. Simulation results and security analysis indicate that the proposed image encryption scheme is competitive with some existing methods."
            },
            {
                "reference_title": "An efficient image encryption scheme based on chaotic and Deoxyribonucleic acid sequencing",
                "reference_link": "publication/341439527_An_efficient_image_encryption_scheme_based_on_chaotic_and_Deoxyribonucleic_acid_sequencing",
                "reference_type": "Article",
                "reference_date": "May 2020",
                "reference_abstract": "In this research article, we have designed a novel image encryption scheme based on Deoxyribonucleic acid (DNA) and chaotic sequencing. We have studied sequences of different genes consisting of four bases, Adenine (A), Cytosine (C), Thymine (T) and Guanine (G), also known as nucleotides which are the fundamental code of life. The purpose of DNA is to store, copy and transmit the genetic information of the living organism. The proposed image encryption scheme uses the chaotic system to generate random sequences and to choose between two or more options. The image is totally transformed by first encoding it into DNA nucleotides, shuffling to achieve diffusion and then substitution is performed to achieve confusion. Then later the DNA fusion operation is performed on the DNA image and the DNA complements are applied on the resulting DNA image, a random number of times to break the residual correlation between the pixels. The cipher image has been tested to fulfill all the standard benchmarks to be categorized as a good ciphered image which includes entropy, correlation coefficient, Pearson\u2019s chi square, peak signal to noise ratio, mean square error, mean absolute error, structure similarity indexed measures, differential analysis and histogram uniformity test."
            },
            {
                "reference_title": "A new color image encryption scheme based on 2DNLCML system and genetic operations",
                "reference_link": "publication/341082057_A_new_color_image_encryption_scheme_based_on_2DNLCML_system_and_genetic_operations",
                "reference_type": "Article",
                "reference_date": "May 2020",
                "reference_abstract": "Based on the spatiotemporal chaos of the two Dimensional Nonlinear Coupled Map Lattices (2DNLCML) and genetic operations, a new optical color image encryption scheme is proposed. Because of the spatial nonlinear coupling which replaces traditional adjacent coupling, the 2DNLCML system contains good features such as ergodic pseudo-random sequence, less periodic windows in bifurcations and larger range of parameters in chaotic dynamics, which is more suitable for image encryption. The scheme also performs genetic operations including mutation and crossover for binary coding, thus reducing the computational complexity. Simulations have been carried out and the results demonstrate that the proposed algorithm has properties of large key space, high sensitivity to key, strong resisting attack. So, it is more secure and effective algorithm for encryption of digital images."
            }
        ]
    },
    {
        "title": "Novice Cybersecurity Students Encounter TracerFIRE: An Experience Report",
        "date": "December 2020",
        "doi": null,
        "conferance": "Conference: EDSIGCON",
        "citations_count": null,
        "reference_count": "References (9)",
        "abstract": "Integrating hands-on learning into a competitive cybersecurity exercise is known to be a popular and potentially powerful way to motivate experiential learning in computer and network security (Childers, et al., 2010; Fanelli & O'Connor, 2010; Vigna, et al., 2014; Siami Namin, et al., 2016). In addition to having taken relevant computer science (CS) and/or information technology (IT) classes, competitive cybersecurity exercise participants typically conduct specific preparation activities based on known technical expectations of the exercise. It is also common, in team events, for some portion of the team to have prior experience navigating the rigors of the event. Over the weekend of February 22-23, 2020, eight students from the University of North Carolina Wilmington (UNCW), with no prior competitive cybersecurity event experience and little preparation attended their first-ever cybersecurity competition-Tracer FIRE 9. In this paper, we describe that experience and relate how experiential learning made it valuable even for a group with very little previous exposure to cybersecurity-specific education.",
        "reference": [
            {
                "reference_title": "The Kolb Learning Style Inventory 4.0: Guide to Theory, Psychometrics, Research & Applications",
                "reference_link": "publication/303446688_The_Kolb_Learning_Style_Inventory_40_Guide_to_Theory_Psychometrics_Research_Applications",
                "reference_type": "Book",
                "reference_date": "Jan 2013",
                "reference_abstract": "Abstract\n\nThe Kolb Learning Style Inventory version 4.0 (KLSI 4.0) revised in 2011, is the latest revision of the original Learning Style Inventory developed by David A. Kolb. Like its predecessors, the KLSI 4.0 is based on experiential learning theory (Kolb 1984) and is designed to help individuals identify the way they learn from experience. The Kolb Learning Style Inventory 4.0 is the first major revision of the KLSI since 1999 and the third since the original LSI was published in 1971. Based on many years of research involving scholars around the world and data from many thousands of respondents, the KLSI 4.0 includes four major additions-- A new nine learning style typology, assessment of learning flexibility, an expanded personal report focused on improving learning effectiveness, and improved psychometrics. The technical specifications are designed to adhere to the standards for educational and psychological testing developed by the American Educational Research Association, the American Psychological Association, and the National Council on Measurement in Education (1999). \n\nThe first chapter describes the conceptual foundations of the LSI 3.1 in the theory of experiential learning (ELT). Chapter 2 provides a description of the inventory that includes its purpose, history, and format. Chapter 3 describes the characteristics of the KLSI 4.0 normative sample. Chapter 4 includes internal reliability and test-retest reliability studies of the inventory. Chapter 5 provides information about research on the internal and external validity for the instrument. Internal validity studies of the structure of the KLSI 4.0.1 using correlation and factor analysis are reported. External validity includes research on demographics, educational specialization, concurrent validity with other experiential learning assessment instruments, aptitude test performance, academic performance and experiential learning in teams. Chapter 6 describes the new Learning Flexibility Index including scoring formulas, normative data and validity evidence. In chapter 7 the current research on educational applications of ELT and the KLSI in many fields is reviewed.\n\n\u00a9Experience Based Learning Systems 2013 www.learningfromexperience.com"
            },
            {
                "reference_title": "Organizing Large Scale Hacking Competitions",
                "reference_link": "publication/221394363_Organizing_Large_Scale_Hacking_Competitions",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2010",
                "reference_abstract": "Computer security competitions and challenges are a way to foster innovation and educate students in a highly-motivating setting.\nIn recent years, a number of different security competitions and challenges were carried out, each with different characteristics,\nconfigurations, and goals. From 2003 to 2007, we carried out a number of live security exercises involving dozens of universities\nfrom around the world. These exercises were designed as \u201ctraditional\u201d Capture The Flag competitions, where teams both attacked\nand defended a virtualized host, which provided several vulnerable services. In 2008 and 2009, we introduced two completely\nnew types of competition: a security \u201ctreasure hunt\u201d and a botnet-inspired competition. These two competitions, to date, represent\nthe largest live security exercises ever attempted and involved hundreds of students across the globe. In this paper, we describe\nthese two new competition designs, the challenges overcome, and the lessons learned, with the goal of providing useful guidelines\nto other educators who want to pursue the organization of similar events."
            },
            {
                "reference_title": "Instrumenting Competition-Based Exercises to Evaluate Cyber Defender Situation Awareness",
                "reference_link": "publication/299708591_Instrumenting_Competition-Based_Exercises_to_Evaluate_Cyber_Defender_Situation_Awareness",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2013",
                "reference_abstract": "Cyber defense exercises create simulated attack and defense scenarios used to train and evaluate incident responders. The most pervasive form of competition-based exercise is comprised of jeopardy-style challenges, which compliment a fictional cyber-security event. Multiple competitions were instrumented to collect usage statistics on a per-challenge basis. The competitions use researcher-developed challenges containing over twenty attack techniques, which generate forensic evidence and observable second-order effects. The following observations were made: (1) a group of defenders performs better than an individual; (2) situation awareness of the fictional event may be measured; (3) challenge complexity does not imply difficulty. This research introduces a novel application of system instrumentation on competition-based exercises and describes an exercise development methodology for effective challenge and competition creation. Effective challenges correctly represent difficulty and reward competitors with objective points and optional forensic clues. Effective competitions compliment training goals and appropriately improve the knowledge and skill of a competitor."
            },
            {
                "reference_title": "Multidisciplinary Experiential Learning for Holistic Cybersecurity Education, Research and Evaluation",
                "reference_link": "publication/281359439_Multidisciplinary_Experiential_Learning_for_Holistic_Cybersecurity_Education_Research_and_Evaluation",
                "reference_type": "Conference Paper",
                "reference_date": "Aug 2015",
                "reference_abstract": null
            },
            {
                "reference_title": "Experiences with practice-focused undergraduate security education",
                "reference_link": "publication/262326821_Experiences_with_practice-focused_undergraduate_security_education",
                "reference_type": "Conference Paper",
                "reference_date": "Aug 2010",
                "reference_abstract": "The combination of competitive security exercises and hands-on learning represents a powerful approach for teaching information system security. Although creating and maintaining such a course can be difficult, the benefits to learning are worthwhile. Our undergraduate Information Assurance course is practice-focused and makes substantial use of competitive exercises, such as the National Security Agency Cyber Defense Exercise, to promote learning. We recount experiences and lessons learned from creating and conducting this course."
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2012",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2012",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2017",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2019",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Whole-Body Manipulation",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (28)",
        "abstract": null,
        "reference": [
            {
                "reference_title": "C-CROC: Continuous and Convex Resolution of Centroidal Dynamic Trajectories for Legged Robots in Multicontact Scenarios",
                "reference_link": "publication/338870888_C-CROC_Continuous_and_Convex_Resolution_of_Centroidal_Dynamic_Trajectories_for_Legged_Robots_in_Multicontact_Scenarios",
                "reference_type": "Article",
                "reference_date": "Jan 2020",
                "reference_abstract": "Synthesizing legged locomotion requires planning one or several steps ahead (literally): when and where, and with which effector should the next contact(s) be created between the robot and the environment? Validating a contact candidate implies a minima the resolution of a slow, nonlinear optimization problem, to demonstrate that a center of mass (CoM) trajectory, compatible with the contact transition constraints, exists. We propose a conservative reformulation of this trajectory generation problem as a convex 3-D linear program, named convex resolution of centroidal dynamic trajectories (CROC). It results from the observation that if the CoM trajectory is a polynomial with only one free variable coefficient, the nonlinearity of the problem disappears. This has two consequences. On the positive side, in terms of computation times, CROC outperforms the state of the art by at least one order of magnitude, and allows to consider interactive applications (with a planning time roughly equal to the motion time). On the negative side, in our experiments, our approach finds a majority of the feasible trajectories found by a nonlinear solver, but not all of them. Still, we demonstrate that the solution space covered by CROC is large enough to achieve the automated planning of a large variety of locomotion tasks for different robots, demonstrated in simulation and on the real HRP-2 robot, several of which were rarely seen before. Another significant contribution is the introduction of a Bezier curve representation of the problem, which guarantees that the constraints of the CoM trajectory are verified continuously, and not only at discrete points as traditionally done. This formulation is lossless, and results in more robust trajectories. It is not restricted to CROC, but could rather be integrated with any method from the state of the art."
            },
            {
                "reference_title": "Dynamic Walking on Compliant and Uneven Terrain using DCM and Passivity-based Whole-body Control",
                "reference_link": "publication/337475169_Dynamic_Walking_on_Compliant_and_Uneven_Terrain_using_DCM_and_Passivity-based_Whole-body_Control",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2019",
                "reference_abstract": "This paper presents a complete trajectory generation and control approach for achieving a robust dynamic walking gait for humanoid robots over compliant and uneven terrain. The work uses the concept of Divergent Component of Motion (DCM) for generating the center of mass (CoM) trajectory, and Cartesian polynomial trajectories for the feet. These reference trajectories are tracked by a passivity-based whole-body controller, which computes the joint torques for commanding our torque-controlled humanoid robot TORO. We provide the implementation details regarding the trajectory generation and control that help preventing discontinuities in the commanded joint torques, which facilitates precise trajectory tracking and robust locomotion. We present extensive experimental results of TORO walking over rough terrain, grass, and, to the best of our knowledge, the first report of a humanoid robot walking over a gym mattress."
            },
            {
                "reference_title": "Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks",
                "reference_link": "publication/335141438_Making_Sense_of_Vision_and_Touch_Self-Supervised_Learning_of_Multimodal_Representations_for_Contact-Rich_Tasks",
                "reference_type": "Conference Paper",
                "reference_date": "May 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Whole-Body Nonlinear Model Predictive Control Through Contacts for Quadrupeds",
                "reference_link": "publication/321718888_Whole-Body_Nonlinear_Model_Predictive_Control_Through_Contacts_for_Quadrupeds",
                "reference_type": "Article",
                "reference_date": "Dec 2017",
                "reference_abstract": "In this work we present a whole-body Nonlinear Model Predictive Control approach for Rigid Body Systems subject to contacts. We use a full dynamic system model which also includes explicit contact dynamics. Therefore, contact locations, sequences and timings are not prespecified but optimized by the solver. Yet, thorough numerical and software engineering allows for running the nonlinear Optimal Control solver at rates up to 190 Hz on a quadruped for a time horizon of half a second. This outperforms the state of the art by at least one order of magnitude. Hardware experiments in form of periodic and non-periodic tasks are applied to two quadrupeds with different actuation systems. The obtained results underline the performance, transferability and robustness of the approach."
            },
            {
                "reference_title": "Autonomous SLAM based humanoid navigation in a cluttered environment while transporting a heavy load",
                "reference_link": "publication/320578612_Autonomous_SLAM_based_humanoid_navigation_in_a_cluttered_environment_while_transporting_a_heavy_load",
                "reference_type": "Article",
                "reference_date": "Oct 2017",
                "reference_abstract": "Although in recent years there have been quite a few studies aimed at the navigation of robots in cluttered environments, few of these have addressed the problem of robots navigating while moving a large or heavy objects. This is especially useful when transporting loads with variable weights and shapes without having to change the robot hardware. Inspired by the wide use of makeshift carts by humans, we tackle, in this work, the problem of a humanoid robot navigating in a cluttered environment while displacing a heavy load that lies on a cart-like object. We present a complete navigation scheme, from the incremental construction of a map of the environment and the computation of collision-free trajectories to the control to execute these trajectories. Our contributions are as follows: (1) a whole-body control scheme that makes the humanoid use its hands and arms to control the motions of the cart\u2013load system (e.g. tight turns) (2) a sensorless approach to automatically select the appropriate primitive set according to the load weight (3) a motion planning algorithm to find an obstacle-free trajectory using the appropriate primitive set and the constructed map of the environment as input (4) an efficient filtering technique to remove the cart from the field of view of the robot while improving the general performances of the SLAM algorithms and (5) a continuous and consistent odometry data formed by fusing the visual and the robot odometry information. We present experiments conducted on a real Nao robot, equipped with an RGB-D sensor mounted on its head, pushing a cart with different loads. Our experiments show that the payload can be significantly increased without changing the robot's main hardware, and therefore enacting the capacity of humanoid robots in real-life situations."
            },
            {
                "reference_title": "Anytime Hybrid Driving-Stepping Locomotion Planning",
                "reference_link": "publication/318947082_Anytime_Hybrid_Driving-Stepping_Locomotion_Planning",
                "reference_type": "Conference Paper",
                "reference_date": "Sep 2017",
                "reference_abstract": "Hybrid driving-stepping locomotion is an effective approach for navigating in a variety of environments. Long, sufficiently even distances can be quickly covered by driving while obstacles can be overcome by stepping. Our quadruped robot Momaro, with steerable pairs of wheels located at the end of each of its compliant legs, allows such locomotion. Planning respective paths attracted only little attention so far. We propose a navigation planning method which generates hybrid locomotion paths. The planner chooses driving mode whenever possible and takes into account the detailed robot footprint. If steps are required, the planner includes those. To accelerate planning, steps are planned first as abstract manoeuvres and are expanded afterwards into detailed motion sequences. Our method ensures at all times that the robot stays stable. Experiments show that the proposed planner is capable of providing paths in feasible time, even for challenging terrain."
            },
            {
                "reference_title": "Detecting and Picking of Folded Objects with a Multiple Sensor Integrated Robot Hand",
                "reference_link": "publication/330595457_Detecting_and_Picking_of_Folded_Objects_with_a_Multiple_Sensor_Integrated_Robot_Hand",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "On the Kinematics of Wheeled Motion Control of a Hybrid Wheeled-Legged CENTAURO robot",
                "reference_link": "publication/330586250_On_the_Kinematics_of_Wheeled_Motion_Control_of_a_Hybrid_Wheeled-Legged_CENTAURO_robot",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Mechanism Design Outline of Hubo",
                "reference_link": "publication/328206945_Mechanism_Design_Outline_of_Hubo",
                "reference_type": "Chapter",
                "reference_date": "Jan 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Multi-Contact Balancing of Humanoid Robots in Confined Spaces: Utilizing Knee Contacts",
                "reference_link": "publication/321669889_Multi-Contact_Balancing_of_Humanoid_Robots_in_Confined_Spaces_Utilizing_Knee_Contacts",
                "reference_type": "Conference Paper",
                "reference_date": "Sep 2017",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "TBFMM: A C++ generic and parallel fast multipole method library",
        "date": "December 2020",
        "doi": "10.21105/joss.02444",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (28)",
        "abstract": null,
        "reference": [
            {
                "reference_title": "Extreme Scale FMM-Accelerated Boundary Integral Equation Solver for Wave Scattering",
                "reference_link": "publication/324055147_Extreme_Scale_FMM-Accelerated_Boundary_Integral_Equation_Solver_for_Wave_Scattering",
                "reference_type": "Article",
                "reference_date": "Mar 2018",
                "reference_abstract": "Algorithmic and architecture-oriented optimizations are essential for achieving performance worthy of anticipated energy-austere exascale systems. In this paper, we present an extreme scale FMM-accelerated boundary integral equation solver for wave scattering, which uses FMM as a matrix-vector multiplication inside the GMRES iterative method. Our FMM Helmholtz kernels treat nontrivial singular and near-field integration points. We implement highly optimized kernels for both shared and distributed memory, targeting emerging Intel extreme performance HPC architectures. We extract the potential thread- and data-level parallelism of the key Helmholtz kernels of FMM. Our application code is well optimized to exploit the AVX-512 SIMD units of Intel Skylake and Knights Landing architectures. We provide different performance models for tuning the task-based tree traversal implementation of FMM, and develop optimal architecture-specific and algorithm aware partitioning, load balancing, and communication reducing mechanisms to scale up to 6,144 compute nodes of a Cray XC40 with 196,608 hardware cores. With shared memory optimizations, we achieve roughly 77% of peak single precision floating point performance of a 56-core Skylake processor, and on average 60% of peak single precision floating point performance of a 72-core KNL. These numbers represent nearly 5.4x and 10x speedup on Skylake and KNL, respectively, compared to the baseline scalar code. With distributed memory optimizations, on the other hand, we report near-optimal efficiency in the weak scalability study with respect to both the logarithmic communication complexity as well as the theoretical scaling complexity of FMM. In addition, we exhibit up to 85% efficiency in strong scaling. We compute in excess of 2 billion DoF on the full-scale of the Cray XC40 supercomputer."
            },
            {
                "reference_title": "Increasing the Degree of Parallelism Using Speculative Execution in Task-based Runtime Systems",
                "reference_link": "publication/323722762_Increasing_the_Degree_of_Parallelism_Using_Speculative_Execution_in_Task-based_Runtime_Systems",
                "reference_type": "Article",
                "reference_date": "Mar 2018",
                "reference_abstract": "Task-based programming models have demonstrated their efficiency in the development of scientific applications on modern high-performance platforms. They allow delegation of the management of parallelization to the runtime system (RS), which is in charge of the data coherency, the scheduling, and the assignment of the work to the computational units. However, some applications have a limited degree of parallelism such that no matter how efficient the RS implementation, they may not scale on modern multicore CPUs. In this paper, we propose using speculation to unleash the parallelism when it is uncertain if some tasks will modify data, and we formalize a new methodology to enable speculative execution in a graph of tasks. This description is partially implemented in our new C++ RS called SPETABARU, which is capable of executing tasks in advance if some others are not certain to modify the data. We study the behavior of our approach to compute Monte Carlo and replica exchange Monte Carlo simulations."
            },
            {
                "reference_title": "Inastemp: A Novel Intrinsics-as-Template Library for Portable SIMD-Vectorization",
                "reference_link": "publication/319947459_Inastemp_A_Novel_Intrinsics-as-Template_Library_for_Portable_SIMD-Vectorization",
                "reference_type": "Article",
                "reference_date": "Sep 2017",
                "reference_abstract": "The development of scientific applications requires highly optimized computational kernels to benefit from modern hardware. In recent years, vectorization has gained key importance in exploiting the processing capabilities of modern CPUs, whose evolution is characterized by increasing register-widths and core numbers, but stagnating clock frequencies. In particular, vectorization allows floating point operations to be performed at a higher rate than the processor\u2019s frequency. However, compilers often fail to vectorize complex codes and pure assembly/intrinsic implementations often suffer from software engineering issues, such as readability and maintainability. Moreover, it is difficult for domain scientists to write optimized code without technical support. To address these issues, we propose Inastemp, a lightweight open-source C++ library. Inastemp offers a solution to develop hardware-independent computational kernels for the CPU. These kernels are portable across compilers and floating point precision and vectorized targeting SSE(3,4.1,4.2), AVX(2), AVX512, or ALTIVEC/VMX instructions. Inastemp provides advanced features, such as an if-else statement that vectorizes branches that cannot be removed. Our performance study shows that Inastemp has the same efficiency as pure intrinsic approaches on modern architectures. As side-results, this study provides micro benchmarks on the latest HPC architectures for three different computational kernels, emphasizing comparisons between scalar and intrinsic-based codes."
            },
            {
                "reference_title": "Bridging the Gap Between OpenMP and Task-Based Runtime Systems for the Fast Multipole Method",
                "reference_link": "publication/316498271_Bridging_the_Gap_Between_OpenMP_and_Task-Based_Runtime_Systems_for_the_Fast_Multipole_Method",
                "reference_type": "Article",
                "reference_date": "Apr 2017",
                "reference_abstract": "With the advent of complex modern architectures, the low-level paradigms long considered sufficient to build High Performance Computing (HPC) numerical codes have met their limits. Achieving efficiency, ensuring portability, while preserving programming tractability on such hardware prompted the HPC community to design new, higher level paradigms while relying on runtime systems to maintain performance. However, the common weakness of these projects is to deeply tie applications to specific expert-only runtime system APIs. The OpenMP specification, which aims at providing common parallel programming means for shared-memory platforms, appears as a good candidate to address this issue thanks to the latest task-based constructs introduced in its revision 4.0. The goal of this paper is to assess the effectiveness and limits of this support for designing a high-performance numerical library, ScalFMM, implementing the fast multipole method (FMM) that we have deeply re-designed with respect to the most advanced features provided by OpenMP 4. We show that OpenMP 4 allows for significant performance improvements over previous OpenMP revisions on recent multicore processors and that extensions to the 4.0 standard allow for strongly improving the performance, bridging the gap with the very high performance that was so far reserved to expert-only runtime system APIs."
            },
            {
                "reference_title": "Optimization and parallelization of the boundary element method for the wave equation in time domain",
                "reference_link": "publication/302564087_Optimization_and_parallelization_of_the_boundary_element_method_for_the_wave_equation_in_time_domain",
                "reference_type": "Thesis",
                "reference_date": "Feb 2016",
                "reference_abstract": "La m\u00e9thode des \u00e9l\u00e9ments fronti\u00e8res pour l\u2019\u00e9quation des ondes (BEM) est utilis\u00e9e en acoustique eten \u00e9lectromagn\u00e9tisme pour simuler la propagation d\u2019une onde avec une discr\u00e9tisation en temps(TD). Elle permet d\u2019obtenir un r\u00e9sultat pour plusieurs fr\u00e9quences \u00e0 partir d\u2019une seule r\u00e9solution.Dans cette th\u00e8se, nous nous int\u00e9ressons \u00e0 l\u2019impl\u00e9mentation efficace d\u2019un simulateur TD-BEM sousdiff\u00e9rents angles. Nous d\u00e9crivons le contexte de notre \u00e9tude et la formulation utilis\u00e9e qui s\u2019exprimesous la forme d\u2019un syst\u00e8me lin\u00e9aire compos\u00e9 de plusieurs matrices d\u2019interactions/convolutions.Ce syst\u00e8me est naturellement calcul\u00e9 en utilisant l\u2019op\u00e9rateur matrice/vecteur creux (SpMV). Nousavons travaill\u00e9 sur la limite du SpMV en \u00e9tudiant la permutation des matrices et le comportementde notre impl\u00e9mentation aid\u00e9 par la vectorisation sur CPU et avec une approche par bloc surGPU. Nous montrons que cet op\u00e9rateur n\u2019est pas appropri\u00e9 pour notre probl\u00e8me et nous proposonsde changer l\u2019ordre de calcul afin d\u2019obtenir une matrice avec une structure particuli\u00e8re.Cette nouvelle structure est appel\u00e9e une matrice tranche et se calcule \u00e0 l\u2019aide d\u2019un op\u00e9rateur sp\u00e9cifique.Nous d\u00e9crivons des impl\u00e9mentations optimis\u00e9es sur architectures modernes du calculhaute-performance. Le simulateur r\u00e9sultant est parall\u00e9lis\u00e9 avec une approche hybride (m\u00e9moirespartag\u00e9es/distribu\u00e9es) sur des noeuds h\u00e9t\u00e9rog\u00e8nes, et se base sur une nouvelle heuristique pour\u00e9quilibrer le travail entre les processeurs. Cette approche matricielle a une complexit\u00e9 quadratiquesi bien que nous avons \u00e9tudi\u00e9 son acc\u00e9l\u00e9ration par la m\u00e9thode des multipoles rapides (FMM). Nousavons tout d\u2019abord travaill\u00e9 sur la parall\u00e9lisation de l\u2019algorithme de la FMM en utilisant diff\u00e9rentsparadigmes et nous montrons comment les moteurs d\u2019ex\u00e9cution sont adapt\u00e9s pour rel\u00e2cher le potentielde la FMM. Enfin, nous pr\u00e9sentons des r\u00e9sultats pr\u00e9liminaires d\u2019un simulateur TD-BEMacc\u00e9l\u00e9r\u00e9 par FMM ."
            },
            {
                "reference_title": "A massively parallel adaptive fast-multipole method on heterogeneous architectures",
                "reference_link": "publication/298344857_A_massively_parallel_adaptive_fast-multipole_method_on_heterogeneous_architectures",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2009",
                "reference_abstract": null
            },
            {
                "reference_title": "PVFMM: A Parallel Kernel Independent FMM for Particle and Volume Potentials",
                "reference_link": "publication/283125194_PVFMM_A_Parallel_Kernel_Independent_FMM_for_Particle_and_Volume_Potentials",
                "reference_type": "Article",
                "reference_date": "Sep 2015",
                "reference_abstract": "We describe our implementation of a parallel fast multipole method for evaluating potentials for discrete and continuous source distributions. The first requires summation over the source points and the second requiring integration over a continuous source density. Both problems require\n\n(\nN\n2\n) complexity when computed directly; however, can be accelerated to\n\n(\nN\n) time using FMM. In our PVFMM software library, we use kernel independent FMM and this allows us to compute potentials for a wide range of elliptic kernels. Our method is high order, adaptive and scalable. In this paper, we discuss several algorithmic improvements and performance optimizations including cache locality, vectorization, shared memory parallelism and use of coprocessors. Our distributed memory implementation uses space-filling curve for partitioning data and a hypercube communication scheme. We present convergence results for Laplace, Stokes and Helmholtz (low wavenumber) kernels for both particle and volume FMM. We measure efficiency of our method in terms of CPU cycles per unknown for different accuracies and different kernels. We also demonstrate scalability of our implementation up to several thousand processor cores on the Stampede platform at the Texas Advanced Computing Center."
            },
            {
                "reference_title": "Optimizing the Black-box FMM for Smooth and Oscillatory Kernels",
                "reference_link": "publication/279257850_Optimizing_the_Black-box_FMM_for_Smooth_and_Oscillatory_Kernels",
                "reference_type": "Article",
                "reference_date": "Feb 2013",
                "reference_abstract": null
            },
            {
                "reference_title": "FMMTL: FMM template library a generalized framework for kernel matrices",
                "reference_link": "publication/282687792_FMMTL_FMM_template_library_a_generalized_framework_for_kernel_matrices",
                "reference_type": "Article",
                "reference_date": "Oct 2015",
                "reference_abstract": "In response to two decades of development in structured dense matrix algorithms and a vast number of research codes, we present designs and progress towards a codebase that is abstracted over the primary domains of research. In the domain of mathematics, this includes the development of interaction kernels and their low-rank expansions. In the domain of high performance computing, this includes the optimized construction, traversal, and scheduling algorithms for the appropriate operations.We present a versatile system that can encompass the design decisions made over a decade of research while providing an abstracted, intuitive, and usable front-end that can integrated into existing linear algebra libraries."
            },
            {
                "reference_title": "Task-based FMM for heterogeneous architectures",
                "reference_link": "publication/278629862_Task-based_FMM_for_heterogeneous_architectures",
                "reference_type": "Article",
                "reference_date": "Apr 2014",
                "reference_abstract": "High performance \\FMM is crucial for the numerical simulation of many physical problems. In a previous study~\\cite{Agullo2013}, we have shown that task-based \\FMM provides the flexibility required to process a wide spectrum of particle distributions efficiently on multicore architectures. In this paper, we now show how such an approach can be extended to fully exploit heterogeneous platforms. For that, we design highly tuned GPU versions of the two dominant operators (P2P and M2L) as well as a scheduling strategy that dynamically decides which proportion of subsequent tasks are processed on regular CPU cores and on GPU accelerators. We assess our method with the StarPU runtime system for executing the resulting task flow on an Intel X5650 Nehalem multicore processor possibly enhanced with one, two or three Nvidia Fermi M2070 or M2090 GPUs. A detailed experimental study on two 30 million particle distributions (a cube and an ellipsoid) shows that the resulting software consistently achieves high performance across architectures."
            }
        ]
    },
    {
        "title": "On proof theory in computer science",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (8)",
        "abstract": "The subject logic in computer science should entail proof theoretic applications. So the question arises whether open problems in computational complexity can be solved by advanced proof theoretic techniques. In particular, consider the complexity classes NP, coNP and PSPACE. It is well-known that NP and coNP are contained in PSPACE, but till recently precise characterization of these relationships remained open. Now [2], [3] (see also [4]) presented proofs of corresponding equalities NP = coNP = PSPACE. These results were obtained by appropriate proof theoretic tree-to-dag compressing techniques to be briefy explained below. [2] L. Gordeev, E. H. Haeusler, Proof Compression and NP Versus PSPACE, Studia Logica (107) (1): 55{83 (2019) [3] L. Gordeev, E. H. Haeusler, Proof Compression and NP Versus PSPACE II, Bulletin of the Section of Logic (49) (3): 213{230 (2020) http://dx.doi.org/10.18788/0138-0680.2020.16 [4] L. Gordeev, E. H. Haeusler, Proof Compression and NP Versus PSPACE II: Addendum, arXiv:2011.09262 (2020)",
        "reference": [
            {
                "reference_title": "Proof Compression and NP Versus PSPACE",
                "reference_link": "publication/322099668_Proof_Compression_and_NP_Versus_PSPACE",
                "reference_type": "Article",
                "reference_date": "Dec 2017",
                "reference_abstract": "We show that arbitrary tautologies of Johansson\u2019s minimal propositional logic are provable by \u201csmall\u201d polynomial-size dag-like natural deductions in Prawitz\u2019s system for minimal propositional logic. These \u201csmall\u201d deductions arise from standard \u201clarge\u201d tree-like inputs by horizontal dag-like compression that is obtained by merging distinct nodes labeled with identical formulas occurring in horizontal sections of deductions involved. The underlying geometric idea: if the height, \\(h\\left( \\partial \\right) \\), and the total number of distinct formulas, \\(\\phi \\left( \\partial \\right) \\), of a given tree-like deduction \\(\\partial \\) of a minimal tautology \\(\\rho \\) are both polynomial in the length of \\(\\rho \\), \\(\\left| \\rho \\right| \\), then the size of the horizontal dag-like compression \\(\\partial ^{{\\textsc {c}} }\\) is at most \\(h\\left( \\partial \\right) \\times \\phi \\left( \\partial \\right) \\), and hence polynomial in \\(\\left| \\rho \\right| \\). That minimal tautologies \\( \\rho \\) are provable by tree-like natural deductions \\(\\partial \\) with \\(\\left| \\rho \\right| \\)-polynomial \\(h\\left( \\partial \\right) \\) and \\(\\phi \\left( \\partial \\right) \\) follows via embedding from Hudelmaier\u2019s result that there are analogous sequent calculus deductions of sequent \\(\\Rightarrow \\rho \\). The notion of dag-like provability involved is more sophisticated than Prawitz\u2019s tree-like one and its complexity is not clear yet. Our approach nevertheless provides a convergent sequence of NP lower approximations of PSPACE-complete validity of minimal logic (Savitch in J Comput Syst Sci 4(2):177\u2013192, 1970); Statman in Theor Comput Sci 9(1):67\u201372, 1979; Svejdar in Arch Math Log 42(7):711\u2013716, 2003)."
            },
            {
                "reference_title": "A Survey of Some Connections Between Classical, Intuitionistic and Minimal Logic",
                "reference_link": "publication/268738683_A_Survey_of_Some_Connections_Between_Classical_Intuitionistic_and_Minimal_Logic",
                "reference_type": "Article",
                "reference_date": "Dec 1968",
                "reference_abstract": "This chapter discusses the relationships among classical, intuitionistic and minimal logic. Classical logic is interpretable (also interpretable with respect to derivability) in intuitionistic and minimal logic by the translation absurdity, conjunctions, implications, and universal formulas. If the intuitionistic natural number theory is consistent, then the classical natural number theory is also consistent. Intuitionistic predicate logic is interpretable (also interpretable with respect to derivability) in classical predicate logic. A classical argument can be understood intuitionistically, if the formulas are interpreted throughout in the weak sense, that is, classical logic can be interpreted in intuitionistic logic by a translation that successively replaces classical formulas by their double negation. The chapter suggests that the axioms for classical and intuitionistic natural number theory are the same."
            },
            {
                "reference_title": "On the polynomial-space completeness of intuitionistic propositional logic",
                "reference_link": "publication/226329499_On_the_polynomial-space_completeness_of_intuitionistic_propositional_logic",
                "reference_type": "Article",
                "reference_date": "Oct 2003",
                "reference_abstract": "We present an alternative, purely semantical and relatively simple, proof of the Statman's result that both intuitionistic propositional logic and its implicational fragment are PSPACE-complete."
            },
            {
                "reference_title": "Intuitionistic propositional logic is polynomial-space complete",
                "reference_link": "publication/222452307_Intuitionistic_propositional_logic_is_polynomial-space_complete",
                "reference_type": "Article",
                "reference_date": "Jul 1979",
                "reference_abstract": "It is the purpose of this note to show that the question of whether a given propositional formula is intuitionistically valid (in Brouwer's sense, in Kripke's sense, or just provable by Heyting's rules, see Kreisel [7]) is p-space complete (see Stockmeyer [14]). Our result has the following consequences:1.(a) There is a simple (i.e. polynomial time) translation of intuitionistic propositional logic into classical propositional logic if and only if NP = p-space.2.(b) The problem of determining if a type of the typed \u03bb-calculus is the type of a closed \u03bb-term is p-space complete (this will be discussed below).3.(c) There is a polynomial bounded intuitionistic proof system if and only if NP = p-space (see Cook and Reckhow [2])."
            },
            {
                "reference_title": "An O(n log n)-Space Decision Procedure for Intuitionistic Propositional Logic",
                "reference_link": "publication/31124855_An_On_log_n-Space_Decision_Procedure_for_Intuitionistic_Propositional_Logic",
                "reference_type": "Article",
                "reference_date": "Feb 1993",
                "reference_abstract": "We present a certain calculus LG which is equivalent to intuitionistic prepositional logic and in which lengths of deductions\nare linearly bounded in terms, of the size of the endsequent. Backwards application of the rules of LG thus gives rise to\nan O(n log n)-SPACE decision algorithm for intuitionistic prepositional logic. The system LG is easily implemented as a tableau system\nfor intuitionistic logic. It is therefore of interest in its own right."
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jul 2014",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 1936",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 1965",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Asymptotic Dimension of Minor-Closed Families and Assouad-Nagata Dimension of Surfaces",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (40)",
        "abstract": "The asymptotic dimension is an invariant of metric spaces introduced by Gromov in the context of geometric group theory. In this paper, we study the asymptotic dimension of metric spaces generated by graphs and their shortest path metric and show their applications to some continuous spaces. The asymptotic dimension of such graph metrics can be seen as a large scale generalisation of weak diameter network decomposition which has been extensively studied in computer science. We prove that every proper minor-closed family of graphs has asymptotic dimension at most 2, which gives optimal answers to a question of Fujiwara and Papasoglu and (in a strong form) to a problem raised by Ostrovskii and Rosenthal on minor excluded groups. For some special minor-closed families, such as the class of graphs embeddable in a surface of bounded Euler genus, we prove a stronger result and apply this to show that complete Riemannian surfaces have Assouad-Nagata dimension at most 2. Furthermore, our techniques allow us to prove optimal results for the asymptotic dimension of graphs of bounded layered treewidth and graphs of polynomial growth, which are graph classes that are defined by purely combinatorial notions and properly contain graph classes with some natural topological and geometric flavours.",
        "reference": [
            {
                "reference_title": "On the Bi-Lipschitz Geometry of Lamplighter Graphs",
                "reference_link": "publication/339633531_On_the_Bi-Lipschitz_Geometry_of_Lamplighter_Graphs",
                "reference_type": "Article",
                "reference_date": "Mar 2020",
                "reference_abstract": "In this article we start a systematic study of the bi-Lipschitz geometry of lamplighter graphs. We prove that lamplighter graphs over trees bi-Lipschitzly embed into Hamming cubes with distortion at most 6. It follows that lamplighter graphs over countable trees bi-Lipschitzly embed into \\(\\ell _1\\). We study the metric behaviour of the operation of taking the lamplighter graph over the vertex-coalescence of two graphs. Based on this analysis, we provide metric characterisations of superreflexivity in terms of lamplighter graphs over star graphs or rose graphs. Finally, we show that the presence of a clique in a graph implies the presence of a Hamming cube in the lamplighter graph over it. An application is a characterisation, in terms of a sequence of graphs with uniformly bounded degree, of the notion of trivial Bourgain\u2013Milman\u2013Wolfson type for arbitrary metric spaces, similar to Ostrovskii\u2019s characterisation previously obtained in Ostrovskii (C. R. Acad. Bulgare Sci. 64(6), 775\u2013784\n(2011))."
            },
            {
                "reference_title": "Orthogonal Tree Decompositions of Graphs",
                "reference_link": "publication/312594439_Orthogonal_Tree_Decompositions_of_Graphs",
                "reference_type": "Article",
                "reference_date": "Jan 2017",
                "reference_abstract": "This paper studies graphs that have two tree decompositions with the property that every bag from the first decomposition has a bounded-size intersection with every bag from the second decomposition. We show that every graph in each of the following classes has a tree decomposition and a linear-sized path decomposition with bounded intersections: (1) every proper minor-closed class, (2) string graphs with a linear number of crossings in a fixed surface, (3) graphs with linear crossing number in a fixed surface. We then show that every $n$-vertex graph that has a tree decomposition and a linear-sized path decomposition with bounded intersections has $O(\\sqrt{n})$ treewidth. As a corollary, we conclude a new lower bound on the crossing number of a graph in terms of its treewidth. Finally, we consider graph classes that have two path decompositions with bounded intersections. Trees and outerplanar graphs have this property. But for the next most simple class, series parallel graphs, we show that no such result holds."
            },
            {
                "reference_title": "Strongly Sublinear Separators and Polynomial Expansion",
                "reference_link": "publication/275279734_Strongly_Sublinear_Separators_and_Polynomial_Expansion",
                "reference_type": "Article",
                "reference_date": "Apr 2015",
                "reference_abstract": "A result of Plotkin, Rao, and Smith implies that graphs with polynomial\nexpansion have strongly sublinear separators. We prove a converse of this\nresult showing that hereditary classes of graphs with strongly sublinear\nseparators have polynomial expansion. This confirms a conjecture of the first\nauthor."
            },
            {
                "reference_title": "Metric dimensions of minor excluded graphs and minor exclusion in groups",
                "reference_link": "publication/265603049_Metric_dimensions_of_minor_excluded_graphs_and_minor_exclusion_in_groups",
                "reference_type": "Article",
                "reference_date": "Sep 2014",
                "reference_abstract": "An infinite graph G is minor excluded if there is a finite graph that is not\na minor of G. We prove that minor excluded graphs have finite Assouad-Nagata\ndimension and study minor exclusion for Cayley graphs of finitely generated\ngroups. Our main results and observations are: (1) minor exclusion is not a\ngroup property: it depends on the choice of generating set; (2) a group with\none end has a generating set for which the Cayley graph is not minor excluded;\n(3) there are groups that are not minor excluded for any set of generators; (4)\nminor exclusion is preserved under free products; and (5) virtually free groups\nare minor excluded for any choice of finite generating set."
            },
            {
                "reference_title": "Partitioning H -minor free graphs into three subgraphs with no large components",
                "reference_link": "publication/319292681_Partitioning_H_-minor_free_graphs_into_three_subgraphs_with_no_large_components",
                "reference_type": "Article",
                "reference_date": "Aug 2017",
                "reference_abstract": "We prove that for every graph H, if a graph G has no (odd) H minor, then its vertex set V(G) can be partitioned into three sets X1, X2, X3 such that for each i, the subgraph induced on Xi has no component of size larger than a function of H and the maximum degree of G. This improves a previous result of Alon, Ding, Oporowski and Vertigan (2003) [1] stating that V(G) can be partitioned into four such sets if G has no H minor. Our theorem generalizes a result of Esperet and Joret (2014) [9], who proved it for graphs embeddable on a fixed surface and asked whether it is true for graphs with no H minor.\nAs a corollary, we prove that for every positive integer t, if a graph G has no Kt+1 minor, then its vertex set V(G) can be partitioned into 3t sets X1,\u2026,X3t such that for each i, the subgraph induced on Xi has no component of size larger than a function of t. This corollary improves a result of Wood (2010) [21], which states that V(G) can be partitioned into \u23083.5t+2\u2309 such sets."
            },
            {
                "reference_title": "Layered separators in minor-closed graph classes with applications",
                "reference_link": "publication/317306034_Layered_separators_in_minor-closed_graph_classes_with_applications",
                "reference_type": "Article",
                "reference_date": "Jun 2017",
                "reference_abstract": "Graph separators are a ubiquitous tool in graph theory and computer science. However, in some applications, their usefulness is limited by the fact that the separator can be as large as \u03a9(n) in graphs with n vertices. This is the case for planar graphs, and more generally, for proper minor-closed classes. We study a special type of graph separator, called a layered separator, which may have linear size in n, but has bounded size with respect to a different measure, called the width. We prove, for example, that planar graphs and graphs of bounded Euler genus admit layered separators of bounded width. More generally, we characterise the minor-closed classes that admit layered separators of bounded width as those that exclude a fixed apex graph as a minor.\nWe use layered separators to prove O(log\u2061n) bounds for a number of problems where O(n) was a long-standing previous best bound. This includes the nonrepetitive chromatic number and queue-number of graphs with bounded Euler genus. We extend these results with a O(log\u2061n) bound on the nonrepetitive chromatic number of graphs excluding a fixed topological minor, and a logO(1)\u2061n bound on the queue-number of graphs excluding a fixed minor. Only for planar graphs were logO(1)\u2061n bounds previously known. Our results imply that every n-vertex graph excluding a fixed minor has a 3-dimensional grid drawing with nlogO(1)\u2061n volume, whereas the previous best bound was O(n3/2)."
            },
            {
                "reference_title": "A continuum of expanders",
                "reference_link": "publication/316537941_A_continuum_of_expanders",
                "reference_type": "Article",
                "reference_date": "Jan 2017",
                "reference_abstract": "A regular equivalence between two graphs is a pair of uniformly proper Lipschitz maps VV and VV. Using separation profiles we prove that there are 2N0 regular equivalence classes of expander graphs, and of finitely generated groups with a representative which isometrically contains expanders."
            },
            {
                "reference_title": "Structure of Graphs with Locally Restricted Crossings",
                "reference_link": "publication/316465686_Structure_of_Graphs_with_Locally_Restricted_Crossings",
                "reference_type": "Article",
                "reference_date": "Jan 2017",
                "reference_abstract": "We consider relations between the size, treewidth, and local crossing number (maximum number of crossings per edge) of graphs embedded on topological surfaces. We show that an $n$-vertex graph embedded on a surface of genus $g$ with at most $k$ crossings per edge has treewidth $O(\\sqrt{(g+1)(k+1)n})$ and layered treewidth $O((g+1)k)$ and that these bounds are tight up to a constant factor. In the special case of $g=0$, so-called $k$-planar graphs, the treewidth bound is $O(\\sqrt{(k+1)n})$, which is tight and improves upon a known $O((k+1)^{3/4}n^{1/2})$ bound. Analogous results are proved for map graphs defined with respect to any surface. Finally, we show that for $g<m$, every $m$-edge graph can be embedded on a surface of genus $g$ with $O((m/(g+1))\\log^2 g)$ crossings per edge, which is tight to a polylogarithmic factor."
            },
            {
                "reference_title": "Choice functions and Tychonoff\u2019s theorem",
                "reference_link": "publication/267134226_Choice_functions_and_Tychonoff's_theorem",
                "reference_type": "Article",
                "reference_date": "Jan 1951",
                "reference_abstract": null
            },
            {
                "reference_title": "Sur la distance de Nagata",
                "reference_link": "publication/265462223_Sur_la_distance_de_Nagata",
                "reference_type": "Article",
                "reference_date": "Jan 1982",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Propagating AI Knowledge Across University Disciplines-The Design of A Multidisciplinary AI Study Module",
        "date": "December 2020",
        "doi": "10.1109/FIE44824.2020.9273940",
        "conferance": "Conference: 2020 IEEE Frontiers in Education Conference (FIE)",
        "citations_count": null,
        "reference_count": "References (48)",
        "abstract": "The ongoing AI revolution has disrupted several industry sectors and will keep having an unprecedented impact on all areas of society. This is predicted to force a major proportion of the workforce to re-educate itself during the next few decades. Consequently, this has led to a growing demand for multidisciplinary AI education also for students outside computer science. Therefore, a 25 credit (ECTS) cross-disciplinary study module on AI, targeting students in all faculties, was designed. We present findings from the design and implementation of the study module as well as students' initial perceptions towards AI at the beginning of the study module. Enrollment for the first implementation of the study module began in autumn 2019. The student distribution (N=144) between faculties was the following: natural sciences (n=37), social sciences (n=23), law (n=17), education (n=17), economics (n=16), medicine (n=10), humanities (n=10) and open university (n=14). Based on a survey distributed to students (N=34), the primary reason for enrolling to study AI was interest towards the subject, followed by the need of AI skills at work and relevance of AI in society.",
        "reference": [
            {
                "reference_title": "Cost-effective survival prediction for patients with advanced prostate cancer using clinical trial and real-world hospital registry datasets",
                "reference_link": "publication/337320455_Cost-effective_survival_prediction_for_patients_with_advanced_prostate_cancer_using_clinical_trial_and_real-world_hospital_registry_datasets",
                "reference_type": "Article",
                "reference_date": "Nov 2019",
                "reference_abstract": "Introduction: \nPredictive survival modeling offers systematic tools for clinical decision-making and individualized tailoring of treatment strategies to improve patient outcomes while reducing overall healthcare costs. In 2015, a number of machine learning and statistical models were benchmarked in the DREAM 9.5 Prostate Cancer Challenge, based on open clinical trial data for metastatic castration resistant prostate cancer (mCRPC). However, applying these models into clinical practice poses a practical challenge due to the inclusion of a large number of model variables, some of which are not routinely monitored or are expensive to measure.\n\nObjectives: \nTo develop cost-specified variable selection algorithms for constructing cost-effective prognostic models of overall survival that still preserve sufficient model performance for clinical decision making.\n\nMethods: \nPenalized Cox regression models were used for the survival prediction. For the variable selection, we implemented two algorithms: (i) LASSO regularization approach; and (ii) a greedy cost-specified variable selection algorithm. The models were compared in three cohorts of mCRPC patients from randomized clinical trials (RCT), as well as in a real-world cohort (RWC) of advanced prostate cancer patients treated at the Turku University Hospital. Hospital laboratory expenses were utilized as a reference for computing the costs of introducing new variables into the models.\n\nResults: \nCompared to measuring the full set of clinical variables, economic costs could be reduced by half without a significant loss of model performance. The greedy algorithm outperformed the LASSO-based variable selection with the lowest tested budgets. The overall top performance was higher with the LASSO algorithm.\n\nConclusion: \nThe cost-specified variable selection offers significant budget optimization capability for the real-world survival prediction without compromising the predictive power of the model."
            },
            {
                "reference_title": "The Impact of AI on Employment and Organisation in the Industrial Working Environment of the Future",
                "reference_link": "publication/334899242_The_Impact_of_AI_on_Employment_and_Organisation_in_the_Industrial_Working_Environment_of_the_Future",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2019",
                "reference_abstract": "AI applications such as robotics, automation or intelligent assistance are becoming drivers of a wide-ranging change process in manufacturing companies, which not only affects the use of algorithms but also affects people and organisation. Automation and algorithmisation will change the working world in a lasting way, whereby all value-adding activities \u2013 from operative production work to skilled work and management \u2013 will be influenced.\nIt is expected that, due to its learning ability, AI will be able to act autonomously, support people through assistance systems, use resources more effectively, make processes more environmentally friendly and enable new working models with direct participation and greater transparency. It should increase efficiency, enhance customer satisfaction and facilitate and enrich work. Current research confirms that it is less about technology and investment than about the openness of employees and executives combined with a supportive organisational structure and culture that is decisive for the success of digitalisation.\nThe influence of AI on employment is controversial. It should lead to secure and demanding jobs, physical and cognitive relief and an improvement in work-life balance. Yet, there are concerns about job losses, disqualification, growing autonomy of digital systems and increased control potential for employees. However, research demonstrates that in the past one robot has replaced on average two workers in the industry, while two jobs have been created outside. AI will probably demonstrate a similar behaviour.\nThe implementation of AI requires reorganisation of management, cooperation, co-determination, qualification and a high level of knowledge exchange. Digital change requires flexible and agile organisational structures and flatter hierarchies to be able to react to new complexity and dynamics. The participative leadership of the future conducts flexibly within the framework of self-organising networks and interdisciplinary, democratically formed teams. Executives see themselves as coaches and moderators.\nThis paper examines the effects of the introduction of AI in industrial enterprises based on a comprehensive literature review. Particular attention will be paid to effects on employment and organisational structure and culture. Best practice examples for AI applications in industrial companies will also be examined. Finally, a critical discussion examines possibilities and instruments for shaping transformation within companies through AI with the involvement of all relevant actors."
            },
            {
                "reference_title": "AI Systems Under Criminal Law: a Legal Analysis and a Regulatory Perspective",
                "reference_link": "publication/334467745_AI_Systems_Under_Criminal_Law_a_Legal_Analysis_and_a_Regulatory_Perspective",
                "reference_type": "Article",
                "reference_date": "Sep 2020",
                "reference_abstract": "Criminal liability for acts committed by AI systems has recently become a hot legal topic. This paper includes three different contributions. The first contribution is an analysis of the extent to which an AI system can satisfy the requirements for criminal liability: accomplishing an actus reus, having the corresponding mens rea, possessing the cognitive capacities needed for responsibility. The second contribution is a discussion of criminal activity accomplished by an AI entity, with reference to a recent case involving an online bot, the Random Darknet Shopper. This discussion will provide the context for the analysis of commonalities and differences between criminal activities by humans and by artificial systems. The third contribution concerns the evaluation of different ways of addressing criminal activities by AI systems in a regulatory perspective."
            },
            {
                "reference_title": "Minimizing the Number of Dropouts in University Pedagogy Online Courses",
                "reference_link": "publication/333346266_Minimizing_the_Number_of_Dropouts_in_University_Pedagogy_Online_Courses",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2019",
                "reference_abstract": "Students\u2019 engagement and retention in online courses have been found to be in general significantly lower than in contact teaching. Multiple reasons for this exist, but improving student retention is ubiquitously seen as a beneficial improvement. We take a look at student engagement in online courses aimed specifically for university teachers and doctoral students, and use a mixed methods approach to obtain a holistic understanding of student engagement in our domain. We analyse quantitative data from two cases (n=346 and n=271) collected from students of three university pedagogy online modules over the course of years 2016-2017. We identify key moments in our modules where students drop out and, for example, differences in dropout rates between various demographics (i.e. faculty and whether the student is a university staff member or not). The main moment where students drop out is found to be in the very beginning of the courses, and the introduction of a pre- and post-test to the courses improved retention. This study suggests that when all other factors affecting student engagement are in order, additional focus should be paid to the very beginning of the course and get as many students to do the first couple tasks as possible in order to reduce the dropout rate."
            },
            {
                "reference_title": "Initial and scientific understandings and the problem of conceptual change",
                "reference_link": "publication/345048420_Initial_and_scientific_understandings_and_the_problem_of_conceptual_change",
                "reference_type": "Chapter",
                "reference_date": "Nov 2017",
                "reference_abstract": null
            },
            {
                "reference_title": "What drives unverified information sharing and cyberchondria during the COVID-19 pandemic?",
                "reference_link": "publication/341994720_What_drives_unverified_information_sharing_and_cyberchondria_during_the_COVID-19_pandemic",
                "reference_type": "Article",
                "reference_date": "Jun 2020",
                "reference_abstract": "The World Health Organisation has emphasised that misinformation \u2013 spreading rapidly through social media \u2013 poses a serious threat to the COVID-19 response. Drawing from theories of health perception and cognitive load, we develop and test a research model hypothesising why people share unverified COVID-19 information through social media. Our findings suggest a person\u2019s trust in online information and perceived information overload are strong predictors of unverified information sharing. Furthermore, these factors, along with a person\u2019s perceived COVID-19 severity and vulnerability influence cyberchondria. Females were significantly more likely to suffer from cyberchondria, with males more likely to share news without verifying its reliability. Our findings suggest that to mitigate the spread of COVID-19 misinformation and cyberchondria, measures should be taken to enhance a healthy scepticism of health news while simultaneously guarding against information overload."
            },
            {
                "reference_title": "Solving Diversity Issues in University Staff Training with UNIPS Pedagogical Online Courses",
                "reference_link": "publication/338370383_Solving_Diversity_Issues_in_University_Staff_Training_with_UNIPS_Pedagogical_Online_Courses",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "The Need for Multi-disciplinary Education About Standardization",
                "reference_link": "publication/336120600_The_Need_for_Multi-disciplinary_Education_About_Standardization",
                "reference_type": "Chapter",
                "reference_date": "Jan 2020",
                "reference_abstract": "In the modern world of rapid and continuous industrial development, standardization plays a considerable role in shaping the global economy. Standards establish quality benchmarks, resolve connectivity problems, fuel innovation and by these means, enable technological advancement and facilitate international commerce. Standards come in a bewildering variety of forms and types and affect various areas of activities, often causing misunderstandings between, inter alia, engineers, economists and lawyers. Although currently ignored, such cross-sectoral misapprehensions may negatively impact standardization in the long run. In this regard, a relevant point to consider is the intertwining of different sectors and activities in the wake of growing digitalization. To grapple with the technical and regulatory challenges arising from this industry shift, an increased understanding of different standardization areas is required. Taking a legal perspective, this contribution describes experiences of incomprehension and misunderstanding when discussing various standardization issues with economists, software developers and mechanical engineers. It emphasizes the importance of cross-sectoral education on standardization for generating a constructive dialogue between different experts. The contribution concludes that increased awareness of various standardization domains enables experts to learn from each other and facilitates their cooperation within different sectors, pacing standardization to the needs of industry and society."
            },
            {
                "reference_title": "Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy",
                "reference_link": "publication/335435943_Artificial_Intelligence_AI_Multidisciplinary_perspectives_on_emerging_challenges_opportunities_and_agenda_for_research_practice_and_policy",
                "reference_type": "Article",
                "reference_date": "Aug 2019",
                "reference_abstract": "As far back as the industrial revolution, significant development in technical innovation has succeeded in transforming numerous manual tasks and processes that had been in existence for decades where humans had reached the limits of physical capacity. Artificial Intelligence (AI) offers this same transformative potential for the augmentation and potential replacement of human tasks and activities within a wide range of industrial, intellectual and social applications. The pace of change for this new AI technological age is staggering, with new breakthroughs in algorithmic machine learning and autonomous decision-making, engendering new opportunities for continued innovation. The impact of AI could be significant, with industries ranging from: finance, healthcare, manufacturing, retail, supply chain, logistics and utilities, all potentially disrupted by the onset of AI technologies. The study brings together the collective insight from a number of leading expert contributors to highlight the significant opportunities, realistic assessment of impact, challenges and potential research agenda posed by the rapid emergence of AI within a number of domains: business and management, government, public sector, and science and technology. This research offers significant and timely insight to AI technology and its impact on the future of industry and society in general, whilst recognising the societal and industrial influence on pace and direction of AI development."
            },
            {
                "reference_title": "Automatic detection of cereal rows by means of pattern recognition techniques",
                "reference_link": "publication/334143235_Automatic_detection_of_cereal_rows_by_means_of_pattern_recognition_techniques",
                "reference_type": "Article",
                "reference_date": "Jul 2019",
                "reference_abstract": "Automatic locating of weeds from fields is an active research topic in precision agriculture. A reliable and practical plant identification technique would enable the reduction of herbicide amounts and lowering of production costs, along with reducing the damage to the ecosystem. When the seeds have been sown row-wise, most weeds may be located between the sowing rows. The present work describes a clustering-based method for recognition of plantlet rows from a set of aerial photographs, taken by a drone flying at approximately ten meters. The algorithm includes three phases: segmentation of green objects in the view, feature extraction, and clustering of plants into individual rows. Segmentation separates the plants from the background. The main feature to be extracted is the center of gravity of each plant segment. A tentative clustering is obtained piecewise by applying the 2D Fourier transform to image blocks to get information about the direction and the distance between the rows. The precise sowing line position is finally derived by principal component analysis. The method was able to find the rows from a set of photographs of size 1452\u00d7969 pixels approximately in 0.11 s, with the accuracy of 94 per cent."
            }
        ]
    },
    {
        "title": "Unlabelled ordered DAGs and labelled DAGs: constructive enumeration and uniform random sampling",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (19)",
        "abstract": "Directed Acyclic Graphs (DAGs) are directed graphs in which there is no path from a vertex to itself. DAGs are an omnipresent data structure in computer science and the problem of counting the DAGs of given number of vertices has been solved in the 70's by Robinson. In many applications one needs to construct connected DAGs and to control their number of edges, but the adaptation of Robinson's enumeration to take this into account led to counting formulas based on the inclusion-exclusion principle, inducing a high computational cost for the uniform random sampling of DAGs based on this formula. In the present paper we propose two contributions. First we enumerate a new class of DAGs, enriched with an independent ordering of the children of each vertex, according to their numbers of vertices and edges. We obtain a constructive recursive counting formula for them (i.e. without using the inclusion-exclusion principle) using a new decomposition scheme. Then we show the applicability of our method by proposing a constructive enumeration of Robinson's labelled DAGs, by vertices and edges, based on the same decomposition. As a consequence we are able to derive efficient uniform random samplers for both models.",
        "reference": [
            {
                "reference_title": "Statistical Tables for Biological, Agricultural and Medical Research.",
                "reference_link": "publication/324396754_Statistical_Tables_for_Biological_Agricultural_and_Medical_Research",
                "reference_type": "Article",
                "reference_date": "Jan 1965",
                "reference_abstract": null
            },
            {
                "reference_title": "Random graph generation for scheduling simulations",
                "reference_link": "publication/313496516_Random_graph_generation_for_scheduling_simulations",
                "reference_type": "Article",
                "reference_date": "Jan 2010",
                "reference_abstract": null
            },
            {
                "reference_title": "Counting labeled acyclic digraphs",
                "reference_link": "publication/266284477_Counting_labeled_acyclic_digraphs",
                "reference_type": "Article",
                "reference_date": "Jan 1973",
                "reference_abstract": null
            },
            {
                "reference_title": "Counting unlabeled acyclic di-graphs",
                "reference_link": "publication/245810008_Counting_unlabeled_acyclic_di-graphs",
                "reference_type": "Article",
                "reference_date": "Jan 1976",
                "reference_abstract": null
            },
            {
                "reference_title": "Monocopy and associative algorithms in extended Lisp",
                "reference_link": "publication/237124765_Monocopy_and_associative_algorithms_in_extended_Lisp",
                "reference_type": "Article",
                "reference_date": "Jan 1974",
                "reference_abstract": null
            },
            {
                "reference_title": "Combinatorial Algorithms for Computers and Calculators",
                "reference_link": "publication/234430339_Combinatorial_Algorithms_for_Computers_and_Calculators",
                "reference_type": "Article",
                "reference_date": "Jan 1978",
                "reference_abstract": null
            },
            {
                "reference_title": "A Calculus for the Random Generation of Labelled Combinatorial Structures",
                "reference_link": "publication/223724477_A_Calculus_for_the_Random_Generation_of_Labelled_Combinatorial_Structures",
                "reference_type": "Article",
                "reference_date": "Sep 1994",
                "reference_abstract": "A systematic approach to the random generation of labelled combinatorial objects is presented. It applies to structures that are decomposable, i.e., formally specifiable by grammars involving set, sequence, and cycle constructions. A general strategy is developed for solving the random generation problem with two closely related types of methods: for structures of size n, the boustrophedonic algorithms exhibit a worst-case behaviour of the form O(n log n); the sequential algorithms have worst case O(n2), while offering good potential for optimizations in the average case. The complexity model is in terms of arithmetic operations and both methods appeal to precomputed numerical table of linear size that can be computed in time O(n2).A companion calculus permits systematically to compute the average case cost of the sequential generation algorithm associated to a given specification. Using optimizations dictated by the cost calculus, several random generation algorithms of the sequential type are developed; most of them have expected complexity n log n, and are thus only slightly superlinear. The approach is exemplified by the random generation of a number of classical combinatorial structures including Cayley trees, hierarchies, the cycle decomposition of permutations, binary trees, functional graphs, surjections, and set partitions."
            },
            {
                "reference_title": "The Network Analysis of Urban Streets: A Dual Approach",
                "reference_link": "publication/222673882_The_Network_Analysis_of_Urban_Streets_A_Dual_Approach",
                "reference_type": "Article",
                "reference_date": "Sep 2006",
                "reference_abstract": "The application of the network approach to the urban case poses several questions in terms of how to deal with metric distances, what kind of graph representation to use, what kind of measures to investigate, how to deepen the correlation between measures of the structure of the network and measures of the dynamics on the network, what are the possible contributions from the GIS community. In this paper, the author considers six cases of urban street networks characterized by different patterns and historical roots. The authors propose a representation of the street networks based firstly on a primal graph, where intersections are turned into nodes and streets into edges. In a second step, a dual graph, where streets are nodes and intersections are edges, is constructed by means of a generalization model named Intersection Continuity Negotiation, which allows to acknowledge the continuity of streets over a plurality of edges. Finally, the authors address a comparative study of some structural properties of the dual graphs, seeking significant similarities among clusters of cases. A wide set of network analysis techniques are implemented over the dual graph: in particular the authors show that the absence of any clue of assortativity differentiates urban street networks from other non-geographic systems and that most of the considered networks have a broad degree distribution typical of scale-free networks and exhibit small-world properties as well."
            },
            {
                "reference_title": "On the number of labeled acyclic digraphs",
                "reference_link": "publication/222507705_On_the_number_of_labeled_acyclic_digraphs",
                "reference_type": "Article",
                "reference_date": "Aug 1992",
                "reference_abstract": "Let An(x) denote the generating function for all labeled acyclic digraphs of order n, i.e. An(x) = \u2211\u221er=0Anrxr, where Anr is equal to the number of labeled acyclic digraphs on n points with r arcs. The following recurrence holds The generating function (where An = An(1) is the number of labeled acyclic digraphs of order n) is given by the formula"
            },
            {
                "reference_title": "Enumerative applications of a decomposition for graphs and digraphs",
                "reference_link": "publication/222507240_Enumerative_applications_of_a_decomposition_for_graphs_and_digraphs",
                "reference_type": "Article",
                "reference_date": "May 1995",
                "reference_abstract": "A simple decomposition for graphs yields generating functions for counting graphs by edges and connected components. A change of variables gives a new interpretation to the Tutte polynomial of the complete graph involving inversions of trees. The relation between the Tutte polynomial of the complete graph and the inversion enumerator for trees is generalized to the Tutte polynomial of an arbitrary graph. When applied to digraphs, the decomposition yields formulas for counting digraphs and acyclic digraphs by edges and initially connected components."
            }
        ]
    },
    {
        "title": "Fonctionnement et R\u00e9glages DES PROTECTIONS DES LIGNES HTB ET TRANSFORMATEURS HTB-HTA Application au poste 90/33kV KMS au S\u00e9n\u00e9gal",
        "date": "December 2020",
        "doi": "10.13140/RG.2.2.24775.73128",
        "conferance": null,
        "citations_count": null,
        "reference_count": null,
        "abstract": "Le pr\u00e9sent rapport d\u00e9crit le travail r\u00e9alis\u00e9 en guise du projet de fin des \u00e9tudes\nde quatre mois que j\u2019ai effectu\u00e9 au sein de Vinci-Energies dans le cadre de la\nformation d\u2019ing\u00e9nieur de l\u2019Ecole Nationale Sup\u00e9rieure de l\u2019Electricit\u00e9 et de la\nM\u00e9canique (ENSEM).\nCe projet de stage traite la r\u00e9alisation d\u2019un guide de r\u00e9glage des protections.\nCe projet est \u00e9labor\u00e9 en quatre parties. La premi\u00e8re partie consiste \u00e0 \u00e9tablir le plan\nde protection en HT apr\u00e8s avoir analys\u00e9 les diff\u00e9rentes contraintes et le principe\nd\u2019\u00e9laboration de ce dernier. La deuxi\u00e8me partie est consacr\u00e9e \u00e0 la r\u00e9alisation d\u2019une\napplication informatique d\u00e9di\u00e9e aux calculs des r\u00e9glages des protections. La\ntroisi\u00e8me partie traite la protection du poste KMS 90/33 kV, \u00e0 travers l\u2019\u00e9tude des\ncourants de court-circuit qui serait la base du r\u00e9glage des \u00e9ventuels relais choisis afin\nde r\u00e9pondre aux fonctions de protection tout en assurant une coordination s\u00e9lective\nentre les diff\u00e9rents \u00e9quipements du r\u00e9seau de l\u2019installation. Et enfin la derni\u00e8re partie\nest r\u00e9serv\u00e9e \u00e0 la simulation de l\u2019\u00e9coulement de puissance, les courants de courtcircuit et les protections sur ETAP.\nMots cl\u00e9s : Poste THT, r\u00e9seau \u00e9lectrique, jeu de barres, application\ninformatique, plan de protection, R\u00e9glage de protection, Court-circuit.",
        "reference": []
    },
    {
        "title": "\u0421\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0441 \u0442\u043e\u0447\u043a\u0438 \u0437\u0440\u0435\u043d\u0438\u044f \u0433\u0435\u043e\u043c\u0435\u0442\u0440\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0442\u0435\u043e\u0440\u0438\u0438 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u044f (Social systems from the geometric theory of behavior point of view)",
        "date": "December 2020",
        "doi": "10.19181/kongress.2020.400",
        "conferance": "Conference: VI \u0412\u0441\u0435\u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u0438\u0439 \u0441\u043e\u0446\u0438\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043a\u043e\u043d\u0433\u0440\u0435\u0441\u0441",
        "citations_count": null,
        "reference_count": "References (6)",
        "abstract": "\u0427\u0435\u043c \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043f\u043e\u043b\u0435\u0437\u043d\u0430 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u043a\u0430 \u0434\u043b\u044f \u0441\u043e\u0446\u0438\u043e\u043b\u043e\u0433\u0438\u0438, \u043a\u0440\u043e\u043c\u0435 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0430\u043b\u044c\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445? \u0412 \u0434\u0430\u043d\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u0435 \u0434\u043b\u044f \u0441\u043e\u0446\u0438\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0434\u0438\u0441\u043a\u0443\u0440\u0441\u0430 \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f \u044f\u0437\u044b\u043a, \u0440\u043e\u0434\u0438\u0432\u0448\u0438\u0439\u0441\u044f \u0432 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0441\u043b\u043e\u0436\u043d\u044b\u0445 \u0441\u0438\u0441\u0442\u0435\u043c \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u043d\u043e\u0439 \u0442\u0435\u043e\u0440\u0438\u0438, \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u0430\u043c\u0438 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u043a\u0438 \u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u043a\u0438. \u0414\u043e\u0441\u0442\u043e\u0438\u043d\u0441\u0442\u0432\u0430\u043c\u0438 \u044d\u0442\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043e\u0434\u043d\u043e\u0437\u043d\u0430\u0447\u043d\u043e\u0441\u0442\u044c \u0438 \u0440\u0430\u0437\u0432\u0438\u0442\u044b\u0435 \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u0430 \u0442\u0438\u043f\u0438\u0437\u0430\u0446\u0438\u0438, \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u044e\u0449\u0438\u0435 \u0440\u0430\u0437\u043b\u0438\u0447\u0430\u0442\u044c \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0447\u0430\u0441\u0442\u043e \u0441\u043c\u0435\u0448\u0438\u0432\u0430\u044e\u0442\u0441\u044f, \u043a\u043e\u0433\u0434\u0430 \u0434\u0438\u0441\u043a\u0443\u0440\u0441 \u0432\u0435\u0434\u0435\u0442\u0441\u044f \u043d\u0430 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u043c \u044f\u0437\u044b\u043a\u0435. \nHow can mathematics be useful for sociology, in other ways than experimental data processing methods? In this paper, we propose a language for sociological discourse that was born during the complex systems modeling, using structural theory, mathematics and computer science. The advantages of this language are mathematical unambiguity and advanced typification tools that allow us to distinguish between entities that are often mixed when the discourse is conducted in natural languages.",
        "reference": [
            {
                "reference_title": "\u041c\u043e\u0434\u0435\u043b\u044c\u043d\u044b\u0439 \u0441\u0438\u043d\u0442\u0435\u0437 \u0438 \u043c\u043e\u0434\u0435\u043b\u044c\u043d\u043e-\u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0435 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435",
                "reference_link": "publication/316280479_Modelnyj_sintez_i_modelno-orientirovannoe_programmirovanie",
                "reference_type": "Book",
                "reference_date": "Jan 2013",
                "reference_abstract": "\u041f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f \u043d\u043e\u0432\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u043a \u043f\u0440\u043e\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044e \u0438 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u043e\u0439 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0438\u043c\u0438\u0442\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0441\u043b\u043e\u0436\u043d\u044b\u0445 \u043c\u043d\u043e\u0433\u043e\u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u043d\u044b\u0445 \u0441\u0438\u0441\u0442\u0435\u043c, \u043e\u0442\u043b\u0438\u0447\u0430\u044e\u0449\u0438\u0439\u0441\u044f \u043e\u0442 \u043e\u0431\u044a\u0435\u043a\u0442\u043d\u043e\u0433\u043e \u043f\u043e\u0434\u0445\u043e\u0434\u0430. \u041e\u043d \u043d\u0430\u0437\u0432\u0430\u043d \u0432 \u0440\u0430\u0431\u043e\u0442\u0435 \u043c\u043e\u0434\u0435\u043b\u044c\u043d\u044b\u043c \u0441\u0438\u043d\u0442\u0435\u0437\u043e\u043c \u0438 \u043c\u043e\u0434\u0435\u043b\u044c\u043d\u043e-\u043e\u0440\u0438\u0435\u043d\u0442\u0438\u00ad\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\u043c. \u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u043c \u043f\u043e\u043d\u044f\u0442\u0438\u0435\u043c \u044d\u0442\u043e\u0433\u043e \u043f\u043e\u0434\u0445\u043e\u0434\u0430 \u0438 \u0432 \u0442\u043e \u0436\u0435 \u0432\u0440\u0435\u043c\u044f \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u0430\u0440\u043d\u044b\u043c \u043a\u0438\u0440\u043f\u0438\u0447\u0438\u043a\u043e\u043c \u0434\u043b\u044f \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u044f \u043b\u044e\u0431\u044b\u0445 \u0431\u043e\u043b\u0435\u0435 \u0441\u043b\u043e\u0436\u043d\u044b\u0445 \u043a\u043e\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u0439 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043f\u043e\u043d\u044f\u0442\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438-\u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u044b. \n\u041e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u0438\u043c\u0438\u0442\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u043c\u043e\u0434\u0435\u043b\u0438-\u043a\u043e\u043c\u043f\u043e\u00ad\u043d\u0435\u043d\u0442\u044b \u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043d\u043e\u0439 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0435\u0439-\u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442 \u0432 \u043c\u043e\u0434\u0435\u043b\u044c-\u043a\u043e\u043c\u043f\u043b\u0435\u043a\u0441. \u042d\u0442\u043e \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0444\u0440\u0430\u043a\u0442\u0430\u043b\u044c\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043b\u044e\u0431\u043e\u0439 \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0438 \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u044b\u0432\u0430\u0442\u044c \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u0434\u0430\u0436\u0435 \u043e\u0447\u0435\u043d\u044c \u0441\u043b\u043e\u0436\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0435\u0434\u0438\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u043d\u043e. \u041f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u043c\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0438\u0441\u043a\u043b\u044e\u0447\u0438\u0442\u044c \u0438\u043c\u043f\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0435 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0438 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u043d\u044b\u0439 \u043a\u043e\u0434 \u0432\u044b\u0441\u043e\u043a\u043e\u0439 \u0441\u0442\u0435\u043f\u0435\u043d\u0438 \u043f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438."
            },
            {
                "reference_title": "How Institutions Think.",
                "reference_link": "publication/275995045_How_Institutions_Think",
                "reference_type": "Article",
                "reference_date": "May 1988",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": null,
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": null,
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": null,
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": null,
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Using phidelta diagrams to discover relevant patterns in multilayer perceptrons",
        "date": "December 2020",
        "doi": "10.1038/s41598-020-76517-0",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (62)",
        "abstract": "Understanding the inner behaviour of multilayer perceptrons during and after training is a goal of paramount importance for many researchers worldwide. This article experimentally shows that relevant patterns emerge upon training, which are typically related to the underlying problem difficulty. The occurrence of these patterns is highlighted by means of \u27e8\u03c6,\u03b4\u27e9\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\langle \\varphi ,\\delta \\rangle$$\\end{document} diagrams, a 2D graphical tool originally devised to support the work of researchers on classifier performance evaluation and on feature assessment. The underlying assumption being that multilayer perceptrons are powerful engines for feature encoding, hidden layers have been inspected as they were in fact hosting new input features. Interestingly, there are problems that appear difficult if dealt with using a single hidden layer, whereas they turn out to be easier upon the addition of further layers. The experimental findings reported in this article give further support to the standpoint according to which implementing neural architectures with multiple layers may help to boost their generalisation ability. A generic training strategy inspired by some relevant recommendations of deep learning has also been devised. A basic implementation of this strategy has been thoroughly used during the experiments aimed at identifying relevant patterns inside multilayer perceptrons. Further experiments performed in a comparative setting have shown that it could be adopted as viable alternative to the classical backpropagation algorithm.",
        "reference": [
            {
                "reference_title": "On Kernel Method-Based Connectionist Models and Supervised Deep Learning Without Backpropagation",
                "reference_link": "publication/337143575_On_Kernel_Method-Based_Connectionist_Models_and_Supervised_Deep_Learning_Without_Backpropagation",
                "reference_type": "Article",
                "reference_date": "Nov 2019",
                "reference_abstract": "We propose a novel family of connectionist models based on kernel machines and consider the problem of learning layer by layer a compositional hypothesis class (i.e., a feedforward, multilayer architecture) in a supervised setting. In terms of the models, we present a principled method to \"kernelize\" (partly or completely) any neural network (NN). With this method, we obtain a counterpart of any given NN that is powered by kernel machines instead of neurons. In terms of learning, when learning a feedforward deep architecture in a supervised setting, one needs to train all the components simultaneously using backpropagation (BP) since there are no explicit targets for the hidden layers (Rumelhart, Hinton, & Williams, 1986). We consider without loss of generality the two-layer case and present a general framework that explicitly characterizes a target for the hidden layer that is optimal for minimizing the objective function of the network. This characterization then makes possible a purely greedy training scheme that learns one layer at a time, starting from the input layer. We provide instantiations of the abstract framework under certain architectures and objective functions. Based on these instantiations, we present a layer-wise training algorithm for an \nl\n-layer feedforward network for classification, where \n\nl\n\u2265\n2\n\ncan be arbitrary. This algorithm can be given an intuitive geometric interpretation that makes the learning dynamics transparent. Empirical results are provided to complement our theory. We show that the kernelized networks, trained layer-wise, compare favorably with classical kernel machines as well as other connectionist models trained by BP. We also visualize the inner workings of the greedy kernelized models to validate our claim on the transparency of the layer-wise algorithm."
            },
            {
                "reference_title": "Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection",
                "reference_link": "publication/332180999_Deep-Learning_Domain_Adaptation_Techniques_for_Credit_Cards_Fraud_Detection",
                "reference_type": "Chapter",
                "reference_date": "Jan 2019",
                "reference_abstract": "Although the incidence of credit card fraud is limited to a small percentage of transactions, the related financial losses may be huge. This demands the design of automatic Fraud Detection Systems (FDS) able to detect fraudulent transactions with high precision and deal with the heterogeneous nature of the fraudster behavior. Indeed, the nature of the fraud behavior may strongly differ according to the payment system (e.g. e-commerce or shop terminal), the country and the population segment. Given the high cost of designing data-driven FDSs, it is more and more important for transactional companies to reuse existing pipelines and adapt them to different domains and contexts: this boils down to a well-known problem of transfer learning."
            },
            {
                "reference_title": "Understanding Autoencoders with Information Theoretic Concepts",
                "reference_link": "publication/324167625_Understanding_Autoencoders_with_Information_Theoretic_Concepts",
                "reference_type": "Article",
                "reference_date": "May 2019",
                "reference_abstract": "Despite their great success in practical applications, there is still a lack of theoretical and systematic methods to analyze deep neural networks. In this paper, we illustrate an advanced information theoretic methodology to understand the dynamics of learning and the design of autoencoders, a special type of deep learning architectures that resembles a communication channel. By generalizing the information plane to any cost function, and inspecting the roles and dynamics of different layers using layer-wise information quantities, we emphasize the role that mutual information plays in quantifying learning from data. We further suggest and also experimentally validate, for mean square error training, three fundamental properties regarding the layer-wise flow of information and intrinsic dimensionality of the bottleneck layer, using respectively the data processing inequality and the identification of a bifurcation point in the information plane that is controlled by the given data. Our observations have direct impact on the optimal design of autoencoders, the design of alternative feedforward training methods, and even in the problem of generalization."
            },
            {
                "reference_title": "XAI\u2014Explainable artificial intelligence",
                "reference_link": "publication/338039379_XAI-Explainable_artificial_intelligence",
                "reference_type": "Article",
                "reference_date": "Dec 2019",
                "reference_abstract": "Explainability is essential for users to effectively understand, trust, and manage powerful artificial intelligence applications."
            },
            {
                "reference_title": "Emergence of invariance and disentanglement in deep representations",
                "reference_link": "publication/328868381_Emergence_of_invariance_and_disentanglement_in_deep_representations",
                "reference_type": "Article",
                "reference_date": "Sep 2018",
                "reference_abstract": "Using established principles from Statistics and Information Theory, we show that invariance to nuisance factors in a deep neural network is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then decompose the cross-entropy loss used during training and highlight the presence of an inherent overfitting term. We propose regularizing the loss by bounding such a term in two equivalent ways: One with a Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other using the information in the weights as a measure of complexity of a learned model, yielding a novel Information Bottleneck for the weights. Finally, we show that invariance and independence of the components of the representation learned by the network are bounded above and below by the information in the weights, and therefore are implicitly optimized during training. The theory enables us to quantify and predict sharp phase transitions between underfitting and overfitting of random labels when using our regularized loss, which we verify in experiments, and sheds light on the relation between the geometry of the loss function, invariance properties of the learned representation, and generalization error."
            },
            {
                "reference_title": "A Survey of Model Compression and Acceleration for Deep Neural Networks",
                "reference_link": "publication/320619613_A_Survey_of_Model_Compression_and_Acceleration_for_Deep_Neural_Networks",
                "reference_type": "Article",
                "reference_date": "Oct 2017",
                "reference_abstract": "Deep convolutional neural networks (CNNs) have recently achieved dramatic accuracy improvements in many visual recognition tasks. However, existing deep convolutional neural network models are computationally expensive and memory intensive, hindering their deployment in devices with low memory resources or in applications with strict latency requirements. Therefore, a natural thought is to perform model compression and acceleration in deep CNNs without significantly decreasing the classification accuracy. During the past few years, tremendous progress has been made in this area. In this paper, we survey the recent advanced techniques for compacting and accelerating CNNs model developed. These techniques are roughly categorized into four schemes: parameter pruning and sharing, low-rank factorization, transfered/compact convolutional filters and knowledge distillation. Methods of parameter pruning and sharing will be described in detail at the beginning, and all the others will introduced. For methods of each scheme, we provide insightful analysis regarding the performance, related applications, advantages and drawbacks etc. Then we will go through a few very recent additional successful methods, for example, dynamic networks and stochastic depths networks. After that, we survey the evaluation matrix, main datasets used for the evaluating the model performance and recent benchmarking efforts. Finally we conclude this paper, discuss remaining challenges and possible directions in this topic."
            },
            {
                "reference_title": "Deep Sparse Rectifier Neural Networks",
                "reference_link": "publication/319770387_Deep_Sparse_Rectifier_Neural_Networks",
                "reference_type": "Article",
                "reference_date": "Jan 2011",
                "reference_abstract": "While logistic sigmoid neurons are more biologically plausable that hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabelled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labelled data sets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised nueral networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training"
            },
            {
                "reference_title": "Understanding intermediate layers using linear classifier probes",
                "reference_link": "publication/319770004_Understanding_intermediate_layers_using_linear_classifier_probes",
                "reference_type": "Conference Paper",
                "reference_date": "Apr 2017",
                "reference_abstract": "Neural network models have a reputation for being black boxes. We propose a new method to understand better the roles and dynamics of the intermediate layers. This has direct consequences on the design of such models and it enables the expert to be able to justify certain heuristics (such as the auxiliary heads in the Inception model). Our method uses linear classifiers, referred to as \"probes\", where a probe can only use the hidden units of a given intermediate layer as discriminating features. Moreover, these probes cannot affect the training phase of a model, and they are generally added after training. They allow the user to visualize the state of the model at multiple steps of training. We demonstrate how this can be used to develop a better intuition about a known model and to diagnose potential problems."
            },
            {
                "reference_title": "Proximal Backpropagation",
                "reference_link": "publication/317614358_Proximal_Backpropagation",
                "reference_type": "Article",
                "reference_date": "Jun 2017",
                "reference_abstract": "We offer a generalized point of view on the backpropagation algorithm, currently the most common technique to train neural networks via stochastic gradient descent and variants thereof. Specifically, we show that backpropagation of a prediction error is equivalent to sequential gradient descent steps on a quadratic penalty energy. This energy comprises the network activations as variables of the optimization and couples them to the network parameters. Based on this viewpoint, we illustrate the limitations on step sizes when optimizing a nested function with gradient descent. Rather than taking explicit gradient steps, where step size restrictions are an impediment for optimization, we propose proximal backpropagation (ProxProp) as a novel algorithm that takes implicit gradient steps to update the network parameters. We experimentally demonstrate that our algorithm is robust in the sense that it decreases the objective function for a wide range of parameter values. In a systematic quantitative analysis, we compare to related approaches on a supervised visual learning task (CIFAR-10) for fully connected as well as convolutional neural networks and for an unsupervised autoencoder (USPS dataset). We demonstrate that ProxProp leads to a significant speed up in training performance."
            },
            {
                "reference_title": "Roles of Pre-training in Deep Neural Networks from Information Theoretical Perspective",
                "reference_link": "publication/314715584_Roles_of_Pre-training_in_Deep_Neural_Networks_from_Information_Theoretical_Perspective",
                "reference_type": "Article",
                "reference_date": "Mar 2017",
                "reference_abstract": "Although deep learning shows high performance in pattern recognition and machine learning, the reasons remain unclarified. To tackle this problem, we calculated the information theoretical variables of the representations in the hidden layers and analyzed their relationship to the performance. We found that entropy and mutual information, both of which decrease in a different way as the layer deepens, are related to the generalization errors after fine-tuning. This suggests that the information theoretical variables might be a criterion for determining the number of layers in deep learning without fine-tuning that requires high computational loads."
            }
        ]
    },
    {
        "title": "GRAPH SIGNAL SAMPLING AND INTERPOLATION BASED ON CLUSTERS AND AVERAGES",
        "date": "December 2020",
        "doi": "10.13140/RG.2.2.16577.17769",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (39)",
        "abstract": "We consider a disjoint cover (a partition) of a set of vertices of a combinatorial undirected weighted finite or infinite graph G by connected subgraphs (clusters) {S j } j\u2208J and select a function \u03c8 j on each of the clusters. For a given function (signal) f on G the set of its samples is a a set of inner products { f, \u03c8 j } j\u2208J (weighted average values). Our main results are certain inequalities which are similar to a Poincare-type inequality. They provide estimate of the norm of a signal on the entire graph through its set of samples and its local gradients on each of subgraphs. This enables us to establish discrete Plancherel-Polya-type inequalities (or Marcinkiewicz-Zigmund-type or frame inequalities) on sets of signals whose gradients satisfy a Bernstein-type inequality. This leads to development of a sampling theory for signals on G. For reconstruction of signals from their sets of samples an interpolation theory by weighted average variational splines is developed. Here by a weighted average variational spline we understand a minimizer of a discrete Sobolev norm which takes on prescribed weighted average values on a set of clusters. Although our results hold true for rather general graphs they are especially well suited for finite and infinite graphs with multiple clusters. Such graphs are known as community graphs and they find many important applications in biology, computer sciences, engineering, economics, social studies.",
        "reference": [
            {
                "reference_title": "Half Sampling on Bipartite Graphs",
                "reference_link": "publication/291390879_Half_Sampling_on_Bipartite_Graphs",
                "reference_type": "Article",
                "reference_date": "Oct 2016",
                "reference_abstract": "On a bipartite graph G we consider the half sampling problem of uniquely recovering a function from its values on the even vertices, under the appropriate half bandlimited assumption with respect to a Laplacian on the graph. We discuss both finite and infinite graphs, give the appropriate definition of \u201chalf bandlimited\u201d that involves splitting the mid frequency, and give an explicit solution to the problem. We discuss in detail the example of a regular tree. We also consider a related sampling problem on graphs that are generated by edge substitution."
            },
            {
                "reference_title": "Signals on Graphs: Uncertainty Principle and Sampling",
                "reference_link": "publication/280630702_Signals_on_Graphs_Uncertainty_Principle_and_Sampling",
                "reference_type": "Article",
                "reference_date": "Jul 2015",
                "reference_abstract": "In many applications, the observations can be represented as a signal defined over the vertices of a graph. The analysis of such signals requires the extension of standard signal processing tools. In this work, first, we provide a class of graph signals that are maximally concentrated on the graph domain and on its dual. Then, building on this framework, we derive an uncertainty principle for graph signals and illustrate the conditions for the recovery of band-limited signals from a subset of samples. We show an interesting link between uncertainty principle and sampling and propose alternative signal recovery algorithms, including a generalization to frame-based reconstruction methods. After showing that the performance of signal recovery algorithms is significantly affected by the location of samples, we suggest and compare a few alternative sampling strategies. Finally, we provide the conditions for perfect recovery of a useful signal corrupted by sparse noise, showing that this problem is also intrinsically related to vertex-frequency localization properties."
            },
            {
                "reference_title": "Reconstruction of bandlimited graph signals from measurements",
                "reference_link": "publication/340067895_Reconstruction_of_bandlimited_graph_signals_from_measurements",
                "reference_type": "Article",
                "reference_date": "Jun 2020",
                "reference_abstract": "Graph signal reconstruction is a fundamental topic in graph signal processing. Recently, graph signal reconstruction in three typical sampling schemes has been studied in the literature. In this paper, we study the reconstruction of bandlimited graph signals in a general measurement scheme, which includes the three existing sampling scheme as its special cases. Theorems for perfect reconstruction are established, and reconstruction by random measurement matrices is studied. An iterative reconstruction algorithm based on convex projection is proposed, and numerical experiments are implemented to demonstrate the effectiveness of the algorithm."
            },
            {
                "reference_title": "Interpolation and Denoising of Graph Signals Using Plug-and-play Admm",
                "reference_link": "publication/332790594_Interpolation_and_Denoising_of_Graph_Signals_Using_Plug-and-play_Admm",
                "reference_type": "Conference Paper",
                "reference_date": "May 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Numerical Integration on Graphs: where to sample and how to weigh",
                "reference_link": "publication/323867684_Numerical_Integration_on_Graphs_where_to_sample_and_how_to_weigh",
                "reference_type": "Article",
                "reference_date": "Mar 2018",
                "reference_abstract": "Let $G=(V,E,w)$ be a finite, connected graph with weighted edges. We are interested in the problem of finding a subset $W \\subset V$ of vertices and weights $a_w$ such that $$ \\frac{1}{|V|}\\sum_{v \\in V}^{}{f(v)} \\sim \\sum_{w \\in W}{a_w f(w)}$$ for functions $f:V \\rightarrow \\mathbb{R}$ that are `smooth' with respect to the geometry of the graph. The main application are problems where $f$ is known to somehow depend on the underlying graph but is expensive to evaluate on even a single vertex. We prove an inequality showing that the integration problem can be rewritten as a geometric problem (`the optimal packing of heat balls'). We discuss how one would construct approximate solutions of the heat ball packing problem; numerical examples demonstrate the efficiency of the method."
            },
            {
                "reference_title": "Graph sampling with determinantal processes",
                "reference_link": "publication/320823542_Graph_sampling_with_determinantal_processes",
                "reference_type": "Conference Paper",
                "reference_date": "Aug 2017",
                "reference_abstract": null
            },
            {
                "reference_title": "Sampling and Reconstruction of Sparse Signals on Circulant Graphs - An Introduction to Graph-FRI",
                "reference_link": "publication/304505751_Sampling_and_Reconstruction_of_Sparse_Signals_on_Circulant_Graphs_-_An_Introduction_to_Graph-FRI",
                "reference_type": "Article",
                "reference_date": "Jun 2016",
                "reference_abstract": "With the objective of employing graphs toward a more generalized theory of signal processing, we present a novel sampling framework for (wavelet-)sparse signals defined on circulant graphs which extends basic properties of Finite Rate of Innovation (FRI) theory to the graph domain, and can be applied to arbitrary graphs via suitable approximation schemes. At its core, the introduced Graph-FRI-framework states that any K-sparse signal on the vertices of a circulant graph can be perfectly reconstructed from its dimensionality-reduced representation in the Graph Fourier domain of minimum size 2K. By leveraging the recently developed theory of e-splines and e-spline wavelets on graphs, one can decompose this graph spectral transformation into the multiresolution low-pass filtering operation with a graph e-spline filter, and subsequent transformation to the spectral graph domain; this allows to infer a distinct sampling pattern, and, ultimately, the structure of an associated coarsened graph, which preserves essential properties of the original, including circularity and, where applicable, the graph generating set."
            },
            {
                "reference_title": "A Multiscale Pyramid Transform for Graph Signals",
                "reference_link": "publication/288179601_A_Multiscale_Pyramid_Transform_for_Graph_Signals",
                "reference_type": "Article",
                "reference_date": "Jan 2015",
                "reference_abstract": "Multiscale transforms designed to process analog and discrete-time signals and images cannot be directly applied to analyze high-dimensional data residing on the vertices of a weighted graph, as they do not capture the intrinsic geometric structure of the underlying graph data domain. In this paper, we adapt the Laplacian pyramid transform for signals on Euclidean domains so that it can be used to analyze high-dimensional data residing on the vertices of a weighted graph. Our approach is to study existing methods and develop new methods for the four fundamental operations of graph downsampling, graph reduction, and filtering and interpolation of signals on graphs. Equipped with appropriate notions of these operations, we leverage the basic multiscale constructs and intuitions from classical signal processing to generate a transform that yields both a multiresolution of graphs and an associated multiresolution of a graph signal on the underlying sequence of graphs."
            },
            {
                "reference_title": "Random Sampling of Bandlimited Signals on Graphs",
                "reference_link": "publication/284096844_Random_Sampling_of_Bandlimited_Signals_on_Graphs",
                "reference_type": "Article",
                "reference_date": "Nov 2015",
                "reference_abstract": "We study the problem of sampling k-bandlimited signals on graphs. We propose\ntwo sampling strategies that consist in selecting a small subset of nodes at\nrandom. The first strategy is non-adaptive, i.e., independent of the graph\nstructure, and its performance depends on a parameter called the graph\ncoherence. On the contrary, the second strategy is adaptive but yields optimal\nresults. Indeed, no more than O(k log(k)) measurements are sufficient to ensure\nan accurate and stable recovery of all k-bandlimited signals. This second\nstrategy is based on a careful choice of the sampling distribution, which can\nbe estimated quickly. Then, we propose a computationally efficient decoder to\nreconstruct k-bandlimited signals from their samples. We prove that it yields\naccurate reconstructions and that it is also stable to noise. Finally, we\nconduct several experiments to test these techniques."
            },
            {
                "reference_title": "Subgraph-Based Filterbanks for Graph Signals",
                "reference_link": "publication/282000447_Subgraph-Based_Filterbanks_for_Graph_Signals",
                "reference_type": "Article",
                "reference_date": "Sep 2015",
                "reference_abstract": "We design a critically-sampled orthogonal transform for graph signals, via\ngraph filterbanks. Instead of partitioning the nodes in two sets so as to\nremove one every two nodes in the filterbank downsampling operations, the\ndesign is based on a partition of the graph in connected subgraphs. Coarsening\nis then achieved by defining one \"supernode\" for each subgraph: the edges for\nthis coarsened graph derives hence from connectivity between the subgraphs.\nUnlike the one every two nodes downsampling operation on bipartite graphs, this\ncoarsening operation does not have an exact formulation in the Fourier space of\nthe graph. Instead, we rely on the local Fourier bases of each subgraph to\ndefine filtering operations. We apply successfully this method to decompose\ngraph signals, and show promising performance on compression and denoising\nexperiments."
            }
        ]
    },
    {
        "title": "VYU\u017dIT\u00cd METOD EYE-TRACKINGU A BIOMETRICK\u00c9HO TESTOV\u00c1N\u00cd P\u0158I STUDIU KOGNITIVN\u00cd Z\u00c1T\u011a\u017dE V PROST\u0158ED\u00cd ARCGIS STORY MAP",
        "date": "December 2020",
        "doi": null,
        "conferance": "Conference: International Journal of Information and Communication Technologies in Education",
        "citations_count": null,
        "reference_count": "References (35)",
        "abstract": "Abstrakt Sou\u010dasn\u00fd trend ve \u0161kolstv\u00ed spole\u010dn\u011b s pandemi\u00ed COVID-19 potvrdil nutnost zav\u00e1d\u011bt informa\u010dn\u00ed technologie i do jin\u00fdch p\u0159edm\u011bt\u016f ne\u017eli informatiky. Ve chv\u00edli, kdy dojde k zav\u0159en\u00ed cel\u00e9 \u0161koly a klasick\u00e1 v\u00fduka bude nahrazena d\u00e1lkovou (online) v\u00fdukou, bude nutn\u00e9 pou\u017e\u00edvat webov\u00fdch aplikac\u00ed \u010di je p\u0159\u00edmo vytv\u00e1\u0159et. Lze tedy p\u0159edpokl\u00e1dat, \u017ee aplikac\u00ed pro d\u00e1lkovou v\u00fduku bude postupn\u011b p\u0159ib\u00fdvat. Jedn\u00edm ze slibn\u00fdch n\u00e1stroj\u016f pro d\u00e1lkov\u00e9 vzd\u011bl\u00e1v\u00e1n\u00ed (nejen) zem\u011bpisu lze ozna\u010dit aplikaci ArcGIS Story map spole\u010dnosti ESRI. Obliba tohoto prezenta\u010dn\u00edho m\u00e9dia rychle roste, p\u0159edev\u0161\u00edm d\u00edky interaktivit\u011b a mo\u017enosti poutav\u011b prezentovat multimedi\u00e1ln\u00ed obsah od webov\u00fdch map a\u017e po YouTube videa. Jak\u00e9 je v\u0161ak efektivn\u00ed mno\u017estv\u00ed informac\u00ed a \u00fakol\u016f a jejich n\u00e1ro\u010dnost, pokud je tento n\u00e1stroj pou\u017eit jako pom\u016fcka distan\u010dn\u00ed v\u00fduky? Kdy je kognitivn\u00ed z\u00e1t\u011b\u017e na \u017e\u00e1ka p\u0159ijateln\u00e1 a kdy za\u010dne doch\u00e1zet k zahlcen\u00ed a rezignaci \u017e\u00e1ka? Hlavn\u00edm c\u00edlem pl\u00e1novan\u00e9ho v\u00fdzkumu je co nejobjektivn\u011bji zm\u011b\u0159it pomoc\u00ed technologi\u00ed eye-trackingu a elektroderm\u00e1ln\u00ed aktivity kognitivn\u00ed z\u00e1t\u011b\u017e a poskytnout tak objektivn\u011bj\u0161\u00ed pohled na to, jak tyto aplikace koncipovat. Abstract The current trend in education, together with the COVID-19 pandemics, has confirmed the need to introduce information technology in subjects other than computer science. In the moment, when all schools will be closed and classical teaching will be replaced by distance (online) teaching, it will be necessary to use web applications or even create them. So, we can expect the number of distance learning applications to increase. the ArcGIS Story map application from ESRI can be marked as one of the promising tools for distance learning (not only) geography. The popularity of this presentation medium is growing rapidly, especially thanks to the interactivity and the ability to present multimedia from a web map to videos on YouTube in an engaging way. How much information and tasks and their complexity are effective if this tool is used as a distance learning utility? When is the cognitive load on the student acceptable and when does the student begin to overwhelm and resign? The main goal of the planned research is to measure the cognitive load as objectively as possible by using eye-tracking and electrodermal activity to bring a more objective view of how to design these applications.",
        "reference": [
            {
                "reference_title": "Measuring Cognitive Load in Embodied Learning Settings",
                "reference_link": "publication/318854401_Measuring_Cognitive_Load_in_Embodied_Learning_Settings",
                "reference_type": "Article",
                "reference_date": "Aug 2017",
                "reference_abstract": "In recent years, research on embodied cognition has inspired a number of studies on multimedia learning and instructional psychology. However, in contrast to traditional research on education and multimedia learning, studies on embodied learning (i.e., focusing on bodily action and perception in the context of education) in some cases pose new problems for the measurement of cognitive load. This review provides an overview over recent studies on embodied learning in which cognitive load was measured using surveys, behavioral data, or physiological measures. The different methods are assessed in terms of their success in finding differences of cognitive load in embodied learning scenarios. At the same time, we highlight the most important challenges for researchers aiming to include these measures into their study designs. The main issues we identified are: (1) Subjective measures must be appropriately phrased to be useful for embodied learning; (2) recent findings indicate potentials as well as problematic aspects of dual-task measures; (3) the use of physiological measures offers great potential, but may require mobile equipment in the context of embodied scenarios; (4) meta-cognitive measures can be useful extensions of cognitive load measurement for embodied learning."
            },
            {
                "reference_title": "Current topics in Czech and Central European geography education",
                "reference_link": "publication/316474694_Current_topics_in_Czech_and_Central_European_geography_education",
                "reference_type": "Book",
                "reference_date": "Jan 2017",
                "reference_abstract": "This book discusses current challenges related to teaching geography, mainly at the secondary school and higher education level. Focusing on a range of current topics, different methods, techniques, materials, applications, and approaches to geography education with a regional Central European perspective, the book makes an original contribution to the field. Most of the chapters aims at the practical development of the themes such as geography curriculum (Part I), global education, inquiry-based education, project-based learning, case studies, powerful teaching (Part II), using of information and communication technologies (Part III) in geography teaching. The final part (Part IV) covers some geopolitical, and socio-geographical aspects of the aforementioned Central European former communist countries from the point of view how to teach them with various methods. Therefore, the book can appeal to many geography or science students, researchers and educators studying geography education around the world."
            },
            {
                "reference_title": "The proliferation and implementation of GIS as an educational tool at gymnasiums/grammar schools in Czechia",
                "reference_link": "publication/287063497_The_proliferation_and_implementation_of_GIS_as_an_educational_tool_at_gymnasiumsgrammar_schools_in_Czechia",
                "reference_type": "Article",
                "reference_date": "Jan 2013",
                "reference_abstract": "This article contains the results of study, whose aim was to provide findings regarding the current state of proliferation of the GIS software at Czech grammar schools and also the barriers of further expansion of its use. Outputs of a questionnaire from beginning of year 2012 serve as the basic datasource for this study. The survey was sent to 103 schools and the resulting sample numbered 57 of them. The questionnaire is based on a methodology espoused in previous research studies and its aim is to assess the implementation 'profile' of grammar schools as a category. The level of implementation was considered in terms of its educational, technology and professional aspects. Results show that most of grammar schools are in the development stage in the 'Technology' and 'Professional' aspects, in the 'Educational' aspect, they find themselves in the institutionalization stage. The results of the study reveal that among teachers, the interest for teaching with GIS exists, but there is a lack of supporting materials and the teacher's poor GIS skills frequently poses additional obstacles. Furthermore, there is not so much information about free software and data resources among teachers."
            },
            {
                "reference_title": "The efficiency of instructional conditions: An approach to combine mental effort and performance measures",
                "reference_link": "publication/279892783_The_efficiency_of_instructional_conditions_An_approach_to_combine_mental_effort_and_performance_measures",
                "reference_type": "Article",
                "reference_date": "Jan 1993",
                "reference_abstract": "This article reports on a calculational approach for combining measures of mental workload and task performance that allows one to obtain information on the relative efficiency of instructional conditions. The method is based on the standardization of raw scores for mental effort and task performance to z scores, which are displayed in a cross of axes. Relative condition efficiency is calculated as the perpendicular distance to the line that is assumed to represent an efficiency of zero. We conclude that the method for calculating and representing relative condition efficiency discussed here can be a valuable addition to research on the training and performance of complex cognitive tasks."
            },
            {
                "reference_title": "Using galvanic skin response for cognitive load measurement in arithmetic and reading tasks",
                "reference_link": "publication/262319319_Using_galvanic_skin_response_for_cognitive_load_measurement_in_arithmetic_and_reading_tasks",
                "reference_type": "Conference Paper",
                "reference_date": "Nov 2012",
                "reference_abstract": "Galvanic Skin Response (GSR) has recently attracted researchers' attention as a prospective physiological indicator of cognitive load and emotions. However, it has commonly been investigated through single or few measures and in one experimental scenario. In this research, aiming to perform a comprehensive study, we have assessed GSR data captured from two different experiments, one including text reading tasks and the other using arithmetic tasks, each imposing multiple cognitive load levels. We have examined temporal and spectral features of GSR against different task difficulty levels. ANOVA test was applied for the statistical evaluation. Obtained results show the strong significance of the explored features, especially the spectral ones, in cognitive workload measurement in the two studied experiments."
            },
            {
                "reference_title": "Cognitive Load Measurement as a Means to Advance Cognitive Load Theory",
                "reference_link": "publication/252083119_Cognitive_Load_Measurement_as_a_Means_to_Advance_Cognitive_Load_Theory",
                "reference_type": "Article",
                "reference_date": "Mar 2003",
                "reference_abstract": "This paper discusses cognitive load measurement techniques with regard to their contribution to cognitive load theory (CLT). CLT is concerned with the design of instructional methods that efficiently use people's limited cognitive processing capacity to apply acquired knowledge and skills to new situations (i.e., transfer). CLT is based on a cognitive architecture that consists of a limited working memory with partly independent processing units for visual and auditory information, which interacts with an unlimited long-term memory. These structures and functions of human cognitive architecture have been used to design a variety of novel efficient instructional methods. The associated research has shown that measures of cognitive load can reveal important information for CLT that is not necessarily reflected by traditional performance-based measures. Particularly, the combination of performance and cognitive load measures has been identified to constitute a reliable estimate of the mental efficiency of instructional methods. The discussion of previously used cognitive load measurement techniques and their role in the advancement of CLT is followed by a discussion of aspects of CLT to which measurement of cognitive load is likely to be of benefit. Within the cognitive load framework, we will also discuss some promising new techniques."
            },
            {
                "reference_title": "How do teachers approach new technologies: geography teachers' attitudes towards geographic information systems (GIS)",
                "reference_link": "publication/228343724_How_do_teachers_approach_new_technologies_geography_teachers'_attitudes_towards_geographic_information_systems_GIS",
                "reference_type": "Article",
                "reference_date": "Jan 2009",
                "reference_abstract": "This study was aimed at understanding the extent to which GIS technology has been diffused throughout secondary school geography lessons in Turkey by focusing on geography teachers' attitudes towards GIS. A survey form was sent to geography teachers of around 200 private secondary schools in Turkey, 79 of which were responded to from 55 high schools located in 33 separate provinces. The study provided an understanding of teachers' knowledge, skills, and attitudes about GIS. As the study revealed, knowledge of GIS and its use in geography lessons by teachers was minimal. More than half of the teachers (66%) had no precise understanding of what GIS is and 82% of the teachers did not know how it could be used in geography lessons. The use of GIS among teachers in geography lessons was also found to be dramatic low. Around one seventh of the teachers (16%) said that they had used GIS software on a basic level before. Only seven of these teachers indicated that they had used GIS software in their geography lessons. Teachers' attitudes, however, were positive towards GIS. Most of the teachers (76%) thought that GIS is an effective teaching tool for geography lessons. Although some external barriers regarding lack of hardware, software, and data exist, the positive attitudes of teachers towards GIS is a significant factor which will contribute to integration of GIS into geography lessons in Turkey in the future."
            },
            {
                "reference_title": "Eye-tracking (nejen) v kognitivn\u00ed kartografii: Praktick\u00fd pr\u016fvodce tvorbou a vyhodnocen\u00edm experimentu",
                "reference_link": "publication/325334558_Eye-tracking_nejen_v_kognitivni_kartografii_Prakticky_pruvodce_tvorbou_a_vyhodnocenim_experimentu",
                "reference_type": "Book",
                "reference_date": "May 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Information Technologies in Teaching Geography from the Teacher\u2019s Point of View",
                "reference_link": "publication/309304486_Information_Technologies_in_Teaching_Geography_from_the_Teacher's_Point_of_View",
                "reference_type": "Chapter",
                "reference_date": "Oct 2017",
                "reference_abstract": "Nowadays we consider information technologies to be an essential part of material teaching tools, i.e. tools which the teacher and student use to achieve given educational goals. The use of visual, auditory and audiovisual media has become necessary in contemporary educational practice. The first section of the chapter deals with the options of using cloud tools in teaching geography (e.g. Google Drive, YouTube, forms, advanced search and easy publication of information, online cooperation in teaching, etc.). At the same time, there is presented a practical example of using mobile technology, social networks and infographics in teaching geography. The second section introduces a tool called ArcGIS online for creating maps and map applications, which is a great help in the creation of interactive learning. The following part of chapter is based on a case study, exploring the possibility of using geocaching to teach geography at Czech schools."
            },
            {
                "reference_title": "Augmented Cognition Foundations and Future Directions\u2014Enabling \u201cAnyone, Anytime, Anywhere\u201d Applications",
                "reference_link": "publication/221099449_Augmented_Cognition_Foundations_and_Future_Directions-Enabling_Anyone_Anytime_Anywhere_Applications",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2007",
                "reference_abstract": "Augmented Cognition is distinct from other disciplines due to its focus on using modern neuroscientific tools to determine\nthe \u2019in real time\u2019 cognitive state of an individual and then adapting the human-system interaction to meet a user\u2019s information\nprocessing needs based on this real-time assessment [1], [7], [14]. Augmented Cognition systems employ the use of physiologicaland\nneurophysiological-driven adaptive automation techniques to mitigate the effects of bottlenecks (e.g., attention, working\nmemory, executive function) and biases in cognition. Being able to non-invasively measure and assess a humansystem computing\noperator\u2019s cognitive state in real time and use adaptive automation (mitigation) techniques to modify and enhance their IP\ncapabilities in any application context is a goal that could substantially improve human performance and the way people interact\nwith 21st Century technology [9]. This paper highlights developments in the field of Augmented Cognition most relevant to future Universal\nAccess (UA) applications."
            }
        ]
    },
    {
        "title": "Preliminary Comparison of K-12 Computing Education in China and the United States",
        "date": "December 2020",
        "doi": null,
        "conferance": "Conference: IEEE TALE2020 \u2013 An International Conference on Engineering, Technology and Education",
        "citations_count": null,
        "reference_count": "References (6)",
        "abstract": "The thesis starts with the background of computing education in today's K-12 education development, and then compares Chinese and American computer science K-12 education based on frameworks and curriculum standards. The authors compared the framework design of computer science K-12 education in two countries from the perspectives of concept definition, structure, content, etc. To compare the curriculum design standards, the authors selected from the content, curriculum structure, curriculum implementation, curriculum teaching evaluation of curriculum and in-depth analysis and discussion were conducted from this perspective. The thesis summarizes the enlightenment of current Chinese education researchers on American K-12 computing education to Chinese education, and puts forward the author's own views.",
        "reference": [
            {
                "reference_title": "Constructionism: A New Opportunity for Elementary Science Education",
                "reference_link": "publication/239025374_Constructionism_A_New_Opportunity_for_Elementary_Science_Education",
                "reference_type": "Article",
                "reference_date": "Jan 1986",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2016",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Aug 2017",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2014",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2018",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Apr 2018",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "POSTER cbrc",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": null,
        "abstract": "Online - 2nd International Computational Biology Workshop of the Amirkabir University of Technology\n\nThe bioinformatics workshop is known as an advanced, specialized international educational-research activity, in which the latest scientific achievements are discussed.On 9th December 2020 , the second workshop of the ICoBi will be held virtually over Adobe Connect for 3 days. Due to the COVID-19 crisis, CBRC decided to switch to online methods. It is organized by the Department of Mathematics and Computer Science at AmirKabir University in collaboration with the Computational Biology Research Center (CBRC).",
        "reference": []
    },
    {
        "title": "A Bibliometric Review of Literature in Library User Education Published at Scopus-Elsevier Database Indexed Journals from 1976-2019",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (12)",
        "abstract": "Aim of this research is focuses on analysis of literature published in journals indexed in Scopus-Elsevier database and produced by library information professionals to facilitate library users in terms of effective use of the library. A total of 114 documents written by 252; 2.2% authors and 2.5 per annum in library user education covers six types of literary work from 1976-2019. Research articles 96; 84% got attention of the majority. Sixty-four documents were written by solo authors and 188; 3.7% of authors contributed in 50 documents. The Library Philosophy and Practice e-journal provide space for 21 documents to publish. The United States felt the importance of this subject and contributed with 35; 31% documents. Library information sciences can better explaine by computer sciences to utilized resources and services in the process of decision-making. This study revealed that, the ultimate goal of library professionals is to save the time of reading through the ease of system with educational programs, instead of this the subject got lase attention among the eyes of library professionals.",
        "reference": [
            {
                "reference_title": "A study of scientific publications on the greater cane rat (Thryonomys swinderianus, Temminck 1827)",
                "reference_link": "publication/340097917_A_study_of_scientific_publications_on_the_greater_cane_rat_Thryonomys_swinderianus_Temminck_1827",
                "reference_type": "Article",
                "reference_date": "Mar 2020",
                "reference_abstract": "Background\nThe greater cane rat (GCR), reputed to be African's second largest rodent, is a precocial hystricomorph with an uncommon phenotype and life history. Scientific and socio\u2010economic interests in the GCR have led to heightened research efforts targeted towards a better understanding of its biology and exploration of its economic and translational usefulness.\n\nMethods\nRecords of all online scientific publications on the GCR from Google, Google Scholar, PubMed, science.gov, Ebscohost and Worldwide science, with the exception of research theses, proceedings, unpublished projects and abstracts, were collated and analyzed using descriptive statistics.\n\nResults\nA total of 146 published scholarly articles spanning about six decades were retrieved, with 98% of the GCR publications originating from African countries. Nigeria boasts the highest number of publications (58.22%) followed by Ghana (21.23%) and South Africa (5.48%) while Senegal contributed the least (0.69%). Publications were sorted into ten field categories. The field with the highest number of articles (41.78%) was animal breeding and management recording, closely followed by anatomy (37.67%). Lesser contributions were made by parasitology (5.48%), biochemistry/hematology (4.8%), pharmacology/toxicology (4.11%), pathology (2.06%), and surgery/anesthesia and physiology (1.37% apiece). The fields with fewest contributions were microbiology and developmental biology (0.69% each).\n\nConclusion\nThis study chronicles the spectrum of knowledge available on the GCR, highlighting the knowledge gap that still exists in various fields in order to provide advocacy for new frontiers in research efforts on this rodent. We suggest the need for a clearly defined and well integrated national/regional policy aimed at establishing Africa's foremost micro\u2010livestock rodent, the greater cane rat, on the world's scientific radar."
            },
            {
                "reference_title": "Future challenges and emerging role of academic libraries in Pakistan: A phenomenology approach",
                "reference_link": "publication/338728308_Future_challenges_and_emerging_role_of_academic_libraries_in_Pakistan_A_phenomenology_approach",
                "reference_type": "Article",
                "reference_date": "Jan 2020",
                "reference_abstract": "This study was carried out to investigate the current and prospective challenges faced by academic libraries in Pakistan and to present possible solutions addressing these challenges. The research design was qualitative, adopting the phenomenology approach. In-depth interviews of 14 senior academic library leaders from public and private sector universities of Pakistan were conducted. Leadership crisis was identified as the top challenge followed by those related to changing user behavior, human resources, financial, technological issues, and changes in higher education. Prospective challenges encompassed issues related to technological modalities, human resources, research data management and library space. While the study participants indicated their readiness to cope with these challenges, they agreed that there was a need of collective effort for human capacity building, initiation of compatible smart services, effective policy making and creation of societal awareness. The support from key players such as the library professionals, library associations, top management, LIS schools, HEC and other funding agencies was deemed to be vital for this purpose."
            },
            {
                "reference_title": "Users\u2019 motivation and satisfaction as predictors of lecturers\u2019 use of Library Resources in Colleges of Education in southwest, Nigeria",
                "reference_link": "publication/332495311_Users'_motivation_and_satisfaction_as_predictors_of_lecturers'_use_of_Library_Resources_in_Colleges_of_Education_in_southwest_Nigeria",
                "reference_type": "Article",
                "reference_date": "Jan 2019",
                "reference_abstract": "The study investigated users\u2019 motivation and satisfaction as predictor of lecturers\u2019 use of library resources in colleges of education in southwest, Nigeria. This is to reveal that library user motivation and satisfaction will predict lecturers\u2019 effective use of library resources. . The descriptive research design was adopted, while a multi-stage sampling procedure was used in the study. Simple random sampling method was employed to select respondents from the selected colleges of education. A total of 568 lecturers (297 males and 271 females) participated in the study. A structured questionnaire tagged \u201cQuestionnaire on Users\u2019 Motivation, Satisfaction and Library Material on Lecturers Use of Library in Colleges of Education South West Nigeria (QMSLMLULCESWN), four research questions were answered. The results showed that the motivational level lecturers in colleges of education on the use of library is low, although the finding revealed that majorly of them frequently use the library. Also, the finding revealed that lecturers\u2019 level of satisfaction is at average level and users\u2019 motivation and satisfaction can predict lecturers\u2019 use of library. Therefore lecturers\u2019 motivation and satisfaction is necessary to enhance their use of library resources."
            },
            {
                "reference_title": "Shaping a Future for Library and CME through Partnerships",
                "reference_link": "publication/340906607_Shaping_a_Future_for_Library_and_CME_through_Partnerships",
                "reference_type": "Article",
                "reference_date": "Apr 2020",
                "reference_abstract": "The purpose of this article is to highlight the value of a partnership between library services and continuing medical education (CME) teams. Examples of a successful partnership between library services and CME within a health system will be shown. Through team collaboration, library and CME services provide quick access to educational resources and activities which benefit the delivery of optimal health care."
            },
            {
                "reference_title": "Mapping Evaluation, Appraisal and Stance in Discourse (2000\u20132015): A Bibliometric Analysis",
                "reference_link": "publication/339383477_Mapping_Evaluation_Appraisal_and_Stance_in_Discourse_2000-2015_A_Bibliometric_Analysis",
                "reference_type": "Article",
                "reference_date": "Feb 2020",
                "reference_abstract": "The present study employs a bibliometric analysis to examine the research trends in the field of evaluation , appraisal and stance . The bibliometric information of publications between 2000 and 2015 was retrieved from the Web of Science SSCI Core Collection database. The indicators analyzed include the number of publications by year, most frequently explored topics, most cited works, major individual contributors, publication venues, distribution among countries/regions and institutions. Our findings showed that the annual publications increased dramatically, revealing an upward trend in this research field. The results concerning the most frequently addressed topics suggested that EAP has been a fruitful domain in terms of the evaluative dimension of discourse. Besides, future research will feature more discipline-specific and language-specific empirical studies and comparative cross-linguistic studies. Pedagogical applications of evaluation research also need to be explored. Citation results indicated that the groundbreaking monographs in this field generate the highest citation counts, and that the most cited works cover a variety of sub-fields of linguistics, which may further prove the heterogeneous nature of the evaluative dimension of language."
            },
            {
                "reference_title": "The Impact of User Education at the University of Botswana Library",
                "reference_link": "publication/249750890_The_Impact_of_User_Education_at_the_University_of_Botswana_Library",
                "reference_type": "Article",
                "reference_date": "Sep 1998",
                "reference_abstract": "Report on a study undertaken to determine the use of library resources and services by graduate students at the University of Botswana. The study collected empirical data on graduate students\u2019 use of resources and services. Data were gathered from 144 of 223 graduate students registered for the 1996/1997 academic year. The findings indicated that guidance in the use of library resources and services is necessary to help students meet some of their information requirements. There is a significant relationship between perceived ability to use information sources and study programme. The study concluded that to enable students make maximum use of the library resources and services, they need training on how to use them."
            },
            {
                "reference_title": "Effect of the User Education Programme on Undergraduate Students' Library Exploration at the University of Ilorin",
                "reference_link": "publication/248577658_Effect_of_the_User_Education_Programme_on_Undergraduate_Students'_Library_Exploration_at_the_University_of_Ilorin",
                "reference_type": "Article",
                "reference_date": "Dec 1998",
                "reference_abstract": "The essence of education, formal or informal, is the ability of the learner to put what he has learnt into practical use. Hence, the knowledge acquired must be reflected in the life of the learner. User education programmes are planned in tertiary institutions to ease the work of librarians and quicken the efforts of clientele in getting and utilizing the library materials. This work, therefore, investigates the use of the programme in the university studied. The investigation also sought to reveal the extent to which the objectives of the user were currently being met. It was discovered that much has not been achieved, as lots of improvements are necessary in the areas investigated. Suggestions for its improvement are given."
            },
            {
                "reference_title": "The Impact of the Web on User Education at the Science, Industry and Business Library (SIBL) of The New York Public Library",
                "reference_link": "publication/232972297_The_Impact_of_the_Web_on_User_Education_at_the_Science_Industry_and_Business_Library_SIBL_of_The_New_York_Public_Library",
                "reference_type": "Article",
                "reference_date": "Dec 1998",
                "reference_abstract": "Like many other libraries, the Science, Industry and Business Library (SIBL) of The New York Public Library instructs customers in using the Web. In addition, the library is using the Web to further educate and assist its customers. SIBL provides Web access to its catalogs, a Web menu for the selection of electronic databases, Web guides for doing research in various subjects, and Web-accessible instructional materials. The library is also planning Web-based tutorials for its site which will reach a new, remote audience. Remote access to learning opportunities will enhance and extend traditional library services."
            },
            {
                "reference_title": "User Education in New Zealand tertiary Libraries: An international comparison",
                "reference_link": "publication/222303317_User_Education_in_New_Zealand_tertiary_Libraries_An_international_comparison",
                "reference_type": "Article",
                "reference_date": "Jul 1998",
                "reference_abstract": "The article reports the results of a survey of user education objectives and practices in New Zealand academic libraries, and makes comparisons to a Canadian study. The New Zealand research also explored librarians' attitudes about the relationship between user education and information literacy."
            },
            {
                "reference_title": "Library User Education: Examining Its Past, Projecting Its Future",
                "reference_link": "publication/32961574_Library_User_Education_Examining_Its_Past_Projecting_Its_Future",
                "reference_type": "Article",
                "reference_date": "Sep 1995",
                "reference_abstract": "published or submitted for publication"
            }
        ]
    },
    {
        "title": "State of the art on system architectures for data integration",
        "date": "December 2020",
        "doi": null,
        "conferance": "Conference: Rio Oil & Gas Expo and Conference 2020",
        "citations_count": null,
        "reference_count": "References (42)",
        "abstract": "Data integration is a major challenge faced by organizations that have a large amount of data. In the literature, the problem of data integration is discussed in different areas of knowledge, from different perspectives.To have a comprehensive view of this research area, a systematic review of the literature on systems architectures for data integration was carried out in the last five years. Based on the recommendations of the PRISMA Declaration, the focus of the review was to seek architectures that adhere to the needs of data integration in the Oil and Gas (O&G) sector. In the searches, only works related to the areas of Computer Engineering, Computer Science and Information Systems were filtered. Among 272 works found in six large databases, 30 were selected, based on predefined criteria. After the reading and analysis process, two works applied in the O&G industry were identified, which used the OBDA approach. It was concluded that the approach most cited by the authors to perform data integration is based on ontology and that most of the presented system architectures are not yet capable of handling large volumes of data.",
        "reference": [
            {
                "reference_title": "A hybrid framework for industrial data storage and exploitation",
                "reference_link": "publication/333987258_A_hybrid_framework_for_industrial_data_storage_and_exploitation",
                "reference_type": "Article",
                "reference_date": "Jan 2019",
                "reference_abstract": "In this paper a hybrid framework is illustrated, with a software and hardware integration strategy, for an industrial platform that exploits features from a Relational Database (RDB) and Triplestore using the blackboard architectural pattern, ensuring efficient and accurate communication concerning data transfer among software applications and devices. Specifically, \u201cRaw Data Handler\u201d, manages unstructured data from IoT devices that are kept in an Apache Cassandra instance, while \u201cProduction Data Handler\u201d acts on structured data, persisted in a MySQL database. Filtered data is transformed into knowledge and persisted into the Triplestore database (DB) and can be retrieved by expert systems at any time. The proposed framework will be tested and validated within Z-Fact0r project."
            },
            {
                "reference_title": "Foundations of Ontology-Based Data Access under Bag Semantics",
                "reference_link": "publication/331143432_Foundations_of_Ontology-Based_Data_Access_under_Bag_Semantics",
                "reference_type": "Article",
                "reference_date": "Feb 2019",
                "reference_abstract": "Ontology-based data access (OBDA) is a popular approach for integrating and querying multiple data sources by means of a shared ontology. The ontology is linked to the sources using mappings, which assign to ontology predicates views over the data. The conventional semantics of OBDA is set-based\u2014that is, the extension of the views defined by the mappings does not contain duplicate tuples. This treatment is, however, in disagreement with the standard semantics of database views and database management systems in general, which is based on bags and where duplicate tuples are retained by default. The distinction between set and bag semantics in databases is very significant in practice, and it influences the evaluation of aggregate queries. In this article, we propose and study a bag semantics for OBDA which provides a solid foundation for the future study of aggregate and analytic queries. Our semantics is compatible with both the bag semantics of database views and the set-based conventional semantics of OBDA. Furthermore, it is compatible with existing bag-based semantics for data exchange recently proposed in the literature. We show that adopting a bag semantics makes conjunctive query answering in OBDA CONP-hard in data complexity. To regain tractability of query answering, we consider suitable restrictions along three dimensions, namely, the query language, the ontology language, and the adoption of the unique name assumption. Our investigation shows a complete picture of the computational properties of query answering under bag semantics over ontologies in the DL-Lite family."
            },
            {
                "reference_title": "BigDimETL with NoSQL Database",
                "reference_link": "publication/327290582_BigDimETL_with_NoSQL_Database",
                "reference_type": "Article",
                "reference_date": "Jan 2018",
                "reference_abstract": "In the last decade, we have witnessed an explosion of data volume available on the Web. This is due to the rapid technological advances with the availability of smart devices and social networks such as Twitter, Facebook, Instagram, etc. Hence, the concept of Big Data was created to face this constant increase. In this context, many domains should take in consideration this growth of data, especially, the Business Intelligence (BI) domain. Where, it is full of important knowledge that is crucial for effective decision making. However, new problems and challenges have appeared for the Decision Support System that must be addressed. Accordingly, the purpose of this paper is to adapt Extract-Transform-Load (ETL) processes with Big Data technologies, in order to support decision-making and knowledge discovery. In this paper, we propose a new approach called Big Dimensional ETL (BigDimETL) dealing with ETL development process and taking into account the Multidimensional structure. In addition, in order to accelerate data handling we used the MapReduce paradigm and Hbase as a distributed storage mechanism that provides data warehousing capabilities. Experimental results show that our ETL operation adaptation can perform well especially with Join operation."
            },
            {
                "reference_title": "Data integration for o\ufb00shore decommissioning waste management",
                "reference_link": "publication/337228653_Data_integration_for_offshore_decommissioning_waste_management",
                "reference_type": "Article",
                "reference_date": "Nov 2019",
                "reference_abstract": "Offshore decommissioning represents significant business opportunities for oil and gas service companies. However, for owners of offshore assets and regulators, it is a liability because of the associated costs. One way of mitigating decommissioning costs is through the sales and reuse of decommissioned items. To achieve this\neffectively, reliability assessment of decommissioned items is required. Such an assessment relies on data collected on the various items over the lifecycle of an engineering asset. Considering that offshore platforms have a design life of about 25 years and data management techniques and tools are constantly evolving, data captured about items to be decommissioned will be in varying forms. In addition, considering the many stakeholders involved with a facility over its lifecycle, information representation of the items will have variations. These challenges make data integration difficult. As a result, this research developed a data integration framework that makes use of Semantic Web technologies and ISO 15926 - a standard for process plant data integration - for rapid assessment of decommissioned items. The proposed solution helps in determining the reuse potential of decommissioned items, which can save on cost and benefit the environment."
            },
            {
                "reference_title": "Integration of Region-based Open Data Using Semantic Web",
                "reference_link": "publication/334859294_Integration_of_Region-based_Open_Data_Using_Semantic_Web",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "An industrial evaluation of data access techniques for the interoperability of engineering software tools",
                "reference_link": "publication/332757659_An_industrial_evaluation_of_data_access_techniques_for_the_interoperability_of_engineering_software_tools",
                "reference_type": "Article",
                "reference_date": "Apr 2019",
                "reference_abstract": "New industrial initiatives such as Industrie 4.0 rely on digital end-to-end engineering across the entire product lifecycle, which in turn depends on the ability of the supporting software tools to interoperate. A tool interoperability approach based on Linked Data, and the OASIS OSLC standard, has the potential to provide such integration, where each software tool can expose its information and services to other tools using the web as a common technology base. In this paper, we report on our negative findings when attempting to use existing ontology-based Linked Data access techniques to expose and manipulate the structured content managed in engineering tools. Such techniques typically target the Data Access Layer (DAL) of a tool to access and manipulate its content, with the assumption that sufficient information and control is available within this layer to automate the process. Based on a case study with the truck manufacturer Scania CV AB, our study finds that an engineering tool controls its artefacts using business logic that is not necessarily reflected at the data layer. This renders such ontology-based access techniques inadequate. Instead we propose an alternative Linked Data extraction architecture that can mitigate the identified shortcomings. While less automated compared to the existing solutions, the proposed architecture is realised as a standalone library that can still facilitate the extraction process."
            },
            {
                "reference_title": "Information Integration of Heterogeneous Medical Database Systems Using Metadata",
                "reference_link": "publication/327263360_Information_Integration_of_Heterogeneous_Medical_Database_Systems_Using_Metadata",
                "reference_type": "Conference Paper",
                "reference_date": "Nov 2017",
                "reference_abstract": null
            },
            {
                "reference_title": "A Tensor Based Data Model for Polystore: An Application to Social Networks Data",
                "reference_link": "publication/326309578_A_Tensor_Based_Data_Model_for_Polystore_An_Application_to_Social_Networks_Data",
                "reference_type": "Conference Paper",
                "reference_date": "Jun 2018",
                "reference_abstract": "In this article, we show how the mathematical object tensor can be used to build a multi-paradigm model for the storage of social data in data warehouses. From an architectural point of view, our approach allows to link different storage systems (polystore) and limits the impact of ETL tools performing model transformations required to feed different analysis algorithms. Therefore, systems can take advantage of multiple data models both in terms of query execution performance and the semantic expressiveness of data representation. The proposed model allows to reach the logical independence between data and programs implementing analysis algorithms. With a concrete case study on message virality on Twitter during the French presidential election of 2017, we highlight some of the contributions of our model."
            },
            {
                "reference_title": "Efficient approach to database integration for an aerospace vehicle design and certification framework",
                "reference_link": "publication/325568827_Efficient_approach_to_database_integration_for_an_aerospace_vehicle_design_and_certification_framework",
                "reference_type": "Article",
                "reference_date": "Apr 2018",
                "reference_abstract": "The integration of distributed data sources is one of the main problems of engineering software. The data integration process for a heterogeneous legacy system is a key aspect of the development of a computerized system and in the integration of a design framework. In this research, our approach to data integration focuses on developing system-building techniques for efficient data integration queries. Keyword-based data searching is investigated and applied within a database for a design framework. A database table connector (DTC) wrapper program is implemented based on the use of data integration processes and keyword-based searching. The DTC provides data integration for various data resources from legacy programs and database management systems using SQL querying. The DTC enables designers and developers to rapidly and efficiently develop integration frameworks for different data resources. This paper also describes the implementation and deployment of the Certification and Aircraft Design Integration System framework, which integrates various analysis and optimization codes, computer-aided design software and database management systems. Multiple data types are used within the framework, including databases, spreadsheets, flat files, XML files and personal data management. Several aircraft design and optimization problems are successfully solved using the developed framework."
            },
            {
                "reference_title": "MONTRA: An agile architecture for data publishing and discovery",
                "reference_link": "publication/324085190_MONTRA_An_agile_architecture_for_data_publishing_and_discovery",
                "reference_type": "Article",
                "reference_date": "Mar 2018",
                "reference_abstract": "Background and Objective\nData catalogues are a common form of capturing and presenting information about a specific kind of entity (e.g. products, services, professionals, datasets, etc.). However, the construction of a web-based catalogue for a particular scenario normally implies the development of a specific and dedicated solution. In this paper, we present MONTRA, a rapid-application development framework designed to facilitate the integration and discovery of heterogeneous objects, which may be characterized by distinct data structures.\n\nMethods\nMONTRA was developed following a plugin-based architecture to allow dynamic composition of services over represented datasets. The core of MONTRA's functionalities resides in a flexible data skeleton used to characterize data entities, and from which a fully-fledged web data catalogue is automatically generated, ensuring access control and data privacy.\n\nResults\nMONTRA is being successfully used by several European projects to collect and manage biomedical databases. In this paper, we describe three of these applications scenarios.\n\nConclusions\nThis work was motivated by the plethora of geographically scattered biomedical repositories, and by the role they can play altogether for the understanding of diseases and of the real-world effectiveness of treatments. Using metadata to expose datasets\u2019 characteristics, MONTRA greatly simplifies the task of building data catalogues. The source code is publicly available at https://github.com/bioinformatics-ua/montra."
            }
        ]
    },
    {
        "title": "Using Interactive Web-Based Animations to Help Students to Find the Optimal Algorithms of River Crossing Puzzles",
        "date": "December 2020",
        "doi": "10.12753/2066-026X-20-010",
        "conferance": "Conference: eLearning and Software for Education",
        "citations_count": "Reference Text and Citations ",
        "reference_count": "References (2)",
        "abstract": "To acquire algorithmic thinking is a long process that has a few steps. The most basic level of algorithmic thinking is when students recognize the algorithms and various problems that can be solved with algorithms. At the second level, students can execute the given algorithms. At the third level of algorithmic thinking, students can analyze the algorithms, they recognize which steps are executed in sequences, conditions or loops. At the fourth level, students can create their algorithms. The last three levels of algorithmic thinking are: the implementation of the algorithms in a programming language, modifying and improving the algorithms, and creating complex algorithms. In preliminary research related to algorithmic thinking, we investigated how first-year undergraduate computer science students of J. Selye University can solve problems associated with the second, third and fourth level of algorithmic thinking. We chose these levels because these levels do not require to know any programming language. The tasks that students had to solve were for example: what will be the route of a robot when it executes the given instructions, how many times we need to cross a river to carry everyone to another river-bank. To solve these types of tasks requires only good algorithmic thinking. The results showed that students reached 81.4% average score on tasks related to the execution of given algorithms, 72.3% average score on tasks where they needed to analyze algorithms, and 66.2% average score on tasks where students needed to create algorithms. The latter type of tasks were mostly various river-crossing problems. Even though, that students reached a 66.2% average score on these tasks, if we had accepted only solutions with the optimal algorithms (minimal number of river crossing), they would have reached only a 21.3% average score, which is very low. To help students find the optimal algorithms of river crossing puzzles, we developed several interactive web-based animations. In the last part of this paper, we describe these animations, we summarize how they were created and how they can be used in education. Finally, we conclude and briefly mention our plans related to our future research.",
        "reference": [
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2006",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2019",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Argument Mining Driven Analysis of Peer-Reviews",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": "dinal Dataset of Peer Reviews and Citations.",
        "reference_count": "References (37)",
        "abstract": "Peer reviewing is a central process in modern research and essential for ensuring high quality and reliability of published work. At the same time, it is a time-consuming process and increasing interest in emerging fields often results in a high review workload, especially for senior researchers in this area. How to cope with this problem is an open question and it is vividly discussed across all major conferences. In this work, we propose an Argument Mining based approach for the assistance of editors, meta-reviewers, and reviewers. We demonstrate that the decision process in the field of scientific publications is driven by arguments and automatic argument identification is helpful in various use-cases. One of our findings is that arguments used in the peer-review process differ from arguments in other domains making the transfer of pre-trained models difficult. Therefore, we provide the community with a new peer-review dataset from different computer science conferences with annotated arguments. In our extensive empirical evaluation, we show that Argument Mining can be used to efficiently extract the most relevant parts from reviews, which are paramount for the publication decision. The process remains interpretable since the extracted arguments can be highlighted in a review without detaching them from their context.",
        "reference": [
            {
                "reference_title": "Hierarchical Transformers for Long Document Classification",
                "reference_link": "publication/339404081_Hierarchical_Transformers_for_Long_Document_Classification",
                "reference_type": "Conference Paper",
                "reference_date": "Dec 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Yes, we can! Mining Arguments in 50 Years of US Presidential Campaign Debates",
                "reference_link": "publication/335778743_Yes_we_can_Mining_Arguments_in_50_Years_of_US_Presidential_Campaign_Debates",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Building an Argument Search Engine for the Web",
                "reference_link": "publication/322582506_Building_an_Argument_Search_Engine_for_the_Web",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2017",
                "reference_abstract": null
            },
            {
                "reference_title": "TACAM: Topic And Context Aware Argument Mining",
                "reference_link": "publication/336661123_TACAM_Topic_And_Context_Aware_Argument_Mining",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2019",
                "reference_abstract": "In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from large text corpora. In previous works, the usual approach is to use a standard search engine to extract text parts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argument mining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argument mining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argument mining task."
            },
            {
                "reference_title": "Classification and Clustering of Arguments with Contextualized Word Embeddings",
                "reference_link": "publication/335781356_Classification_and_Clustering_of_Arguments_with_Contextualized_Word_Embeddings",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Argument Mining for Understanding Peer Reviews",
                "reference_link": "publication/334601696_Argument_Mining_for_Understanding_Peer_Reviews",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Does My Rebuttal Matter? Insights from a Major",
                "reference_link": "publication/334601118_Does_My_Rebuttal_Matter_Insights_from_a_Major",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "A Streamlined Method for Sourcing Discourse-level Argumentation Annotations from the Crowd",
                "reference_link": "publication/334600246_A_Streamlined_Method_for_Sourcing_Discourse-level_Argumentation_Annotations_from_the_Crowd",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "ArgumenText: Searching for Arguments in Heterogeneous Sources",
                "reference_link": "publication/325449247_ArgumenText_Searching_for_Arguments_in_Heterogeneous_Sources",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Cross-topic Argument Mining from Heterogeneous Sources Using Attention-based Neural Networks",
                "reference_link": "publication/323257205_Cross-topic_Argument_Mining_from_Heterogeneous_Sources_Using_Attention-based_Neural_Networks",
                "reference_type": "Article",
                "reference_date": "Feb 2018",
                "reference_abstract": "Argument mining is a core technology for automating argument search in large document collections. Despite its usefulness for this task, most current approaches to argument mining are designed for use only with specific text types and fall short when applied to heterogeneous texts. In this paper, we propose a new sentential annotation scheme that is reliably applicable by crowd workers to arbitrary Web texts. We source annotations for over 25,000 instances covering eight controversial topics. The results of cross-topic experiments show that our attention-based neural network generalizes best to unseen topics and outperforms vanilla BiLSTM models by 6% in accuracy and 11% in F-score."
            }
        ]
    },
    {
        "title": "New Teaching Methods by Using Microcontrollers in Teaching Programming",
        "date": "December 2020",
        "doi": "10.12753/2066-026X-20-082",
        "conferance": "Conference: eLearning and Software for Education",
        "citations_count": "Reference Text and Citations ",
        "reference_count": "References (8)",
        "abstract": "Everyone knows that IT and IT education are important today. Within that, programming has a high priority. From the point of view of education, it does not matter what method we teach. In the educational process, it is very important to attract students' attention and interest in programming. Programming methodology is one of the oldest areas of computer science education, so several methods are used to teach it today. Some of these can be used effectively in primary and secondary education, while others can be used in higher education. Important teaching methods have now emerged in certain areas of programming education. Most teachers do not use a single method, but a mixture of methods in which one of them dominates. Nowadays almost everything is controlled by electronics. We see various electronics gadgets almost everywhere around us. More and more microcontrollers are being used. In any school, it would be good to teach IT to mean more than basic computer management. Information technology has a lot of possibilities and it is very easy to do amazing things with it! Technology, electronics, and information technology are increasingly part of our world. By programming microcontrollers, we can make the teaching of programming more interesting. With the help of lights and sounds, students' attention can be better captured. This can be used to make programming learning interesting. Nowadays, a lot of primary and secondary schools are using this method and students are visibly motivated and more and more students are choosing electrotechnical and IT in their further studies. In this article, we introduce the use of this method.",
        "reference": [
            {
                "reference_title": "A fun and effective self-learning approach to teaching microcontrollers and mobile robotics",
                "reference_link": "publication/277913155_A_fun_and_effective_self-learning_approach_to_teaching_microcontrollers_and_mobile_robotics",
                "reference_type": "Article",
                "reference_date": "May 2015",
                "reference_abstract": "This paper describes how students can be guided to become their own best teachers, able to test their newly acquired skills with only minimum or no direct assistance from an instructor or lecturer. \u2018Closed-loop\u2019 student-centred learning and problem-based learning approaches are described, involving weekly lectures and hands-on lab activities that keep students highly curious, motivated and engaged in self-regulated learning. Students are required to design and test their own original circuits and software code by modifying, extending or expanding the sample circuits and example code described in the lecture notes, in order to complete and demonstrate specific objectives or requirements for each weekly lab session. These \u2018closed-loop\u2019 student-centred learning labs ensure that all teams of students achieve a common or minimum acceptable level of practical skills, which adequately prepares them for a \u2018design-and-build\u2019 competition. This style of learning also helps develop generic life-long learning skills such as investigating and identifying problems (problem definition and analysis), independent research and experimentation, decision making, communication and teamwork. Even without any prior hands-on experience with electronic circuit design, programming and microcontrollers, each team of students was able to apply and demonstrate new knowledge and skills, devise and test original designs for circuits and software without any supervision, solve and fix complex problems successfully and confidently, and build an operational remote-controlled electric vehicle or mobile robot for a final competition. Some went even further and built sensor-guided fully autonomous mobile robots. Three different competitions described in this paper include a timed racing car competition, a multi-player box grabbing contest over a rocky obstacle course and a \u2018robot wars\u2019 competition that was televised on Channel 10 news in Australia."
            },
            {
                "reference_title": "Teaching microcontrollers",
                "reference_link": "publication/3679017_Teaching_microcontrollers",
                "reference_type": "Conference Paper",
                "reference_date": "Dec 1996",
                "reference_abstract": "This presentation provides a brief look at the process used to\nteach microcontrollers to junior/seniors in EE and CpE. Each pair of\nstudents is issued a development hardware kit and related software. All\nsoftware development is perfected on the student's required personal\ncomputer. The style presented permits teaching of several\nmicrocontrollers i.e. Motorola 68HC11, Intel 8051 and Microchip PIC\nfamily. Example laboratories are suggested"
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2004",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2016",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2019",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2011",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2013",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2015",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Build your Own [Virtual] Microprocessor (CDT-48)",
        "date": "December 2020",
        "doi": "10.13140/RG.2.2.17794.50887/3",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (1)",
        "abstract": "Though mostly unnoticed, microprocessors are everywhere, being responsible for operation of a wide range of critically important systems. Learning microprocessor concepts can therefore contribute to better understanding computer science and architecture, as well as providing basis for probing further into assembly, operating systems, microcontroller-based machines, genetic programming, distributed, multicore and GPU systems. In this work, we provide a brief introduction to sequential machines (von Neumann architecture) and how simple related microprocessor can be modeled and interpreted by using a very short and simple piece of code, which is then used to illustrate some assembly programs including adding two values, block transfer, and input/output.",
        "reference": [
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2009",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Explaining Cognitive Computing Through the Information Systems Lens",
        "date": "December 2020",
        "doi": null,
        "conferance": "Conference: Forty-First International Conference on Information Systems, ICIS",
        "citations_count": null,
        "reference_count": "References (41)",
        "abstract": "Cognitive computing (COC) aims to embed human cognition into computerized models. However, there is no scientific classification that delineates the nature of Cognitive Computing. Unlike the medical and computer science fields, Information Systems (IS) has conducted very little research on COC. Although the potential to make important research contributions in this area is great, we argue that the lack of a cohesive interpretation of what constitutes COC has led to inferior COC research in IS. Therefore, we need first to clearly identify COC as a phenomenon to be able to identify and guide prospective research areas in IS. In this research, a phenomenological approach is adopted using thematic analysis of the published literature in COC research. Then, we discuss how IS may contribute to the development of design science artifacts under the COC umbrella. In addition, the paper raises important questions for future research by highlighting how IS researchers could make meaningful contributions to this emerging topic.",
        "reference": [
            {
                "reference_title": "A Cognitive Framework for Detecting Phishing Websites",
                "reference_link": "publication/331535390_A_Cognitive_Framework_for_Detecting_Phishing_Websites",
                "reference_type": "Conference Paper",
                "reference_date": "Mar 2019",
                "reference_abstract": "Phishing over internet is a permeating threat that represents fifth of online business websites. Despite the extensive research in phishing websites detection, none cope with the continuous development in phishing techniques. Therefore, a cognitive, dynamic, and self-adaptive phishing detection system is needed to automatically detect new phishing strategies. Cognitive Computing techniques mimic the reasoning and learning abilities of human brain. In this paper, we propose a cognitive framework for phishing websites detection. The framework uses a cognitive network called a bidirectional long short-term memory (BLSTM) recurrent neural network (RNN). In addition, we integrated a Convolutional Neural Network (CNN) for semantically identifying objects and actions in websites' images. Existing phishing website detection systems suffer from poor image features performance as they use only statistical and structural features of images. The framework is supposed to outperform existing systems because it can learn from context continuously detect new phishing techniques."
            },
            {
                "reference_title": "Service-Dominant Logic and Information Systems Research: A Review and Analysis Using Topic Modeling",
                "reference_link": "publication/320224929_Service-Dominant_Logic_and_Information_Systems_Research_A_Review_and_Analysis_Using_Topic_Modeling",
                "reference_type": "Conference Paper",
                "reference_date": "Dec 2017",
                "reference_abstract": "Since its introduction 2004, Service-Dominant Logic (SDL) has attracted increasing interest among scholars of IS. To advance knowledge about the role of IT in service, IS researchers draw on SDL as a meta-theoretical foundation for their inquiries. Developing a better understanding of the role of IT in service consequently represents a key research priority for both, the emerging service science field, as well as for IS research. Strikingly, no comprehensive review of this body of knowledge exists today. We conduct such a review by using topic modeling algorithms, which help to map the evolution of SDL research from its inception until today. Our approach is systematic, fully replicable, and enables us to identify hidden structures and thematic interdependencies. We analyze 1.441 articles comprising 23,568 pages of text. We delineate 72 topics and track their diffusion across research disciplines in the field of management research and IS in particular."
            },
            {
                "reference_title": "Preparing for the cognitive generation of decision support",
                "reference_link": "publication/319929429_Preparing_for_the_cognitive_generation_of_decision_support",
                "reference_type": "Article",
                "reference_date": "Jan 2017",
                "reference_abstract": "Every 10 years there has been a significant evolution in computer-based support for decision making. The next cycle, or generation, is due in the early 2020s and is starting to emerge. While this new cognitive generation has several important characteristics, the most significant will be the widespread use of artificial intelligence. This article describes the cognitive generation and provides recommendations for how companies should prepare for it."
            },
            {
                "reference_title": "New paradigms of cognitive management extending computational intelligence approaches",
                "reference_link": "publication/311892415_New_paradigms_of_cognitive_management_extending_computational_intelligence_approaches",
                "reference_type": "Article",
                "reference_date": "Jun 2018",
                "reference_abstract": "This publication presents new paradigms of cognitive management in information systems. The cognitive management aspects dedicated to support management processes are based on interpretations of the meaning of the analyzed data/situation, etc. The management processes are shown as it is applied to different subjects because of the diverse possibilities of executing management processes. This publication shows the versatility of the presented approaches. Cognitive management processes were defined on the basis of methods for the cognitive interpretation of information applied to semantic information contained in data sets. Semantic data interpretation used to determine the meaning of the analyzed information, and semantic layers contained in data sets. Also is dedicated to significance for the whole-process of cognitive management. Cognitive management processes were discussed as they are applied to various subjects, i.e. data, information, structures, processes or situations. Cognitive management processes were defined for the purpose of supporting existing data management solutions."
            },
            {
                "reference_title": "Realist trials and the testing of context-mechanism-outcome configurations: A response to Van Belle et al.",
                "reference_link": "publication/309061186_Realist_trials_and_the_testing_of_context-mechanism-outcome_configurations_A_response_to_Van_Belle_et_al",
                "reference_type": "Article",
                "reference_date": "Oct 2016",
                "reference_abstract": "Background\nVan Belle et al. argue that our attempt to pursue realist evaluation via a randomised trial will be fruitless because we misunderstand realist ontology (confusing intervention mechanisms with intervention activities and with statistical mediation analyses) and because RCTs cannot comprehensively examine how and why outcome patterns are caused by mechanisms triggered in specific contexts.\n\nMethods\nThrough further consideration of our trial methods, we explain more fully how we believe complex social interventions work and what realist evaluation should aim to do within a trial.\n\nResults\nLike other realists, those undertaking realist trials assume that: social interventions provide resources which local actors may draw on in actions that can trigger mechanisms; these mechanisms may interact with contextual factors to generate outcomes; and data in the \u2018empirical\u2019 realm can be used to test hypotheses about mechanisms in the \u2018real\u2019 realm. Whether or not there is sufficient contextual diversity to test such hypotheses is a contingent not a necessary feature of trials. Previous exemplars of realist evaluation have compared empirical data from intervention and control groups to test hypotheses about real mechanisms. There is no inevitable reason why randomised trials should not also be able to do so. Random allocation merely ensures the comparability of such groups without necessarily causing evaluation to lapse from a realist into a \u2018positivist\u2019 or \u2018post-positivist\u2019 paradigm.\n\nConclusions\nRealist trials are ontologically and epistemologically plausible. Further work is required to assess whether they are feasible and useful but such work should not be halted on spurious philosophical grounds."
            },
            {
                "reference_title": "Cognitive Computing for Intelligence Systems",
                "reference_link": "publication/338667755_Cognitive_Computing_for_Intelligence_Systems",
                "reference_type": "Article",
                "reference_date": "Jan 2020",
                "reference_abstract": null
            },
            {
                "reference_title": "Neural Cognitive Computing Mechanisms: The Brain and Machine Eyes",
                "reference_link": "publication/331133182_Neural_Cognitive_Computing_Mechanisms_The_Brain_and_Machine_Eyes",
                "reference_type": "Chapter",
                "reference_date": "Jan 2020",
                "reference_abstract": "In this chapter, a mobile robotic system is designed under the vision\u2013brain hypothesis, taking the wheeled mobile robotic (WMR) system as an example. Based on the hypothesis and results of Chap. 3, robots can selectively detect and tracking objects and the robot path-planning problem have been solved. Therefore, an adaptive neural network (NN)-based tracking control algorithm is enough to design the full state constrained WMR system. To deal with the brain-inspired tracking task requirements of the WMR system, it is necessary to take the full state constraints problem into account and based on the assumptions and lemmas given in this chapter, the uniform ultimate boundedness for all signals in the WMR system can be guaranteed to ensure the tracking error converges to zero. Numerical experiments are presented to illustrate the good performance of our control algorithm. Moreover, a partial reinforcement learning neural network (PRLNN)-based tracking algorithm is proposed to enhance the WMR system performance. As the major neural cognitive computing mechanisms the enhanced WMR system, PRLNN adaptive control solve the WMR tracking problem with the time-varying advance angle. The critic NN and action NN adaptive laws for decoupled controllers are designed using the standard gradient-based adaptation method. The Lyapunov stability analysis theorem is employed to test whether the uniform ultimate boundedness of all signals in the system can be guaranteed, and in addition, a numerical simulation is also presented to verify the effectiveness of the proposed control algorithm."
            },
            {
                "reference_title": "From Molecule to Metaphor: A Neural Theory of Language",
                "reference_link": "publication/329648006_From_Molecule_to_Metaphor_A_Neural_Theory_of_Language",
                "reference_type": "Book",
                "reference_date": "Jan 2006",
                "reference_abstract": null
            },
            {
                "reference_title": "A Recommender System of Medical Reports Leveraging Cognitive Computing and Frame Semantics",
                "reference_link": "publication/326188793_A_Recommender_System_of_Medical_Reports_Leveraging_Cognitive_Computing_and_Frame_Semantics",
                "reference_type": "Chapter",
                "reference_date": "Jan 2019",
                "reference_abstract": "During the last decades, a huge amount of data have been collected in clinical databases in the form of medical reports, laboratory results, treatment plans, etc., representing patients health status. Hence, digital information available for patient-oriented decision making has increased drastically but it is often not mined and analyzed in depth since: (i) medical documents are often unstructured and therefore difficult to analyze automatically, (ii) doctors traditionally rely on their experience to recognize an illness, give a diagnosis, and prescribe medications. However doctors experience can be limited by the cases they are treated so far and medication errors can occur frequently. In addition, it is generally hard and time-consuming inferring information for comparing unstructured data and evaluating similarities between heterogeneous resources. Technologies as Data Mining, Natural Language Processing, and Machine Learning can provide possibilities to explore and exploit potential knowledge from diagnosis history records and help doctors to prescribe medication correctly to decrease medication error effectively. In this paper, we design and implement a medical recommender system that is able to cluster a collection of medical reports on features detected by IBM Watson and Framester, two emerging tools from, respectively, Cognitive Computing and Frame Semantics, and then, giving a medical report from a specific patient as input, to recommend similar other medical reports from patients who had analogues symptoms. Experiments and results have proved the quality of the resulting clustering and recommendations, and the key role that these innovative services can play on the biomedical sector. The proposed system is able to classify new medical cases thus supporting physicians to take more correct and reliable actions about specific diagnosis and cares."
            },
            {
                "reference_title": "Cognitive Computing \u2013 the new Paradigm of the Digital World",
                "reference_link": "publication/319773289_Cognitive_Computing_-_the_new_Paradigm_of_the_Digital_World",
                "reference_type": "Chapter",
                "reference_date": "Jan 2018",
                "reference_abstract": "Cognitive systems like IBM Watson understand at scale, reason with a purpose, learn with each new interaction, and interact naturally with humans, augmenting human intelligence \u2013 but before discerning the significance of cognitive computing, it is firstly important to place it within historical context. To date, there have been three distinct eras of computing: 1) The Tabulating Era which helped revolutionize the way in which we were able manage large volumes of structured data like the census; 2) The Programmatic Era which ushered in the computer revolution allowing us to apply rules and logic revolutionizing the ways in which large and complex transactions were automated (e. g. financial systems, travel systems, etc.); and 3) The Cognitive Era which is extending our ability to work with information of all types, structured and unstructured (e. g. text, pictures, video), with natural conversational interaction, exploration not simple search, decision optimization based on evidence and confidence, all in a time frame that allows us to achieve better results."
            }
        ]
    },
    {
        "title": "RIT Computer Science \u2022 Capstone Report \u2022 20201 Data Recovery with Erasure Coding for Batched Ciphertexts in Fully Homomorphic Encryption",
        "date": "December 2020",
        "doi": "10.13140/RG.2.2.36373.96487",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (17)",
        "abstract": "Storage systems have grown to the point where failures are inevitable. Sometimes part of disk sectors become corrupted, other times entire disks become unavailable for use. Thus overtime, researchers came up with recovery codes that worked with plain text and recovered data upto one disk. Later, two disk recovery codes were also available such as XCode but the problem remained the same that none of these data recovery techniques worked with Homomorphic Encryption. The traditional techniques either worked with plain text or minimal form of encryption therefore we needed an algorithm that could recover the data efficiently and also provide end-to-end security with Fully Homomorphic Encryption. In this project, we present an algorithm, FHE-XCode that is fault tolerant upto two disks and works with Fully Homomorphic Encryption to provide end-to-end security in the cloud with increased data reliability. FHE-XCode has multiple applications and can be used in encrypted distributed data environments such as patient records, defense information, financial records of clients.",
        "reference": [
            {
                "reference_title": "Sector-disk (SD) erasure codes for mixed failure modes in RAID systems",
                "reference_link": "publication/262212386_Sector-disk_SD_erasure_codes_for_mixed_failure_modes_in_RAID_systems",
                "reference_type": "Article",
                "reference_date": "Jan 2014",
                "reference_abstract": "Traditionally, when storage systems employ erasure codes, they are designed to tolerate the failures of entire disks. However, the most common types of failures are latent sector failures, which only affect individual disk sectors, and block failures which arise through wear on SSD\u2019s. This article introduces SD codes, which are designed to tolerate combinations of disk and sector failures. As such, they consume far less storage resources than traditional erasure codes. We specify the codes with enough detail for the storage practitioner to employ them, discuss their practical properties, and detail an open-source implementation."
            },
            {
                "reference_title": "An Efficient XOR-Scheduling Algorithm for Erasure Codes Encoding",
                "reference_link": "publication/224596709_An_Efficient_XOR-Scheduling_Algorithm_for_Erasure_Codes_Encoding",
                "reference_type": "Conference Paper",
                "reference_date": "Aug 2009",
                "reference_abstract": "In large storage systems, it is crucial to protect data from loss due to failures. Erasure codes lay the foundation of this protection, enabling systems to reconstruct lost data when components fail. Erasure codes can however impose significant performance overhead in two core operations: encoding, where coding information is calculated from newly written data, and decoding, where data is reconstructed after failures. This paper focuses on improving the performance of encoding, the more frequent operation. It does so by scheduling the operations of XOR-based erasure codes to optimize their use of cache memory. We call the technique XOR-scheduling and demonstrate how it applies to a wide variety of existing erasure codes. We conduct a performance evaluation of scheduling these codes on a variety of processors and show that XOR-scheduling significantly improves upon the traditional approach. Hence, we believe that XOR-scheduling has great potential to have wide impact in practical storage systems."
            },
            {
                "reference_title": "A Hybrid Approach to Failed Disk Recovery Using RAID-6 Codes: Algorithms and Performance Evaluation",
                "reference_link": "publication/220398211_A_Hybrid_Approach_to_Failed_Disk_Recovery_Using_RAID-6_Codes_Algorithms_and_Performance_Evaluation",
                "reference_type": "Article",
                "reference_date": "Oct 2011",
                "reference_abstract": "The current parallel storage systems use thousands of inexpensive disks to meet the storage requirement of applications. Data redundancy and/or coding are used to enhance data availability, for instance, Row-diagonal parity (RDP) and EVENODD codes, which are widely used in RAID-6 storage systems, provide data availability with up to two disk failures. To reduce the probability of data unavailability, whenever a single disk fails, disk recovery will be carried out. We find that the conventional recovery schemes of RDP and EVENODD codes for a single failed disk only use one parity disk. However, there are two parity disks in the system, and both can be used for single disk failure recovery. In this article, we propose a hybrid recovery approach that uses both parities for single disk failure recovery, and we design efficient recovery schemes for RDP code (RDOR-RDP) and EVENODD code (RDOR-EVENODD). Our recovery scheme has the following attractive properties: (1) \u201cread optimality\u201d in the sense that our scheme issues the smallest number of disk reads to recover a single failed disk and it reduces approximately 1/4 of disk reads compared with conventional schemes; (2) \u201cload balancing property\u201d in that all surviving disks will be subjected to the same (or almost the same) amount of additional workload in rebuilding the failed disk.\nWe carry out performance evaluation to quantify the merits of RDOR-RDP and RDOR-EVENODD on some widely used disks with DiskSim. The offline experimental results show that RDOR-RDP and RDOR-EVENODD outperform the conventional recovery schemes of RDP and EVENODD codes in terms of total recovery time and recovery workload on individual surviving disk. However, the improvements are less than the theoretical value (approximately 25&percnt;), as RDOR-RDP and RDOR-EVENODD change the disk access pattern from purely sequential to a more random one compared with their conventional schemes."
            },
            {
                "reference_title": "Cost Analysis of the X-code Double Parity Array",
                "reference_link": "publication/4284934_Cost_Analysis_of_the_X-code_Double_Parity_Array",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2007",
                "reference_abstract": "The popular RAID5 disk arrays tolerate a single disk failure by using a parity code to reconstruct the contents of a failed disk on demand, but are susceptible to data loss if a second disk fails. The rebuild process which systematically reconstructs the contents of a failed disk on a spare disk may be unsuccessful due to media failures or a second disk failure. Two disk failure tolerant arrays dealing with both problems can be implemented using Reed-Solomon codes or multiple parity schemes such as EVENODD, RDP, X- code, and RM2. All methods incur the minimum level of redundancy in disk accesses and also capacity overhead (except RM2). An appropriate choice of symbol sizes in EVENODD and RDP results in the same access pattern as RAID6 and little disk load imbalance in degraded mode. In this study we consider the load increase and imbalance of the X-code method, since other methods were investigated in previous studies. We derive a general expression for disk loads and present graphs to quantify the load imbalance."
            },
            {
                "reference_title": "Efficient Multi-Key FHE With Short Extended Ciphertexts and Directed Decryption Protocol",
                "reference_link": "publication/332775676_Efficient_Multi-Key_FHE_With_Short_Extended_Ciphertexts_and_Directed_Decryption_Protocol",
                "reference_type": "Article",
                "reference_date": "Apr 2019",
                "reference_abstract": "Multi-Key Full Homomorphic Encryption (MKFHE) can perform arbitrary operations on encrypted data under different public keys (users), and the final ciphertext can be jointly decrypted by all involved users. Therefore, MKFHE has natural advantages and application value in security multi-party computation (MPC). The MKFHE scheme based on Brakerski-Gentry-Vaikuntanathan (BGV) inherits the advantages of BGV FHE scheme in aspects of encrypting a ring element, the ciphertext/plaintext ratio, and supporting the Chinese Remainder Theorem (CRT)-based ciphertexts packing technique. However some weaknesses also exist such as large ciphertexts and keys, and complicated process of generating evaluation keys. In this paper, we present an efficient BGV-type MKFHE scheme. Firstly, we construct a nested ciphertext extension for BGV and separable ciphertext extension for Gentry-Sahai-Waters (GSW), which can reduce the size of the extended ciphertexts about a half. Secondly, we apply the hybrid homomorphic multiplication between RBGV ciphertext and RGSW ciphertext to the generation process of evaluation keys, which can significantly reduce the amount of input/output ciphertexts and improve the efficiency. Finally, we construct a directed decryption protocol which allows the evaluated ciphertext to be decrypted by any target user, thereby enhancing the ability of data owner to control their own plaintext, and abolish the limitation in current MKFHE schemes that the evaluated ciphertext can only be decrypted by users involved in homomorphic evaluation."
            },
            {
                "reference_title": "Erasure coding for distributed storage: an overview",
                "reference_link": "publication/327626264_Erasure_coding_for_distributed_storage_an_overview",
                "reference_type": "Article",
                "reference_date": "Oct 2018",
                "reference_abstract": "In a distributed storage system, code symbols are dispersed across space in nodes or storage units as opposed to time. In settings such as that of a large data center, an important consideration is the efficient repair of a failed node. Efficient repair calls for erasure codes that in the face of node failure, are efficient in terms of minimizing the amount of repair data transferred over the network, the amount of data accessed at a helper node as well as the number of helper nodes contacted. Coding theory has evolved to handle these challenges by introducing two new classes of erasure codes, namely regenerating codes and locally recoverable codes as well as by coming up with novel ways to repair the ubiquitous Reed-Solomon code. This survey provides an overview of the efforts in this direction that have taken place over the past decade."
            },
            {
                "reference_title": "Failures in large scale systems: long-term measurement, analysis, and implications",
                "reference_link": "publication/320938563_Failures_in_large_scale_systems_long-term_measurement_analysis_and_implications",
                "reference_type": "Conference Paper",
                "reference_date": "Nov 2017",
                "reference_abstract": "Resilience is one of the key challenges in maintaining high efficiency of future extreme scale supercomputers. Researchers and system practitioners rely on field-data studies to understand reliability characteristics and plan for future HPC systems. In this work, we compare and contrast the reliability characteristics of multiple large-scale HPC production systems. Our study covers more than one billion compute node hours across five different systems over a period of 8 years. We confirm previous findings which continue to be valid, discover new findings, and discuss their implications."
            },
            {
                "reference_title": "Performance Comparison on the Heterogeneous File System in Cloud Storage Systems",
                "reference_link": "publication/314985060_Performance_Comparison_on_the_Heterogeneous_File_System_in_Cloud_Storage_Systems",
                "reference_type": "Conference Paper",
                "reference_date": "Dec 2016",
                "reference_abstract": null
            },
            {
                "reference_title": "S-Code: Lowest Density MDS Array Codes for RAID-6",
                "reference_link": "publication/282228042_S-Code_Lowest_Density_MDS_Array_Codes_for_RAID-6",
                "reference_type": "Article",
                "reference_date": "Jul 2014",
                "reference_abstract": null
            },
            {
                "reference_title": "A new minimum density RAID-6 code with a word size of eight",
                "reference_link": "publication/4360207_A_new_minimum_density_RAID-6_code_with_a_word_size_of_eight",
                "reference_type": "Conference Paper",
                "reference_date": "Aug 2008",
                "reference_abstract": "RAID-6 storage systems protect k disks of data with twoparity disks so that the system of k+2 disks may toleratethe failure of any two disks. Coding techniques for RAID-6 systems are varied, but an important class of techniquesare those with minimum density, featuring an optimalcombination of encoding, decoding and modificationcomplexity. The word size of a code impacts both how thecode is laid out on each disk's sectors and how large k canbe. Word sizes which are powers of two are especiallyimportant, since they fit precisely into file systemblocks. Minimum density codes exist for many word sizeswith the notable exception of eight. This paper fills thatgap by describing new codes for this important word size.The description includes performancencoding,e properties as well asdetails of the discovery process."
            }
        ]
    },
    {
        "title": "BlocklyScript: Design and Pilot Evaluation of an RPG Platform Game for Cultivating Computational Thinking Skills to Young Students",
        "date": "December 2020",
        "doi": "10.15388/infedu.2020.28",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (31)",
        "abstract": "At 21st century Computational Thinking (CT) is considered a fundamental skill that anyone should possess and develop from a young age. Serious games and more specifically educational games (EGs) are a promising means of introducing algorithmic thinking and programming concepts and engaging students through the process of learning. In this article, a new EG called BlocklyScript is presented. BlocklyScript aims to help students develop their CT by learning basic programming concepts, designing algorithms and correcting mistakes. During the designing phase different EGs were taken under consideration and an EG design framework was followed in order to provide a better user experience. The game was evaluated by 10 experienced computer science educators of primary and secondary schools. The positive results of this pilot evaluation show that BlocklyScript is expected to help students understand the basic concepts of CT. However, the game should be evaluated by more teachers and students in order to provide future researchers with safe results.",
        "reference": [
            {
                "reference_title": "The educational effectiveness of serious games",
                "reference_link": "publication/337367093_The_educational_effectiveness_of_serious_games",
                "reference_type": "Article",
                "reference_date": "Nov 2019",
                "reference_abstract": "A serious game is a computer application that combines a serious intention of a pedagogical, informative and a communicational type with playful springs of the video game (want to win, collaboration, competition, strategy). This two-dimensional approach has transformed the game from a simple means of entertainment to a robust-integrated tool growing in the world of training and learning. Serious games include the engagement of video games with the worlds of educational and computer simulation to integrate the user in a safe and entertaining learning environment. Many techniques have been used to ameliorate computer graphics and technology in the last few years to make this type of game more adaptive to the learning context. In this study, we are interested in presenting the pedagogical contributions of serious games as well as the different possible approaches of their integration in a learning situation and this is based on a variety of case studies and examples of experimentation. We will start with definitions of other video games that have some valuable characteristics of learning in order to contrast and relate them with serious games. Subsequently, we discuss the definition of serious game and the benefits of its use in education. We will, then, examine approaches to integrate serious games into classrooms with an emphasis on the assets and liabilities of each approach. To finish, we conclude on the trends that will follow the serious games technology in the educational field as well as some recommendations to be taken into consideration in order to better exploit these tools in a pedagogical context."
            },
            {
                "reference_title": "The Effect of Unplugged Coding Activities on Computational Thinking Skills of Middle School Students",
                "reference_link": "publication/337013717_The_Effect_of_Unplugged_Coding_Activities_on_Computational_Thinking_Skills_of_Middle_School_Students",
                "reference_type": "Article",
                "reference_date": "Nov 2019",
                "reference_abstract": "The purpose of the study is to examine the effect of unplugged coding activities carried out with middle school students on their computational thinking skills. This study employed nested-mixed design, which is a mixed research method; the data were supported by including the qualitative phase into an experimental study. In this frame, a group of 114 middle school students consisting of 5th graders were given coding training titled \"Kesfet Project - I Discover Coding\" by using unplugged coding content. The Computational Thinking Scale was applied to the students at the beginning and end of the training; the results obtained from the scale were analyzed by means of a paired t test. Finally, it was found out that unplugged coding activities had a positive effect on the improvement of computational thinking skills of the students. An examination of the sub-factors revealed that there is statistically no significant change in the problem solving skill despite the positive impact observed on creativity, algorithmic thinking, collaboration and critical thinking skills. Following the analysis of observation and daily data, the findings obtained revealed that the students usually displayed high levels of motivation and class participation in unplugged coding activities, they had difficulty in concretizing certain concepts as well as subjects requiring mathematical knowledge; various teaching methods and techniques were used in classes; the students liked the activities especially due to their appealing nature and their relation to daily life; however, there were occasional problems with scheduling of activities and teamwork due to over-crowded class size; the students experienced problems in achieving outcomes such as perceiving the relationship between computer science and mathematics and analyzing the given problem, and could have difficulty in associating between computer science and mathematics or between the subjects learned and the computer lesson, and in analyzing a given problem."
            },
            {
                "reference_title": "MEEGA+: A Method for the Evaluation of Educational Games for Computing Education",
                "reference_link": "publication/326722665_MEEGA_A_Method_for_the_Evaluation_of_Educational_Games_for_Computing_Education",
                "reference_type": "Technical Report",
                "reference_date": "Jul 2018",
                "reference_abstract": "Educational games are assumed to be an effective and efficient instructional strategy for computing education. However, it is essential to systematically evaluate such games in order to obtain sound evidence on their quality. A prominent evaluation model is MEEGA (Model for the Evaluation of Educational Games) providing a support to evaluate games in terms of motivation, user experience and learning. However, analyses of the initial version of the MEEGA model have identified limitations regarding its validity and reliability. Furthermore, a more comprehensive support is required in order to guide instructors and/or researchers in how to conduct game evaluations in order to obtain reliable and valid results. Thus, the ob-jective of this technical report is to present the MEEGA+ method, which aims to evaluate the quality of educational games used as instructional strategy for computing education, improv-ing the initial version of the MEEGA model. The MEEGA+ method has been developed tak-ing also into consideration results from a systematic literature review on the evaluation of ed-ucational games. The MEEGA+ method is composed by the quality model, defining quality factors to be evaluated through a standardized measurement instrument, and defines the process to evaluate the quality of educational games used for computing education using the MEEGA+ model. The MEEGA+ method provides game creators, instructors and researchers with a systematic support in order to evaluate the quality of educational games and, thus, contribute to their improvement and effective and efficient adoption in practice."
            },
            {
                "reference_title": "A pilot study on the effectiveness and acceptance of an educational game for teaching programming concepts to primary school students",
                "reference_link": "publication/323657622_A_pilot_study_on_the_effectiveness_and_acceptance_of_an_educational_game_for_teaching_programming_concepts_to_primary_school_students",
                "reference_type": "Article",
                "reference_date": "Sep 2018",
                "reference_abstract": "Educational games are increasingly used in informal and formal educational settings for promoting active learning and gaining students\u2019 interest in cognitively demanding subjects, such as programming. However, empirical studies that investigate the true impact of educational games on teaching and learning programming, especially to small aged students, are limited. This article presents the results of a pilot study that utilized the educational game Run Marco for teaching basic programming concepts to primary school students. Students\u2019 performance was studied through specially designed worksheets, while their acceptance of the intervention was evaluated through a questionnaire that was based on the principles of the Technology Acceptance Model (TAM). The results of the pilot study showed that the educational game supported students in comprehending basic programming concepts, while the results regarding the acceptance of its usage in the learning process were quite positive. However, the game did not succeed in raising students\u2019 interest as expected and further research is necessary in order to study the reasons for this fact and make informed choices on designing and utilizing such games."
            },
            {
                "reference_title": "Computer Science Teachers' Perceptions, Beliefs and Attitudes on Computational Thinking in Greece",
                "reference_link": "publication/344349736_Computer_Science_Teachers'_Perceptions_Beliefs_and_Attitudes_on_Computational_Thinking_in_Greece",
                "reference_type": "Article",
                "reference_date": "Nov 2019",
                "reference_abstract": "The role of teachers is very important for the educational utilization of Computational Thinking (CT) and its integration in education. As with any innovation, CTs' successful integration considerably depends on the perceptions, beliefs and attitudes of the teachers who will be asked to implement it. The study of these characteristics, concerning Computer Science (CS) teachers in Greece, was the objective of a survey research, theoretically supported by the Theory of Reasoned Action (TRA) and the Technology Acceptance Model (TAM). Findings reveal intense interest of participants on CT and their willingness to participate in professional development programs. Participants also reveal misconceptions of CT and negative attitudes toward its integration in education, that require further study and discussion. The researchers propose directions for the design and implementation of appropriate teachers training programs, while the findings can be exploited to support any effort of integrating CT in education."
            },
            {
                "reference_title": "A Review of Educational Games for Teaching Programming to Primary School Students",
                "reference_link": "publication/342397334_A_Review_of_Educational_Games_for_Teaching_Programming_to_Primary_School_Students",
                "reference_type": "Chapter",
                "reference_date": "Jun 2020",
                "reference_abstract": "In recent years several educational games for learning programming have been developed with promising results. The main purpose of this chapter is to present twenty two educational games or platforms that aim to cultivate Computational Thinking through teaching computer programming concepts to primary school students. A short description of each game followed by a comparative analysis of both their game mechanics and their educational aspects is presented. Additionally, less typical functionalities such as online classrooms, the support for learning analytics and the creation of new levels are analyzed. This chapter could be useful for game designers and IT teachers who would like to use a game-based approach in the teaching process."
            },
            {
                "reference_title": "Current Trends in On-line Games for Teaching Programming Concepts to Primary School Students",
                "reference_link": "publication/333425288_Current_Trends_in_On-line_Games_for_Teaching_Programming_Concepts_to_Primary_School_Students",
                "reference_type": "Chapter",
                "reference_date": "May 2019",
                "reference_abstract": "In this paper current trends in online educational games for teaching programming concepts, or else computational thinking, to primary school students are analyzed. Specifically, several online games such as CodeMonkey, Getcoding, Kodable, Lightbot, Program Your Robot, Rapid Router and Run Marco are briefly presented. This is followed by a comparative analysis of important features both regarding the game mechanics and their educational aspects. Specifically, the following features are analyzed: the genre and the scenario of the game, game mechanics used for entertainment purposes, programming concepts/constructs supported, the support provided by the editor for implementing programs, testing and debugging facilities. Moreover, enhanced features, such as the support for creating on-line classrooms, monitoring students\u2019 progress and capabilities of creating new levels in the game are investigated. Conclusions and open-issues for further research in the field are presented."
            },
            {
                "reference_title": "Integrating Computational Thinking with a Music Education Context",
                "reference_link": "publication/328741184_Integrating_Computational_Thinking_with_a_Music_Education_Context",
                "reference_type": "Article",
                "reference_date": "Oct 2018",
                "reference_abstract": "Computational thinking is becoming common in K-12 curricula, and at the same time there is interest in how STEM subjects can be integrated with the Arts (referred to as STEAM). There are some obvious connections between music and computation, but the idea of engaging with genuine computational thinking while also having authentic music learning experiences for students provides new opportunities. In this paper we consider ways to explore computational thinking ideas such as decomposition, patterns, abstraction and algorithms in a meaningful way while also exploring key concepts that a music educator would expect to work with. We review some existing ideas for doing this, and also provide novel approaches that connect computational thinking and music. This is done through a series of vignettes that describe creative ways to connect the two subjects using approaches that have been used successfully with school students. The first approach is based on the use of comparisons in sorting, which can be used to have students physically compare musical elements such as note pitches. The second uses simple programming on physical devices to represent music notation. Further examples include exploring binary representations using sound, writing programs for musical scales, understanding musical phrases in the context of programming, and using programming for music composition. Integrating the opportunity to learn about computational thinking and music at the same time has the benefit that some efficiency can be gained in teaching, but more importantly, students are able to experience the relevance of these two subjects to each other, when they might otherwise pigeonhole them into separate areas of their lives."
            },
            {
                "reference_title": "MiniColon; Teaching Kids Computational Thinking Using an Interactive Serious Game: 4th Joint International Conference, JCSG 2018, Darmstadt, Germany, November 7-8, 2018, Proceedings",
                "reference_link": "publication/328312450_MiniColon_Teaching_Kids_Computational_Thinking_Using_an_Interactive_Serious_Game_4th_Joint_International_Conference_JCSG_2018_Darmstadt_Germany_November_7-8_2018_Proceedings",
                "reference_type": "Chapter",
                "reference_date": "Jan 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Formative evaluation of an adaptive game for engaging learners of programming concepts in K-12",
                "reference_link": "publication/326005808_Formative_evaluation_of_an_adaptive_game_for_engaging_learners_of_programming_concepts_in_K-12",
                "reference_type": "Article",
                "reference_date": "Jun 2018",
                "reference_abstract": "As the global demand for programmers is soaring, several countries have integrated programming into their K-12 curricula. Finding effective ways to engage children in programming education is an important objective. One effective method for this can be presenting learning materials via games, which are known to increase engagement and motivation. Current programming education games often focus on a single genre and offer one-size-fits-all experience to heterogeneous learners. In this study, we presented Minerva, a multi-genre (adventure, action, puzzle) game to engage elementary school students in learning programming concepts. The game content is adapted to play and learning styles of the player to personalize the gameplay. We conducted a formative mixed-method evaluation of Minerva with 32 Korean 6th grade students who played the game and compared their learning outcomes with 32 6th grade students who studied the same concepts using handouts. The results indicated that, in terms of retention, learning was equally effective in both groups. Furthermore, the game was shown to facilitate engagement among the students. These results, together with uncovered issues, will guide Minerva\u2019s further development."
            }
        ]
    },
    {
        "title": "Computational modeling of human reasoning processes for interpretable visual knowledge: a case study with radiographers",
        "date": "December 2020",
        "doi": "10.1038/s41598-020-77550-9",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (46)",
        "abstract": "Visual reasoning is critical in many complex visual tasks in medicine such as radiology or pathology. It is challenging to explicitly explain reasoning processes due to the dynamic nature of real-time human cognition. A deeper understanding of such reasoning processes is necessary for improving diagnostic accuracy and computational tools. Most computational analysis methods for visual attention utilize black-box algorithms which lack explainability and are therefore limited in understanding the visual reasoning processes. In this paper, we propose a computational method to quantify and dissect visual reasoning. The method characterizes spatial and temporal features and identifies common and contrast visual reasoning patterns to extract significant gaze activities. The visual reasoning patterns are explainable and can be compared among different groups to discover strategy differences. Experiments with radiographers of varied levels of expertise on 10 levels of visual tasks were conducted. Our empirical observations show that the method can capture the temporal and spatial features of human visual attention and distinguish expertise level. The extracted patterns are further examined and interpreted to showcase key differences between expertise levels in the visual reasoning processes. By revealing task-related reasoning processes, this method demonstrates potential for explaining human visual understanding.",
        "reference": [
            {
                "reference_title": "A Review of Perceptual Expertise in Radiology-How it develops, How we can test it, and Why humans still matter in the era of Artificial Intelligence",
                "reference_link": "publication/337914502_A_Review_of_Perceptual_Expertise_in_Radiology-How_it_develops_How_we_can_test_it_and_Why_humans_still_matter_in_the_era_of_Artificial_Intelligence",
                "reference_type": "Article",
                "reference_date": "Jan 2020",
                "reference_abstract": "As the first step in image interpretation is detection, an error in perception can prematurely end the diagnostic process leading to missed diagnoses. Because perceptual errors of this sort\u2013\u201cfailure to detect\u201d\u2013are the most common interpretive error (and cause of litigation) in radiology, understanding the nature of perceptual expertise is essential in decreasing radiology's long-standing error rates. In this article, we review what constitutes a perceptual error, the existing models of radiologic image perception, the development of perceptual expertise and how it can be tested, perceptual learning methods in training radiologists, and why understanding perceptual expertise is still relevant in the era of artificial intelligence. Adding targeted interventions, such as perceptual learning, to existing teaching practices, has the potential to enhance expertise and reduce medical error."
            },
            {
                "reference_title": "Analysis of Perceptual Expertise in Radiology \u2013 Current Knowledge and a New Perspective",
                "reference_link": "publication/334016972_Analysis_of_Perceptual_Expertise_in_Radiology_-_Current_Knowledge_and_a_New_Perspective",
                "reference_type": "Article",
                "reference_date": "Jun 2019",
                "reference_abstract": "Radiologists rely principally on visual inspection to detect, describe, and classify findings in medical images. As most interpretive errors in radiology are perceptual in nature, understanding the path to radiologic expertise during image analysis is essential to educate future generations of radiologists. We review the perceptual tasks and challenges in radiologic diagnosis, discuss models of radiologic image perception, consider the application of perceptual learning methods in medical training, and suggest a new approach to understanding perceptional expertise. Specific principled enhancements to educational practices in radiology promise to deepen perceptual expertise among radiologists with the goal of improving training and reducing medical error."
            },
            {
                "reference_title": "Eye-tracking for assessing medical image interpretation: A pilot feasibility study comparing novice vs expert cardiologists",
                "reference_link": "publication/332359016_Eye-tracking_for_assessing_medical_image_interpretation_A_pilot_feasibility_study_comparing_novice_vs_expert_cardiologists",
                "reference_type": "Article",
                "reference_date": "Apr 2019",
                "reference_abstract": "Introduction\nAs specialized medical professionals such as radiologists, pathologists, and cardiologists gain education and experience, their diagnostic efficiency and accuracy change, and they show altered eye movement patterns during medical image interpretation. Existing research in this area is limited to interpretation of static medical images, such as digitized whole slide biopsies, making it difficult to understand how expertise development might manifest during dynamic image interpretation, such as with angiograms or volumetric scans.\n\nMethods\nA two-group (novice, expert) comparative pilot study examined the feasibility and utility of tracking and interpreting eye movement patterns while cardiologists viewed video-based coronary angiograms. A non-invasive eye tracking system recorded cardiologists\u2019 (n = 8) visual behaviour while they viewed and diagnosed a series of eight angiogram videos. Analyses assessed frame-by-frame video navigation behaviour, eye fixation behaviour, and resulting diagnostic decision making.\n\nResults\nRelative to novices, expert cardiologists demonstrated shorter and less variable video review times, fewer eye fixations and saccadic eye movements, and less time spent paused on individual video frames. Novices showed repeated eye fixations on critical image frames and regions, though these were not predictive of accurate diagnostic decisions.\n\nDiscussion\nThese preliminary results demonstrate interpretive decision errors among novices, suggesting they identify and process critical diagnostic features, but sometimes fail to accurately interpret those features. Results also showcase the feasibility of tracking and understanding eye movements during video-based coronary angiogram interpretation and suggest that eye tracking may be valuable for informing assessments of competency progression during medical education and training."
            },
            {
                "reference_title": "Emotional Attention: A Study of Image Sentiment and Visual Attention",
                "reference_link": "publication/329744033_Emotional_Attention_A_Study_of_Image_Sentiment_and_Visual_Attention",
                "reference_type": "Conference Paper",
                "reference_date": "Jun 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "State of the Art: Eye-Tracking Studies in Medical Imaging",
                "reference_link": "publication/326046602_State_of_the_Art_Eye-Tracking_Studies_in_Medical_Imaging",
                "reference_type": "Article",
                "reference_date": "Jun 2018",
                "reference_abstract": "Eye-tracking \u2013 the process of measuring where people look in a visual field \u2013 has been widely used to study how humans process visual information. In medical imaging, eye-tracking has become a popular technique in many applications to reveal how visual search and recognition tasks are performed, providing information that can improve human performance. In this paper, we present a comprehensive review of eye-tracking studies conducted with medical images and videos for diverse research purposes, including identification of degree of expertise, development of training, and understanding and modelling of visual search patterns. In addition, we present our recent eye-tracking study that involves a large number of screening mammograms viewed by experienced breast radiologists. Based on the eye-tracking data, we evaluate the plausibility of predicting visual attention by computational models."
            },
            {
                "reference_title": "Artificial Intelligence for Diabetes Management and Decision Support: Literature Review",
                "reference_link": "publication/325162420_Artificial_Intelligence_for_Diabetes_Management_and_Decision_Support_Literature_Review",
                "reference_type": "Article",
                "reference_date": "May 2018",
                "reference_abstract": "Background: \nArtificial intelligence methods in combination with the latest technologies, including medical devices, mobile computing, and sensor technologies, have the potential to enable the creation and delivery of better management services to deal with chronic diseases. One of the most lethal and prevalent chronic diseases is diabetes mellitus, which is characterized by dysfunction of glucose homeostasis.\n\nObjective: \nThe objective of this paper is to review recent efforts to use artificial intelligence techniques to assist in the management of diabetes, along with the associated challenges.\n\nMethods: \nA review of the literature was conducted using PubMed and related bibliographic resources. Analyses of the literature from 2010 to 2018 yielded 1849 pertinent articles, of which we selected 141 for detailed review.\n\nResults: \nWe propose a functional taxonomy for diabetes management and artificial intelligence. Additionally, a detailed analysis of each subject category was performed using related key outcomes. This approach revealed that the experiments and studies reviewed yielded encouraging results.\n\nConclusions: \nWe obtained evidence of an acceleration of research activity aimed at developing artificial intelligence-powered tools for prediction and prevention of complications associated with diabetes. Our results indicate that artificial intelligence methods are being progressively established as suitable for use in clinical daily practice, as well as for the self-management of diabetes. Consequently, these methods provide powerful tools for improving patients' quality of life."
            },
            {
                "reference_title": "Reasons For Physicians Not Adopting Clinical Decision Support Systems: Critical Analysis",
                "reference_link": "publication/324619139_Reasons_For_Physicians_Not_Adopting_Clinical_Decision_Support_Systems_Critical_Analysis",
                "reference_type": "Article",
                "reference_date": "Apr 2018",
                "reference_abstract": "Background: \nClinical decision support systems (CDSSs) are an integral component of today's health information technologies. They assist with interpretation, diagnosis, and treatment. A CDSS can be embedded throughout the patient safety continuum providing reminders, recommendations, and alerts to health care providers. Although CDSSs have been shown to reduce medical errors and improve patient outcomes, they have fallen short of their full potential. User acceptance has been identified as one of the potential reasons for this shortfall.\n\nObjective: \nThe purpose of this paper was to conduct a critical review and task analysis of CDSS research and to develop a new framework for CDSS design in order to achieve user acceptance.\n\nMethods: \nA critical review of CDSS papers was conducted with a focus on user acceptance. To gain a greater understanding of the problems associated with CDSS acceptance, we conducted a task analysis to identify and describe the goals, user input, system output, knowledge requirements, and constraints from two different perspectives: the machine (ie, the CDSS engine) and the user (ie, the physician).\n\nResults: \nFavorability of CDSSs was based on user acceptance of clinical guidelines, reminders, alerts, and diagnostic suggestions. We propose two models: (1) the user acceptance and system adaptation design model, which includes optimizing CDSS design based on user needs/expectations, and (2) the input-process-output-engagemodel, which reveals to users the processes that govern CDSS outputs.\n\nConclusions: \nThis research demonstrates that the incorporation of the proposed models will improve user acceptance to support the beneficial effects of CDSSs adoption. Ultimately, if a user does not accept technology, this not only poses a threat to the use of the technology but can also pose a threat to the health and well-being of patients."
            },
            {
                "reference_title": "Where\u2019s WALDO: a potential tool for training radiology residents?",
                "reference_link": "publication/339963642_Where's_WALDO_a_potential_tool_for_training_radiology_residents",
                "reference_type": "Conference Paper",
                "reference_date": "Mar 2020",
                "reference_abstract": null
            },
            {
                "reference_title": "Emotion-Aware Human Attention Prediction",
                "reference_link": "publication/338506846_Emotion-Aware_Human_Attention_Prediction",
                "reference_type": "Conference Paper",
                "reference_date": "Jun 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Common resident errors when interpreting CT of the abdomen and pelvis: A review of types, pitfalls, and strategies for improvement",
                "reference_link": "publication/322289444_Common_resident_errors_when_interpreting_CT_of_the_abdomen_and_pelvis_A_review_of_types_pitfalls_and_strategies_for_improvement",
                "reference_type": "Article",
                "reference_date": "Jan 2018",
                "reference_abstract": "Objective\nThe purpose of this study was to identify common errors that radiology residents make when interpreting abdominopelvic (AP) CT while on call, to review the typical imaging findings of these cases, and to discuss strategies for improvement.\n\nMaterials and Methods\nAP (or chest, abdomen, pelvis) CTs from 518 weekend senior call shifts (R3 or R4) were retrospectively reviewed. Discrepancies between preliminary and final reports were identified and then rated by whether the miss could impact short-term management. The imaging findings from the cases were reviewed.\n\nResults\n4695 CTs were reviewed, revealing a total of 145 discrepancies that could affect short-term clinical management (miss rate 3.1%). The most common misses were related to blood clots (13.8%), colitis (8.3%), misplaced lines/tubes (6.9%), or pyelonephritis (5.5%). Common pitfalls and strategies from improved detection are discussed using image examples.\n\nConclusions\nThrough increased attention to the vasculature, colon, devices, and kidneys, trainees may improve their discrepancy rates and improve on-call reporting."
            }
        ]
    },
    {
        "title": "THE EFFECTIVENESS OF TIMELINE INTERACTIVE WEB SERVICE IN DELIVERING LESSONS ON \u201cHISTORY OF PERSONAL COMPUTERS\u201d",
        "date": "December 2020",
        "doi": "10.15863/TAS.2020.12.92.20",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (9)",
        "abstract": "The article provides information on the role of timeline interactive web service in the educational process, the advantages of visualization of computer science topics using timeline and their application in the educational process.",
        "reference": [
            {
                "reference_title": "THE IMPORTANCE OF THE VIRTUAL MUSEUMS IN THE EDUCATIONAL PROCESS",
                "reference_link": "publication/339644328_THE_IMPORTANCE_OF_THE_VIRTUAL_MUSEUMS_IN_THE_EDUCATIONAL_PROCESS",
                "reference_type": "Article",
                "reference_date": "Feb 2020",
                "reference_abstract": "The article discusses the place of the virtual museums in the educational process, the ideas about museum pedagogy, the advantages of the virtual museums and the results of the explorations on using them in the teaching process."
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2014",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": null,
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2013",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2016",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2020",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2020",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": null,
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": null,
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "\u00c9ducation et num\u00e9rique, D\u00e9fis et enjeux",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (16)",
        "abstract": null,
        "reference": [
            {
                "reference_title": "Systematic review of young children's writing on screen: what do we know and what do we need to know: Systematic review of early writing on screen",
                "reference_link": "publication/330516813_Systematic_review_of_young_children's_writing_on_screen_what_do_we_know_and_what_do_we_need_to_know_Systematic_review_of_early_writing_on_screen",
                "reference_type": "Article",
                "reference_date": "Jan 2019",
                "reference_abstract": "Writing is part and parcel of children's active meaning\u2010making on and with screens, but it has been relatively neglected in the literature focused on children's digital literacies. This study synthesises existing empirical evidence focused on young children's (aged between 2 and 8 years) writing on screen and identifies the relationships between dominant themes in published literature and contemporary theories of children's technology use. A systematic literature review that included studies from diverse disciplines yielded 21 papers. Constant comparative analysis generated five themes that indicate four key directions for future research. We call attention to researchers' theoretical framing to supplement mono\u2010disciplinary approaches and single levels of analysis. We suggest that future research should provide greater specification of the purpose of children's writing on screen and the different types of tools and applications supporting the activity. We also highlight the need for interdisciplinary approaches that would capture the composing stages involved in the writing process with and around screens. Finally, we point out possible age\u2010related differences in documenting and reporting the composing process in classrooms. Overall, limitations in the current evidence base highlight the need for research conducted from a critical perspective and focused more directly on multimodality."
            },
            {
                "reference_title": "Identifying Brain Characteristics of Bright Students",
                "reference_link": "publication/326873413_Identifying_Brain_Characteristics_of_Bright_Students",
                "reference_type": "Article",
                "reference_date": "Jan 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Second Handbook of Information Technology in Primary and Secondary Education",
                "reference_link": "publication/345602388_Second_Handbook_of_Information_Technology_in_Primary_and_Secondary_Education",
                "reference_type": "Book",
                "reference_date": "Jan 2018",
                "reference_abstract": "In this second edition the editors continue their efforts to synthesize research and practice and project future directions in the field of information and communication technology. \n\nThe proliferation of mobile devices and applications have had major implications on how the nature of teaching and learning should be conceptualised, and what pedagogical practices should be used to support bridging formal and informal learning. The advent of social media also highlights the importance of gaining a deeper understanding of social learning theories and computer-supported collaborative learning theories and practices. The advancement of new technologies to support easy accessibility of educational resources such as OER and to a lesser extent MOOCs have led to the debate on how assessment should be conducted and how technologies could be used to support it. \n\nThe demand of the knowledge society requires that researchers, policy makers, and educational practitioners be conversant with new research findings in order to understand the impact of ICT in teaching and learning, and how to support learners to use new technologies and applications creatively and effectively. New research paradigms have emerged to meet these challenges."
            },
            {
                "reference_title": "Cultivating Communities of Practice: A Guide to Managing Knowledge",
                "reference_link": "publication/44837609_Cultivating_Communities_of_Practice_A_Guide_to_Managing_Knowledge",
                "reference_type": "Book",
                "reference_date": "Jan 2002",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2015",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2010",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2014",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2019",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2013",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2012",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Is GPA Enough? A Platform for Promoting Computer Science Undergraduates\u2019 Pursuit of Career Related Extracurricular Activities",
        "date": "December 2020",
        "doi": "10.46328/ijtes.146",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (45)",
        "abstract": "Despite the perceived value of extracurricular experience, higher education relies heavily on grade point average (GPA) as a measure of undergraduates\u2019 academic success. When used as a singular standard assessment, GPA, which is based on student progress and completion of coursework, may inadvertently steer undergraduates away from valuable out-of-class experiences that might enhance their employability after graduation. With this premise in mind, the current study proposes to supplement GPA scores in an undergraduate Computer Science program with a wholistic assessment known as an Innovation, Competency, and Experience (ICE) score. The ICE score is a point system for documenting and rewarding students\u2019 extracurricular activities, in addition to their GPA scores. We designed and developed a web-based technology platform called RadGrad to implement ICE scores and promote student engagement and participation in extracurricular activities. Preliminary data shows that over 36% of students started to use RadGrad, even though its use was completely voluntary. More than half of those students planned and participated in various extracurricular activities, ultimately earning ICE points. Importantly, the ICE score deployed through RadGrad demonstrates the potential value of a supplemental assessment to GPA, which can promote extracurricular experiences relevant to students\u2019 future careers beyond coursework.",
        "reference": [
            {
                "reference_title": "Is it all a game? Understanding the principles of gamification",
                "reference_link": "publication/275059704_Is_it_all_a_game_Understanding_the_principles_of_gamification",
                "reference_type": "Article",
                "reference_date": "Apr 2015",
                "reference_abstract": "There is growing interest in how gamification\u2014defined as the application of game design principles in non-gaming contexts\u2014can be used in business. However, academic research and management practice have paid little attention to the challenges of how best to design, implement, manage, and optimize gamification strategies. To advance understanding of gamification, this article defines what it is and explains how it prompts managers to think about business practice in new and innovative ways. Drawing upon the game design literature, we present a framework of three gamification principles\u2014mechanics, dynamics, and emotions (MDE)\u2014to explain how gamified experiences can be created. We then provide an extended illustration of gamification and conclude with ideas for future research and application opportunities."
            },
            {
                "reference_title": "Using Undergraduate Grade Point Average as a Selection Tool: A Synthesis of the Literature.",
                "reference_link": "publication/271722877_Using_Undergraduate_Grade_Point_Average_as_a_Selection_Tool_A_Synthesis_of_the_Literature",
                "reference_type": "Article",
                "reference_date": "Feb 2015",
                "reference_abstract": "As educational attainment becomes increasingly more important in employment settings, undergraduate grade point average (GPA) will persist in being an attractive selection tool. It is important that recruitment professionals understand the implications of the use of this measure. Thus, this article aims to unify the GPA literature in order to offer guidelines for practice based on current research. Common themes within the literature are reviewed to address topics such as what GPA reflects as well as the reliability and accuracy of the measure. Employment professionals are urged to consider how using GPA impacts fairness and accuracy in selection. As such, the recommendations are designed to help practitioners understand the limitations associated with the use of GPA in order to maximize both fairness and accuracy."
            },
            {
                "reference_title": "An Innovative Near-Peer Mentoring Model for Undergraduate and Secondary Students: STEM Focus",
                "reference_link": "publication/266749993_An_Innovative_Near-Peer_Mentoring_Model_for_Undergraduate_and_Secondary_Students_STEM_Focus",
                "reference_type": "Article",
                "reference_date": "Nov 2014",
                "reference_abstract": "This study examined a novel mentoring model, near-peer mentorship, that supports the development of mentee and mentor, incorporates established principles of mentoring, and offers unique opportunities to integrate research and teaching in a science, technology, engineering, and mathematics (STEM) internship. Using qualitative methods, this model was examined from the perspectives of near-peer mentors and student mentees during a science education internship at the Walter Reed Army Institute of Research. Results revealed that this mentorship model contributed to personal, educational, and professional growth for near-peer mentors and increased the interest and engagement of students studying STEM. We discuss implications, limitations, and future directions."
            },
            {
                "reference_title": "Supporting Personalized Learning through Individualized Learning Plans",
                "reference_link": "publication/345474748_Supporting_Personalized_Learning_through_Individualized_Learning_Plans",
                "reference_type": "Article",
                "reference_date": "Mar 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "An Introduction to Gamification: Adding Game Elements for Engagement",
                "reference_link": "publication/283262551_An_Introduction_to_Gamification_Adding_Game_Elements_for_Engagement",
                "reference_type": "Article",
                "reference_date": "Oct 2015",
                "reference_abstract": "Free full-text here: http://www.tandfonline.com/eprint/pC9pfGQZuAaBQf63hKu4/full\n\nGamification is defined as the use of game design elements in a nongame context. While gamification is not a new concept, new dynamics are unfolding that may cause more businesses, educators, and librarians to consider the use of game-like elements into future endeavors. In addition to more generation Y or millennials entering higher education and the workplace, there has been a significant acceptance of routinely using smartphones and playing games. This column will explain what gamification is, provide an overview of the benefits and concerns surrounding gamification, and describe how and where it is currently being used."
            },
            {
                "reference_title": "The effect of college activities and grades on job placement potential.",
                "reference_link": "publication/281102066_The_effect_of_college_activities_and_grades_on_job_placement_potential",
                "reference_type": "Article",
                "reference_date": "Jan 1994",
                "reference_abstract": null
            },
            {
                "reference_title": "Student Assessment Center Performance in the Prediction of Early Career Success",
                "reference_link": "publication/278003614_Student_Assessment_Center_Performance_in_the_Prediction_of_Early_Career_Success",
                "reference_type": "Article",
                "reference_date": "Jun 2004",
                "reference_abstract": "Our primary purpose in this study was to evaluate the criterion-related validity of an academic-based assessment center (AC), specifically in relation to early career progress. A total of 66 undergraduate business students participated in a day-long AC. The AC included a combination of in-basket, leaderless group discussion, case, interview simulation, and oral presentation exercises, as well as paper-and-pencil measures including Big Five personality variables. Assessment was performed using three trained assessors for each assessee. These assessees also participated in a follow-up survey, typically between 2 and 31/2 years following AC participation. We designed the survey to measure early career progress in the form of job satisfaction, number of promotions, and current salary. Analyses revealed that student grade point average (GPA) was generally not a good predictor of early career progress, with the exception of salary. In contrast, AC performance was most consistently predictive of both intrinsic and extrinsic aspects of career success, even after controlling for personality and GPA. Results are discussed in terms of their relevance to academically based assessment centers and the career success literature. Implications are drawn regarding the challenges and opportunities associated with the use of AC methodology in business schools."
            },
            {
                "reference_title": "Graduate Employability: A Review of Conceptual and Empirical Themes",
                "reference_link": "publication/263327283_Graduate_Employability_A_Review_of_Conceptual_and_Empirical_Themes",
                "reference_type": "Article",
                "reference_date": "Dec 2012",
                "reference_abstract": "The purpose of this paper is to provide an overview of some of the dominant empirical and conceptual themes in the area of graduate employment and employability over the past decade. The paper considers the wider context of higher education (HE) and labour market change, and the policy thinking towards graduate employability. It draws upon various studies to highlight the different labour market perceptions, experiences and outcomes of graduates in the United Kingdom and other national contexts. It further draws upon research that has explored the ways in which students and graduates construct their employability and begin to manage the transition from HE to work. The paper explores some of the conceptual notions that have informed understandings of graduate employability, and argues for a broader understanding of employability than that offered by policymakers."
            },
            {
                "reference_title": "The Nature and Use of Individualized Learning Plans as a Promising Career Intervention Strategy",
                "reference_link": "publication/258144291_The_Nature_and_Use_of_Individualized_Learning_Plans_as_a_Promising_Career_Intervention_Strategy",
                "reference_type": "Article",
                "reference_date": "Dec 2012",
                "reference_abstract": "Individualized learning plans (ILPs) are being implemented in high schools throughout the United States as strategic planning tools that help students align course plans with career aspirations and often include the development of postsecondary plans. Initial indications are that ILPs may be an important method for helping students achieve both college and career readiness. Parents, teachers, and students indicate that ILPs result in students selecting more rigorous courses, better teacher\u2013student relationships, and positive parent\u2013school relations. This article describes the emergence and nature of ILPs, promising practice strategies as well as challenges associated with gaining whole school buy-in, and the potential for career and vocational research."
            },
            {
                "reference_title": "A Meta-Analysis of the Predictive Validity of the Graduate Management Admission Test (GMAT) and Undergraduate Grade Point Average (UGPA) for Graduate Student Academic Performance",
                "reference_link": "publication/242288886_A_Meta-Analysis_of_the_Predictive_Validity_of_the_Graduate_Management_Admission_Test_GMAT_and_Undergraduate_Grade_Point_Average_UGPA_for_Graduate_Student_Academic_Performance",
                "reference_type": "Article",
                "reference_date": "Mar 2007",
                "reference_abstract": "Considerable debate both within and outside of academic circles surrounds the validity of standardized tests for predicting student performance in graduate business schools. This meta-analysis aggregates the existing literature on the validity of the two most heavily used predictors in business school admissions: the GMAT and undergraduate grade point average. Results based on over 402 independent samples across 64,583 students indicate that the GMAT is a superior predictor to UGPA and that the two combined yield a high level of validity for predicting student performance."
            }
        ]
    },
    {
        "title": "Inaugural Address at CONIAPS XXVI_Advertisement",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": null,
        "abstract": "Russian Prof. Vladimir V. Egorov to inaugurate 26th International Webinar on Advances in Physics, Chemistry, Mathematics, Computer Sciences & Biological Sciences (CONIAPS XXVI) hosted by MG University.",
        "reference": []
    },
    {
        "title": "M\u00e9thodes informatiques pour l\u2019\u00e9tude des gravures rupestres : les exemples du Valcamonica (Italie) et du mont Bego (France)",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (23)",
        "abstract": "Le Valcamonica (100 000 gravures estim\u00e9es) et le mont Bego (36 000 gravures) sont les deux principales concentrations d\u2019art rupestre pour la M\u00e9diterran\u00e9e occidentale au cours de la Pr\u00e9histoire r\u00e9cente. Bien que des recherches y aient \u00e9t\u00e9 men\u00e9es depuis de nombreuses ann\u00e9es : au Valcamonica par E. Anati (1960, 2008) et au mont Bego par H. de Lumley et al. (1976, 2011), les interpr\u00e9tations propos\u00e9es restent peu convaincantes et largement d\u00e9battues. Les mod\u00e8les th\u00e9oriques et les outils pratiques employ\u00e9s par ces auteurs sont par ailleurs critiqu\u00e9s. Ainsi, Richard Bradley qualifiait la recherche au Valcamonica et au mont Bego de : \u00ab curiously introverted kind of research, which seems quite out of contact with the main currents in modern archaeology \u00bb (Bradley, 1997 p. 8). Deux th\u00e8ses doctorales ont r\u00e9cemment introduit l\u2019analyse syst\u00e9matique et statistique de bases de donn\u00e9es spatiales : pour les gravures du Valcamonica (Alexander, 2011) et du mont Bego (Huet, 2012). Les m\u00e9thodes informatiques (SIG, tests d\u2019hypoth\u00e8ses, analyses multifactorielles, etc.) permettent : (1) d\u2019\u00e9tudier conjointement les distributions iconographiques et spatiales des gravures, (2) d\u2019identifier parmi celles-ci des sous-ensembles consistants, (3) de construire un cadre th\u00e9orique explicite pour envisager les significations symboliques. Avec ces m\u00e9thodes, la \u00ab d\u00e9couverte \u00bb arch\u00e9ologique n\u2019est plus guid\u00e9e par les gravures exceptionnelles (l\u2019exception), mais par les tendances g\u00e9n\u00e9rales au sein du corpus. Ces tendances correspondent plus certainement aux ph\u00e9nom\u00e8nes que l\u2019arch\u00e9ologie entend expliquer lorsqu\u2019elle se propose d\u2019\u00e9tudier ces sites. Nous pr\u00e9senterons l\u2019historique de la recherche sur les gravures du Valcamonica et du mont Bego, et le renouvellement m\u00e9thodologique que repr\u00e9sente l\u2019introduction des m\u00e9thodes informatiques pour leur \u00e9tude. A travers quelques exemples, nous montrerons comment ces m\u00e9thodes peuvent fournir des indices \u2014 plus objectifs que ceux \u00e9tablis sur quelques gravures s\u00e9lectionn\u00e9es \u2014 pour comparer les syst\u00e8mes symboliques de ces deux sites.",
        "reference": [
            {
                "reference_title": "LA CIVILISATION DU VAL CAMONICA / EMMANUEL ANATI",
                "reference_link": "publication/31819938_LA_CIVILISATION_DU_VAL_CAMONICA_EMMANUEL_ANATI",
                "reference_type": "Article",
                "reference_date": null,
                "reference_abstract": "INCLUYE BIBLIOGRAFIA"
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 1995",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 1988",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 1972",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 1994",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 1979",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Apr 1981",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 1990",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2007",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2011",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Experimental relativistic zero-knowledge proofs",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (25)",
        "abstract": "Protecting secrets is a key challenge in our contemporary information-based era. In common situations, however, revealing secrets appears unavoidable, for instance, when identifying oneself in a bank to retrieve money. In turn, this may have highly undesirable consequences in the unlikely, yet not unrealistic, case where the bank's security gets compromised. This naturally raises the question of whether disclosing secrets is fundamentally necessary for identifying oneself, or more generally for proving a statement to be correct. Developments in computer science provide an elegant solution via the concept of zero-knowledge proofs: a prover can convince a verifier of the validity of a certain statement without facilitating the elaboration of a proof at all. In this work, we report the experimental realisation of such a zero-knowledge protocol involving two separated verifier-prover pairs. Security is enforced via the physical principle of special relativity, and no computational assumption (such as the existence of one-way functions) is required. Our implementation exclusively relies on off-the-shelf equipment and works at both short (60 m) and long distances (400 m) in about one second. This demonstrates the practical potential of multi-prover zero-knowledge protocols, promising for identification tasks and blockchain-based applications such as cryptocurrencies or smart contracts.",
        "reference": [
            {
                "reference_title": "Quantum supremacy using a programmable superconducting processor",
                "reference_link": "publication/336744162_Quantum_supremacy_using_a_programmable_superconducting_processor",
                "reference_type": "Article",
                "reference_date": "Oct 2019",
                "reference_abstract": "The promise of quantum computers is that certain computational tasks might be executed exponentially faster on a quantum processor than on a classical processor\u00b9. A fundamental challenge is to build a high-fidelity processor capable of running quantum algorithms in an exponentially large computational space. Here we report the use of a processor with programmable superconducting qubits2,3,4,5,6,7 to create quantum states on 53 qubits, corresponding to a computational state-space of dimension 2\u2075\u00b3 (about 10\u00b9\u2076). Measurements from repeated experiments sample the resulting probability distribution, which we verify using classical simulations. Our Sycamore processor takes about 200 seconds to sample one instance of a quantum circuit a million times\u2014our benchmarks currently indicate that the equivalent task for a state-of-the-art classical supercomputer would take approximately 10,000 years. This dramatic increase in speed compared to all known classical algorithms is an experimental realization of quantum supremacy8,9,10,11,12,13,14 for this specific computational task, heralding a much-anticipated computing paradigm."
            },
            {
                "reference_title": "Zerocash: Decentralized Anonymous Payments from Bitcoin",
                "reference_link": "publication/286668243_Zerocash_Decentralized_Anonymous_Payments_from_Bitcoin",
                "reference_type": "Conference Paper",
                "reference_date": "May 2014",
                "reference_abstract": null
            },
            {
                "reference_title": "On proper secrets, (\nt\n, \nk\n)-bases and linear codes",
                "reference_link": "publication/225108746_On_proper_secrets_t_k-bases_and_linear_codes",
                "reference_type": "Article",
                "reference_date": "Aug 2009",
                "reference_abstract": "This paper contains three parts where each part triggered and motivated the subsequent one. In the first part (Proper Secrets) we study the Shamir\u2019s \u201ck-out-of-n\u201d threshold secret sharing scheme. In that scheme, the dealer generates a random polynomial of degree k\u22121 whose free coefficient is the secret and the private shares are point values of that polynomial. We show that the secret\nmay, equivalently, be chosen as any other point value of the polynomial (including the point at infinity), but, on the other\nhand, setting the secret to be any other linear combination of the polynomial coefficients may result in an imperfect scheme.\nIn the second part ((t, k)-bases) we define, for every pair of integers t and k such that 1\u2264 t \u2264k\u22121, the concepts of (t, k)-spanning sets, (t, k)-independent sets and (t, k)-bases as generalizations of the usual concepts of spanning sets, independent sets and bases in a finite-dimensional vector\nspace. We study the relations between those notions and derive upper and lower bounds for the size of such sets. In the third\npart (Linear Codes) we show the relations between those notions and linear codes. Our main notion of a (t, k)-base bridges between two well-known structures: (1, k)-bases are just projective geometries, while (k\u22121, k)-bases correspond to maximal MDS-codes. We show how the properties of (t, k)-independence and (t, k)-spanning relate to the notions of minimum distance and covering radius of linear codes and how our results regarding the\nsize of such sets relate to known bounds in coding theory. We conclude by comparing between the notions that we introduce\nhere and some well known objects from projective geometry."
            },
            {
                "reference_title": "On the Einstein Podolsky Rosen paradox",
                "reference_link": "publication/323130140_On_the_Einstein_Podolsky_Rosen_paradox",
                "reference_type": "Article",
                "reference_date": "Nov 1964",
                "reference_abstract": null
            },
            {
                "reference_title": "Post-quantum cryptography",
                "reference_link": "publication/319683742_Post-quantum_cryptography",
                "reference_type": "Article",
                "reference_date": "Sep 2017",
                "reference_abstract": "Cryptography is essential for the security of online communication, cars and implanted medical devices. However, many commonly used cryptosystems will be completely broken once large quantum computers exist. Post-quantum cryptography is cryptography under the assumption that the attacker has a large quantum computer; post-quantum cryptosystems strive to remain secure even in this scenario. This relatively young research area has seen some successes in identifying mathematical operations for which quantum algorithms offer little advantage in speed, and then building cryptographic systems around those. The central challenge in post-quantum cryptography is to meet demands for cryptographic usability and flexibility without sacrificing confidence. \u00a9 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved."
            },
            {
                "reference_title": "Relativistic (or $2$-prover $1$-round) zero-knowledge protocol for $\\mathsf{NP}$ secure against quantum adversaries",
                "reference_link": "publication/311842335_Relativistic_or_2-prover_1-round_zero-knowledge_protocol_for_mathsfNP_secure_against_quantum_adversaries",
                "reference_type": "Article",
                "reference_date": "Dec 2016",
                "reference_abstract": "In this paper, we show that the zero-knowledge construction for Hamiltonian cycle remains secure against quantum adversaries in the relativistic setting. Our main technical contribution is a tool for studying the action of consecutive measurements on a quantum state which in turn gives upper bounds on the value of some entangled games. This allows us to prove the security of our protocol against quantum adversaries. We also prove security bounds for the (single-round) relativistic string commitment and bit commitment in parallel against quantum adversaries. As an additional consequence of our result, we answer an open question from [Unr12] and show tight bounds on the quantum knowledge error of some $\\Sigma$-protocols."
            },
            {
                "reference_title": "A zero-knowledge protocol for nuclear warhead verification",
                "reference_link": "publication/263474953_A_zero-knowledge_protocol_for_nuclear_warhead_verification",
                "reference_type": "Article",
                "reference_date": "Jun 2014",
                "reference_abstract": "The verification of nuclear warheads for arms control involves a paradox: international inspectors will have to gain high confidence in the authenticity of submitted items while learning nothing about them. Proposed inspection systems featuring 'information barriers', designed to hide measurements stored in electronic systems, are at risk of tampering and snooping. Here we show the viability of a fundamentally new approach to nuclear warhead verification that incorporates a zero-knowledge protocol, which is designed in such a way that sensitive information is never measured and so does not need to be hidden. We interrogate submitted items with energetic neutrons, making, in effect, differential measurements of both neutron transmission and emission. Calculations for scenarios in which material is diverted from a test object show that a high degree of discrimination can be achieved while revealing zero information. Our ideas for a physical zero-knowledge system could have applications beyond the context of nuclear disarmament. The proposed technique suggests a way to perform comparisons or computations on personal or confidential data without measuring the data in the first place."
            },
            {
                "reference_title": "Cryptography Miracles, Secure Auctions, Matching Problem Verification",
                "reference_link": "publication/262246835_Cryptography_Miracles_Secure_Auctions_Matching_Problem_Verification",
                "reference_type": "Article",
                "reference_date": "Feb 2014",
                "reference_abstract": "In this article, we extend the methods of Rabin et al.10,11 in a major way and provide a solution to the long-standing important problem of preventing collusion in secondprice (Vickrey) auctions. The new tools presented are deniable revelation of a secret value and uncontrollable deniable bidding. In Rabin et al.,10,11 new highly efficient methods for proving correctness of announced results of computations were introduced. These proofs completely conceal input values and intermediate results of the computation. One application was to enable an Auctioneer to announce outcome of a sealed bid auction and provide verification of correctness of the outcome, while keeping bid values informationtheoretically secret. We quickly survey these methods for completeness of the discussion and because of their wide applicability. Another example of an application is to prove to participants of a stable matching process such as the assignment residents to hospitals, of the correctness of the announced assignment without revealing any preferences of residents with respect to hospitals and vice-versa."
            },
            {
                "reference_title": "Optimal Ternary Cyclic Codes with Minimum Distance Four and Five",
                "reference_link": "publication/256441213_Optimal_Ternary_Cyclic_Codes_with_Minimum_Distance_Four_and_Five",
                "reference_type": "Article",
                "reference_date": "Sep 2013",
                "reference_abstract": "Cyclic codes are an important subclass of linear codes and have wide\napplications in data storage systems, communication systems and consumer\nelectronics. In this paper, two families of optimal ternary cyclic codes are\npresented. The first family of cyclic codes has parameters $[3^m-1, 3^m-1-2m,\n4]$ and contains a class of conjectured cyclic codes and several new classes of\noptimal cyclic codes. The second family of cyclic codes has parameters $[3^m-1,\n3^m-2-2m, 5]$ and contains a number of classes of cyclic codes that are\nobtained from perfect nonlinear functions over $\\fthreem$, where $m>1$ and is a\npositive integer."
            },
            {
                "reference_title": "Constructive generation of very hard 3-colorability instances",
                "reference_link": "publication/222082717_Constructive_generation_of_very_hard_3-colorability_instances",
                "reference_type": "Article",
                "reference_date": "Jan 2008",
                "reference_abstract": "Graph colorability (COL), is a typical constraint satisfaction problem to which phase transition phenomena (PTs), are important in the computational complexity of combinatorial search algorithms. PTs are significant and subtle because, in the PT region, extraordinarily hard problem instances are found, which may require exponential-order computational time to solve. To clarify PT mechanism, many studies have been undertaken to produce very hard instances, many of which were based on generate-and-test approaches. We propose a rather systematic or constructive algorithm that repeats the embedding of 4-critical graphs to arbitrarily generate large extraordinarily hard 3-colorability instances. We demonstrated experimentally that the computational cost to solve our generated instances is of an exponential order of the number of vertices by using a few actual coloring algorithms and constraint satisfaction algorithms."
            }
        ]
    },
    {
        "title": "A study of university students' attitude towards integration of information technology in higher education in Mauritius",
        "date": "December 2020",
        "doi": "10.1111/hequ.12288",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (52)",
        "abstract": "en Technology is changing the way teaching and learning take place. The main purpose of this study is to investigate tertiary students' attitude towards integrating information technology (IT) in higher education. Using stratified random sampling, 180 questionnaires were distributed to students from six tertiary institutions in Mauritius. Exploratory Factor Analysis followed by multiple regression analysis were used to identify factors which influence the perception of students towards integrating IT in education. Our findings show that prior experience, IT self\u2010efficacy, compatibility and institute support are the main determinants of the attitude towards IT integration in tertiary education. Our findings are expected to be useful to a number of players in the tertiary education sector.\n\nAbstrait\nes La technologie change la fa\u00e7on dont l\u2019enseignement et l\u2019apprentissage se d\u00e9roulent. L\u2019objectif principal de cette \u00e9tude est d\u2019\u00e9tudier la perception de l\u2019attitude des \u00e9tudiants de l\u2019enseignement sup\u00e9rieur \u00e0 l\u2019\u00e9gard de l\u2019int\u00e9gration des technologies de l\u2019information (TI) dans l\u2019enseignement sup\u00e9rieur. En utilisant un \u00e9chantillonnage al\u00e9atoire stratifi\u00e9, 180 questionnaires ont \u00e9t\u00e9 distribu\u00e9s aux \u00e9tudiants de six \u00e9tablissements d\u2019enseignement sup\u00e9rieur \u00e0 Maurice. Une analyse factorielle exploratoire suivie d\u2019une analyse de r\u00e9gression multiple a \u00e9t\u00e9 utilis\u00e9e pour identifier les facteurs qui influencent la perception des \u00e9tudiants quant \u00e0 l\u2019int\u00e9gration des TI dans l\u2019\u00e9ducation. Nos r\u00e9sultats montrent que l\u2019exp\u00e9rience ant\u00e9rieure, l\u2019auto\u2010efficacit\u00e9 informatique, la compatibilit\u00e9 et le soutien de l\u2019institut sont les principaux d\u00e9terminants de l\u2019attitude envers l\u2019int\u00e9gration informatique dans l\u2019enseignement sup\u00e9rieur. Nos r\u00e9sultats devraient \u00eatre utiles \u00e0 un certain nombre d\u2019acteurs du secteur de l\u2019enseignement sup\u00e9rieur.",
        "reference": [
            {
                "reference_title": "Mediating Role of Attitude, Subjective Norm And Perceived Behavioural Control In The Relationships Between Their Respective Salient Beliefs And Behavioural Intention To Adopt E-Learning Among Instructors In Jordanian Universities",
                "reference_link": "publication/331639699_Mediating_Role_of_Attitude_Subjective_Norm_And_Perceived_Behavioural_Control_In_The_Relationships_Between_Their_Respective_Salient_Beliefs_And_Behavioural_Intention_To_Adopt_E-Learning_Among_Instructo",
                "reference_type": "Article",
                "reference_date": "Mar 2019",
                "reference_abstract": "The purpose of this study is to examine the factors that determine intention to adopt e-learning in Jordanian universities. Two models of e-learning that are observed among adopting institutions: E-learning as a supplement to traditional classroom mode, and total electronic learning. The respondents in this research have just been introduced to the first model. The paper takes a social, and technical approach in its investigation by using a research model based on the Theory of Planned Behaviour (TPB) to identify the factors that affect intention to adopt e-learning. The model identifies specific salient beliefs that may influence technology usage, such as instructors attitude, subjective norm, perceived behavioural control, perceived usefulness, ease of use, Normative beliefs, Internet Self-efficacy, Perceived Accessibility and university support. stratified random sampling method was used to select instructors. Hierarchical Multiple Regression Analysis was used to assess the relationships in the constructs. The paper presents some findings on e-learning adoption intention determinants. It also discusses some of the implications of the findings on theory and practice."
            },
            {
                "reference_title": "Online Social Networking as a Tool to Enhance Learning in the Mauritian Education System",
                "reference_link": "publication/318792893_Online_Social_Networking_as_a_Tool_to_Enhance_Learning_in_the_Mauritian_Education_System",
                "reference_type": "Article",
                "reference_date": "Jul 2012",
                "reference_abstract": "Social networking sites enable individuals to connect to each other to form online communities. In the last decade, online social networking (OSN) has become a fashionable means of communication among Internet users, particularly the young generation. It is known that the majority of adolescents log on their OSN accounts daily and stay log on for a prolonged period of time. Thus, social networking sites represent a potential technology that can be exploited to enhance learning so as to help students in their education. In this paper, the outcome of a major survey carried out among the secondary level students in Mauritius is reported. It is seen that most students are already using OSN for non-formal learning with 39% of the respondents stating that they make use of OSN to discuss school work. It is also observed that OSN acts as a platform for these adolescents to communicate and collaborate on school related projects or assignments. Most students agreed on the fact that OSN can be beneficial for their studies. It is also found that a small percentage of teachers are using OSN as a tool to facilitate teaching and learning. Therefore, a few recommendations are made in this paper with respect to the integration of OSN in the Mauritian secondary education system."
            },
            {
                "reference_title": "Leading to Intention: The Role of Attitude in Relation to Technology Acceptance Model in E-Learning",
                "reference_link": "publication/314130738_Leading_to_Intention_The_Role_of_Attitude_in_Relation_to_Technology_Acceptance_Model_in_E-Learning",
                "reference_type": "Article",
                "reference_date": "Dec 2017",
                "reference_abstract": "Malaysian government has given attention to the burgeoning of E-learning due to the excessive demand in line with technology advancement for tertiary education in the country. However, the intensity of its usage is not very remarkable, there is a need to understand student's attitude on the use of the e-learning system from the perspectives of the Technology Acceptance Model. The objective of the study is to investigate the attitude of university students about the use of E-learning based on the Technology Acceptance Model. This research is to analyze the relationship of university students\u2019 intention to use e-learning with three antecedents include attitude, perceived usefulness and perceived ease of use. A survey method was used to 151 students by using a questionnaire as a tool. Participants were selected using random sampling. The informed consent was obtained from participants as for ethical consideration. Findings indicated that attitude was a significant predictor towards student's intention to use E-Learning. As a result, it is seen that students\u2019 attitude plays an important role in contributing to the intention to use e-learning system. The outcome of this study is expected to improve and upgraded E-learning system to be beneficial according to the needs of the students."
            },
            {
                "reference_title": "Exploring students acceptance of e-learning using Technology Acceptance Model in Jordanian universities",
                "reference_link": "publication/284676003_Exploring_students_acceptance_of_e-learning_using_Technology_Acceptance_Model_in_Jordanian_universities",
                "reference_type": "Article",
                "reference_date": "Apr 2013",
                "reference_abstract": null
            },
            {
                "reference_title": "User Acceptance of Information Technology: Theories and Models",
                "reference_link": "publication/277983543_User_Acceptance_of_Information_Technology_Theories_and_Models",
                "reference_type": "Article",
                "reference_date": "Jan 1996",
                "reference_abstract": null
            },
            {
                "reference_title": "The technology acceptance model (TAM): A meta-analytic structural equation modeling approach to explaining teachers\u2019 adoption of digital technology in education",
                "reference_link": "publication/327724197_The_technology_acceptance_model_TAM_A_meta-analytic_structural_equation_modeling_approach_to_explaining_teachers'_adoption_of_digital_technology_in_education",
                "reference_type": "Article",
                "reference_date": "Sep 2018",
                "reference_abstract": "The extent to which teachers adopt technology in their teaching practice has long been in the focus of research. Indeed, a plethora of models exist explaining influential factors and mechanisms of technology use in classrooms, one of which\u2014the Technology Acceptance Model (TAM) and versions thereof\u2014has dominated the field. Although consensus exists about which factors in the TAM might predict teachers\u2019 technology adoption, the current field abounds in some controversies and inconsistent findings. This meta-analysis seeks to clarify some of these issues by combining meta-analysis with structural equation modeling approaches. Specifically, we synthesized 124 correlation matrices from 114 empirical TAM studies (N = 34,357 teachers) and tested the fit of the TAM and its versions. Overall, the TAM explains technology acceptance well; yet, the role of certain key constructs and the importance of external variables contrast some existing beliefs about the TAM. Implications for research and practice are discussed.\n\n50 days' free access to the article:\nhttps://www.sciencedirect.com/science/authShare/S0360131518302458/20180922T060800Z/1?md5=8d2c1df052d2f4f18019d1d39d479a27&dgcid=coauthor"
            },
            {
                "reference_title": "Emerging educational technology: Assessing the factors that influence instructors' acceptance in information systems and other classrooms",
                "reference_link": "publication/312976801_Emerging_educational_technology_Assessing_the_factors_that_influence_instructors'_acceptance_in_information_systems_and_other_classrooms",
                "reference_type": "Article",
                "reference_date": "Jan 2008",
                "reference_abstract": null
            },
            {
                "reference_title": "Integrating academic support to develop undergraduate research in Dental Technology: A case study in a South African University of Technology",
                "reference_link": "publication/312300331_Integrating_academic_support_to_develop_undergraduate_research_in_Dental_Technology_A_case_study_in_a_South_African_University_of_Technology",
                "reference_type": "Article",
                "reference_date": "Jan 2017",
                "reference_abstract": "This paper explores students\u2019 experiences of academic support within the teaching of undergraduate research. Although the literature on undergraduate teaching documents the value of this support, few studies have assessed this area of work within the teaching and learning of undergraduate research in a South African context. This is considered significant within the field of academic development in view of indications from the South African Council on Higher Education that the curricula contain key transitions for which students are differently prepared. A qualitative research design and a case study methodology were used. Lee\u2019s conceptual framework on research supervision was used to analyse students\u2019 reflective reports and interview feedback. The main findings of this study revealed that academic support in the teaching of undergraduate research enabled students to progressively manage, question and analyse their research. Potentially, the integration of academic support into the teaching-research nexus supports graduate attributes."
            },
            {
                "reference_title": "The web-enhanced classroom",
                "reference_link": "publication/297130132_The_web-enhanced_classroom",
                "reference_type": "Article",
                "reference_date": "Jan 2002",
                "reference_abstract": "The four fundamental components to successfully web-enhance a course are discussed. A combination of online and traditional classroom instruction is the most popular way to use Internet teaching and learning tools. It is observed that students with access to both traditional lectures and an online environment fair better academically than students instructed entirely in the traditional classroom or by the Internet. The four components are administration, assessment, content and community and each component can be incorporated to a course to enhance student learning in various ways."
            },
            {
                "reference_title": "How does student ability and self-efficacy affect the usage of computer technology?",
                "reference_link": "publication/279588493_How_does_student_ability_and_self-efficacy_affect_the_usage_of_computer_technology",
                "reference_type": "Article",
                "reference_date": "Jan 2009",
                "reference_abstract": "The main aim of this research was to find out the self-efficacy level among participant students and analyze their beliefs. This study showed that male students are more confident comparing to female student, similar to research of Bimer (2000), the computer usage has been known as biased toward the interests and fashion of men, this research also showed that females are not as confident as men are to computers. Awoleye & Siyanbola (2005), Bimer (2000), indicated that computers have some gendered attributes that favor man in some way so that men are more likely to use computers and they are more confident. Therefore it can be said that many studies has been support the gender factor in self-efficacy, but this research also showed that this changes are more likely to be depend on the complexity of the task and the year of computer usage of the particular student as Busch(1995) has been found the similar results. Similar to the study of Compeau, and Higging (1995), it is found that self-efficacy shapes the individuals beliefs and behaviors as well. It is not surprising to find out that students have different computer levels and this affects their self-efficacy. Also some students have advance computer knowledge, therefore they complain about the level of the computer courses offered to them. In addition to this it can be said that students with different computer skills shows different self-efficacy levels as well."
            }
        ]
    },
    {
        "title": "State Estimation of Power Flows for Smart Grids via Belief Propagation",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (38)",
        "abstract": "Belief propagation is an algorithm that is known from statistical physics and computer science. It provides an efficient way of calculating marginals that involve large sums of products which are efficiently rearranged into nested products of sums to approximate the marginals. It allows a reliable estimation of the state and its variance of power grids that is needed for the control and forecast of power grid management. At prototypical examples of IEEE-grids we show that belief propagation not only scales linearly with the grid size for the state estimation itself, but also facilitates and accelerates the retrieval of missing data and allows an optimized positioning of measurement units. Based on belief propagation, we give a criterion for how to assess whether other algorithms, using only local information, are adequate for state estimation for a given grid. We also demonstrate how belief propagation can be utilized for coarse-graining power grids towards representations that reduce the computational effort when the coarse-grained version is integrated into a larger grid. It provides a criterion for partitioning power grids into areas in order to minimize the error of flow estimates between different areas.",
        "reference": [
            {
                "reference_title": "The role of spatial scale in joint optimisations of generation and transmission for European highly renewable scenarios",
                "reference_link": "publication/317061757_The_role_of_spatial_scale_in_joint_optimisations_of_generation_and_transmission_for_European_highly_renewable_scenarios",
                "reference_type": "Article",
                "reference_date": "May 2017",
                "reference_abstract": "The effects of the spatial scale on the results of the optimisation of transmission and generation capacity in Europe are quantified under a 95% CO2 reduction compared to 1990 levels, interpolating between one-node-per-country solutions and many-nodes-per-country. The trade-offs that come with higher spatial detail between better exposure of transmission bottlenecks, exploitation of sites with good renewable resources (particularly wind power) and computational limitations are discussed. It is shown that solutions with no grid expansion beyond today's capacities are only around 20% more expensive than with cost-optimal grid expansion."
            },
            {
                "reference_title": "State Estimation in Electric Power Systems Using Belief Propagation: An Extended DC Model",
                "reference_link": "publication/303545235_State_Estimation_in_Electric_Power_Systems_Using_Belief_Propagation_An_Extended_DC_Model",
                "reference_type": "Article",
                "reference_date": "May 2016",
                "reference_abstract": "In this paper, we model an extended DC state estimation (SE) in an electric power system as a factor graph (FG) and solve it using belief propagation (BP) algorithm. The DC model comprises bus voltage angles as state variables, while the extended DC model includes bus voltage angles and bus voltage magnitudes as state variables. By applying BP to solve the SE problem in the extended DC model, we obtain a Gaussian BP scenario for which we derive closed-form expressions for BP messages exchanged along the FG. The performance of the BP algorithm is demonstrated for the IEEE 14 bus test case. Finally, the application of BP algorithm on the extended DC scenario provides significant insights into a fundamental structure of BP equations in more complex models such as the AC model - the topic we will investigate in our follow up work. As a side-goal of this paper, we aim at thorough and detailed presentation on applying BP on the SE problem in order to make the powerful BP algorithm more accessible and applicable within the power-engineering community."
            },
            {
                "reference_title": "Distributed Gauss-Newton Method for AC State Estimation: A Belief Propagation Approach",
                "reference_link": "publication/303545211_Distributed_Gauss-Newton_Method_for_AC_State_Estimation_A_Belief_Propagation_Approach",
                "reference_type": "Article",
                "reference_date": "May 2016",
                "reference_abstract": "In this paper, we propose a solution to an AC state estimation problem in electric power systems using a fully distributed Gauss-Newton method. The proposed method is placed within the context of factor graphs and belief propagation algorithms and closed-form expressions for belief propagation messages exchanged along the factor graph are derived. The obtained algorithm provides the same solution as the conventional weighted least-squares state estimation. Using a simple example, we provide a step-by-step presentation of the proposed algorithm. Finally, we discuss the convergence behaviour using the IEEE 14 bus test case."
            },
            {
                "reference_title": "The Factor Graph Approach to Model-Based Signal Processing Factor graphs can model complex systems and help to design effective algorithms for detection and estimation problems",
                "reference_link": "publication/237435321_The_Factor_Graph_Approach_to_Model-Based_Signal_Processing_Factor_graphs_can_model_complex_systems_and_help_to_design_effective_algorithms_for_detection_and_estimation_problems",
                "reference_type": "Article",
                "reference_date": null,
                "reference_abstract": "The message-passing approach to model-based signal processing is developed with a focus on Gaussian message passing in linear state-space models, which includes recursive least squares, linear minimum-mean-squared-error estimation, and Kalman filtering algorithms. Tabulated mes- sage computation rules for the building blocks of linear models allow us to compose a variety of such algorithms without additional derivations or computations. Beyond the Gaussian case, it is emphasized that the message-passing approach encourages us to mix and match different algorithmic tech- niques, which is exemplified by two different approachesV steepest descent and expectation maximizationVto message passing through a multiplier node."
            },
            {
                "reference_title": "Power systems test case archive - UWEE",
                "reference_link": "publication/258222861_Power_systems_test_case_archive_-_UWEE",
                "reference_type": "Article",
                "reference_date": "Jan 2000",
                "reference_abstract": null
            },
            {
                "reference_title": "Tree-Structured Approximations By Expectation",
                "reference_link": "publication/2869146_Tree-Structured_Approximations_By_Expectation",
                "reference_type": "Article",
                "reference_date": "Mar 2004",
                "reference_abstract": "Approximation structure plays an important role in inference on loopy graphs. As a tractable structure, tree approximations have been utilized in the variational method of Ghahramani & Jordan (1997) and the sequential projection method of Frey et al. (2000). However, belief propagation represents each factor of the graph with a product of single-node messages. In this paper, belief propagation is extended to represent factors with tree approximations, by way of the expectation propagation framework. That is, each factor sends a \"message\" to all pairs of nodes in a tree structure. The result is more accurate inferences and more frequent convergence than ordinary belief propagation, at a lower cost than variational trees or double-loop algorithms."
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2004",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2016",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2009",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2012",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Ci\u00eancia Da Computa\u00e7\u00e3o: Tecnologias Emergentes Em Computa\u00e7\u00e3o",
        "date": "December 2020",
        "doi": "10.37885/978-65-87196-55-8",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (161)",
        "abstract": "Ci\u00eancia Da Computa\u00e7\u00e3o: Tecnologias Emergentes Em Computa\u00e7\u00e3o",
        "reference": [
            {
                "reference_title": "rnaSPAdes: a de novo transcriptome assembler and its application to RNA-Seq data",
                "reference_link": "publication/335597152_rnaSPAdes_a_de_novo_transcriptome_assembler_and_its_application_to_RNA-Seq_data",
                "reference_type": "Article",
                "reference_date": "Sep 2019",
                "reference_abstract": "Background\nThe possibility of generating large RNA-sequencing datasets has led to development of various reference-based and de novo transcriptome assemblers with their own strengths and limitations. While reference-based tools are widely used in various transcriptomic studies, their application is limited to the organisms with finished and well-annotated genomes. De novo transcriptome reconstruction from short reads remains an open challenging problem, which is complicated by the varying expression levels across different genes, alternative splicing, and paralogous genes.\n\nResults\nHerein we describe the novel transcriptome assembler rnaSPAdes, which has been developed on top of the SPAdes genome assembler and explores computational parallels between assembly of transcriptomes and single-cell genomes. We also present quality assessment reports for rnaSPAdes assemblies, compare it with modern transcriptome assembly tools using several evaluation approaches on various RNA-sequencing datasets, and briefly highlight strong and weak points of different assemblers.\n\nConclusions\nBased on the performed comparison between different assembly methods, we infer that it is not possible to detect the absolute leader according to all quality metrics and all used datasets. However, rnaSPAdes typically outperforms other assemblers by such important property as the number of assembled genes and isoforms, and at the same time has higher accuracy statistics on average comparing to the closest competitors."
            },
            {
                "reference_title": "Towards evaluation of cloud ontologies",
                "reference_link": "publication/330013697_Towards_evaluation_of_cloud_ontologies",
                "reference_type": "Article",
                "reference_date": "Dec 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Indicadores para Avalia\u00e7\u00e3o de Software Educacional com base no guia GDSM (Goal Driven Software Measurement)",
                "reference_link": "publication/328735886_Indicadores_para_Avaliacao_de_Software_Educacional_com_base_no_guia_GDSM_Goal_Driven_Software_Measurement",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "fastp: an ultra-fast all-in-one FASTQ preprocessor",
                "reference_link": "publication/327641740_fastp_an_ultra-fast_all-in-one_FASTQ_preprocessor",
                "reference_type": "Article",
                "reference_date": "Sep 2018",
                "reference_abstract": "Motivation\nQuality control and preprocessing of FASTQ files are essential to providing clean data for downstream analysis. Traditionally, a different tool is used for each operation, such as quality control, adapter trimming and quality filtering. These tools are often insufficiently fast as most are developed using high-level programming languages (e.g. Python and Java) and provide limited multi-threading support. Reading and loading data multiple times also renders preprocessing slow and I/O inefficient.\n\nResults\nWe developed fastp as an ultra-fast FASTQ preprocessor with useful quality control and data-filtering features. It can perform quality control, adapter trimming, quality filtering, per-read quality pruning and many other operations with a single scan of the FASTQ data. This tool is developed in C++ and has multi-threading support. Based on our evaluation, fastp is 2\u20135 times faster than other FASTQ preprocessing tools such as Trimmomatic or Cutadapt despite performing far more operations than similar tools.\n\nAvailability and implementation\nThe open-source code and corresponding instructions are available at https://github.com/OpenGene/fastp."
            },
            {
                "reference_title": "A multi-source domain annotation pipeline for quantitative metagenomic and metatranscriptomic functional profiling",
                "reference_link": "publication/327275701_A_multi-source_domain_annotation_pipeline_for_quantitative_metagenomic_and_metatranscriptomic_functional_profiling",
                "reference_type": "Article",
                "reference_date": "Aug 2018",
                "reference_abstract": "Background\nBiochemical and regulatory pathways have until recently been thought and modelled within one cell type, one organism and one species. This vision is being dramatically changed by the advent of whole microbiome sequencing studies, revealing the role of symbiotic microbial populations in fundamental biochemical functions. The new landscape we face requires the reconstruction of biochemical and regulatory pathways at the community level in a given environment. In order to understand how environmental factors affect the genetic material and the dynamics of the expression from one environment to another, we want to evaluate the quantity of gene protein sequences or transcripts associated to a given pathway by precisely estimating the abundance of protein domains, their weak presence or absence in environmental samples. ResultsMetaCLADE is a novel profile-based domain annotation pipeline based on a multi-source domain annotation strategy. It applies directly to reads and improves identification of the catalog of functions in microbiomes. MetaCLADE is applied to simulated data and to more than ten metagenomic and metatranscriptomic datasets from different environments where it outperforms InterProScan in the number of annotated domains. It is compared to the state-of-the-art non-profile-based and profile-based methods, UProC and HMM-GRASPx, showing complementary predictions to UProC. A combination of MetaCLADE and UProC improves even further the functional annotation of environmental samples. Conclusions\nLearning about the functional activity of environmental microbial communities is a crucial step to understand microbial interactions and large-scale environmental impact. MetaCLADE has been explicitly designed for metagenomic and metatranscriptomic data and allows for the discovery of patterns in divergent sequences, thanks to its multi-source strategy. MetaCLADE highly improves current domain annotation methods and reaches a fine degree of accuracy in annotation of very different environments such as soil and marine ecosystems, ancient metagenomes and human tissues."
            },
            {
                "reference_title": "Internet of Things-Enabled Smart Cities: State-of-the-Art and Future Trends",
                "reference_link": "publication/326552666_Internet_of_Things-Enabled_Smart_Cities_State-of-the-Art_and_Future_Trends",
                "reference_type": "Article",
                "reference_date": "Jul 2018",
                "reference_abstract": "The dramatic spread of urbanization in modern cities requires smart solutions to address critical issues such as mobility, healthcare, energy, and civil infrastructure. The Internet of Things (IoT) is one of the most promising enabling technologies for tackling these challenges by creating a massive world-wide network of interconnected physical objects embedded with electronics, software, sensors, and network connectivity. Arguably, IoT is becoming the building block for next generation smart cities owing to its potential in exploiting sustainable information and communication technologies. The rapid development of the IoT is impacting several scientific and engineering application domains. This paper presents a comprehensive literature review of key features and applications of the IoT paradigm to support sustainable development of smart cities. An emphasis is placed on concomitance of the IoT solutions with other enabling technologies such as cloud computing, robotics, micro-electromechanical systems (MEMS), wireless communications, and radio-frequency identification (RFID). Furthermore, a case study is presented to demonstrate how an affordable and suitable IoT-based working prototype can be designed for real-time monitoring of civil infrastructure. Finally, challenges and future directions for IoT-based smart city applications are discussed."
            },
            {
                "reference_title": "Knowledge Graphs: Methodology, Tools and Selected Use Cases",
                "reference_link": "publication/338943166_Knowledge_Graphs_Methodology_Tools_and_Selected_Use_Cases",
                "reference_type": "Book",
                "reference_date": "Jan 2020",
                "reference_abstract": "This book describes methods and tools that empower information providers to build and maintain knowledge graphs, including those for manual, semi-automatic, and automatic construction; implementation; and validation and verification of semantic annotations and their integration into knowledge graphs. It also presents lifecycle-based approaches for semi-automatic and automatic curation of these graphs, such as approaches for assessment, error correction, and enrichment of knowledge graphs with other static and dynamic resources.\n\nChapter 1 defines knowledge graphs, focusing on the impact of various approaches rather than mathematical precision. Chapter 2 details how knowledge graphs are built, implemented, maintained, and deployed. Chapter 3 then introduces relevant application layers that can be built on top of such knowledge graphs, and explains how inference can be used to define views on such graphs, making it a useful resource for open and service-oriented dialog systems. Chapter 4 discusses applications of knowledge graph technologies for e-tourism and use cases for other verticals. Lastly, Chapter 5 provides a summary and sketches directions for future work. The additional appendix introduces an abstract syntax and semantics for domain specifications that are used to adapt schema.org to specific domains and tasks.\n\nTo illustrate the practical use of the approaches presented, the book discusses several pilots with a focus on conversational interfaces, describing how to exploit knowledge graphs for e-marketing and e-commerce. It is intended for advanced professionals and researchers requiring a brief introduction to knowledge graphs and their implementation."
            },
            {
                "reference_title": "Interoperability in semantic Web of Things: Design issues and solutions",
                "reference_link": "publication/330780052_Interoperability_in_semantic_Web_of_Things_Design_issues_and_solutions",
                "reference_type": "Article",
                "reference_date": "Jan 2019",
                "reference_abstract": "The significant improvement in processing power, communication, energy consumption, and the size of computational devices has led to the emergence of the Internet of Things (IoT). IoT projects raise many challenges, such as the interoperability between IoT applications because of the high number of sensors, actuators, services, protocols, and data associated with these systems. Semantics solves this problem by using annotations that define the role of each IoT element and reduces the ambiguity of information exchanged between the devices. This work presents SWoTPAD, a semantic framework that helps in the development of IoT projects. The framework is designer oriented and provides a semantic language that is more user\u2010friendly than OWL\u2010S and WSML and allows the IoT designer to specify devices, services, environment, and requests. Following this, it makes use of these specifications and maps them for RESTful services. Additionally, it generates an automatic service composition engine that is able to combine services needed to handle complex user requests. We validated this approach with two case studies. The former concerns a residential security system and the latter, the cloud application deployment. The average time required for service discovery and automatic service composition corresponds to 72.9% of the service execution time in the case study 1 and 64.4% in the case study 2. This work presents SWoTPAD, a semantic framework that helps in the development of IoT projects. The framework is designer oriented and provides a semantic language that is more user\u2010friendly than OWL\u2010S and WSML and allows the IoT designer to specify devices, services, environment, and requests. Following this, it makes use of these specifications and maps them for RESTful services. Additionally, it generates an automatic service composition engine that is able to combine services needed to handle complex user requests."
            },
            {
                "reference_title": "Cloud Brokerage: A Systematic Survey",
                "reference_link": "publication/330697960_Cloud_Brokerage_A_Systematic_Survey",
                "reference_type": "Article",
                "reference_date": "Jan 2019",
                "reference_abstract": "Background\u2014The proliferation of cloud services has opened a space for cloud brokerage services. Brokers intermediate between cloud customers and providers to assist the customer in selecting the most suitable service, helping to manage the dimensionality, heterogeneity, and uncertainty associated with cloud services. Objective\u2014Unlike other surveys, this survey focuses on the customer perspective. The survey systematically analyses the literature to identify and classify approaches to realise cloud brokerage, presenting an understanding of the state-of-the-art and a novel taxonomy to characterise cloud brokers. Method\u2014A systematic literature survey was conducted to compile studies related to cloud brokerage and explore how cloud brokers are engineered. These studies are then analysed from multiple perspectives, such as motivation, functionality, engineering approach, and evaluation methodology. Results\u2014The survey resulted in a knowledge base of current proposals for realising cloud brokers. The survey identified differences between the studies\u2019 implementations, with engineering efforts directed at combinations of market-based solutions, middlewares, toolkits, algorithms, semantic frameworks, and conceptual frameworks. Conclusion\u2014Our comprehensive meta-analysis shows that cloud brokerage is still a formative field. Although significant progress has been achieved in this field, considerable challenges remain to be addressed, which are also identified in this survey."
            },
            {
                "reference_title": "Avaliando Jogos Digitais Educativos para Indiv\u00edduos Portadores do Transtorno do Espectro Autista",
                "reference_link": "publication/328735508_Avaliando_Jogos_Digitais_Educativos_para_Individuos_Portadores_do_Transtorno_do_Espectro_Autista",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2018",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Methodological recommendations to face an online semester in a catastrophic situation: the case of a computer science program during the COVID-19 lockdown",
        "date": "December 2020",
        "doi": "10.1109/SCCC51225.2020.9281214",
        "conferance": "Conference: 2020 39th International Conference of the Chilean Computer Science Society (SCCC)",
        "citations_count": null,
        "reference_count": "References (20)",
        "abstract": "The pandemic context forced most of the Higher Education Institutions, and therefore their faculties, to face a semester of teaching in an online mode. The universities defined general guidelines and strategies to try to guarantee a minimum quality of the process. However, the literature reports that, depending on its particularities, preparing a semester-long online course requires a design of several months. Therefore, each lecturer worked according to her experience, guidelines, and support she received: \"What could be done, was done.\" This article reports the teaching experience of the 1st semester 2020 of the Computer Science undergraduate program at the Universidad Cat\u00f3lica de Temuco. Some lecturers of this undergraduate program had previous experience in online training; hence they designed strategies that differed in teaching, evaluation, and interaction methodologies while using several technologies available to them. A sample of 115 of our students answered a questionnaire to assess the preferences and problems during this semester. The results show a heterogeneous set of clusters. However, they point to a constructivist asynchronous vision in a group of our students that prefer short videos uploaded by their professors, and the resources that, by their means, they find on the Internet, point towards a constructivist asynchronous vision in a group of our students. The results also allowed us to rescue good practices that help in proposing an emergency online teaching model that will guide the design of the second-semester courses.",
        "reference": [
            {
                "reference_title": "The Difference Between Emergency Remote Teaching and Online Learning",
                "reference_link": "publication/340535196_The_Difference_Between_Emergency_Remote_Teaching_and_Online_Learning",
                "reference_type": "Article",
                "reference_date": "Mar 2020",
                "reference_abstract": "Well-planned online learning experiences are meaningfully different from courses offered online in response to a crisis or disaster. Colleges and universities working to maintain instruction during the COVID-19 pandemic should understand those differences when evaluating this emergency remote teaching."
            },
            {
                "reference_title": "Investigating synchronous and asynchronous class attendance as predictors of academic success in online education",
                "reference_link": "publication/338866385_Investigating_synchronous_and_asynchronous_class_attendance_as_predictors_of_academic_success_in_online_education",
                "reference_type": "Article",
                "reference_date": "Jan 2020",
                "reference_abstract": "Learning is facilitated by participation and interaction and can be synchronously or asynchronously in online education. This study investigated the relationship between students\u2019 academic success and online interaction and participation and explored their class attendance (synchronous virtual classes and/or watching the recorded virtual classes) in the online study mode of an enabling program at Southern Cross University in Australia. The Preparing for Success at SCU Program equips students with study skills for success at university. The data were retrieved from usage information data provided by the Blackboard Learn learning management system. The results show that it is important for students to attend class, but it does not necessarily make a difference whether students attend synchronous virtual classes or watch the recordings of the virtual classes. A significant relationship was found between academic success and the number of hours students participated in and interacted with the online learning system. Academic success may be increased by providing various options for students to participate and interact online, and to attend classes synchronously or asynchronously. The flexibility of online education can enable students to be successful in their studies. The inclusion of varied activities is therefore recommended to increase academic success in online education."
            },
            {
                "reference_title": "Fostering Significant Learning in Sciences",
                "reference_link": "publication/294280214_Fostering_Significant_Learning_in_Sciences",
                "reference_type": "Article",
                "reference_date": "Jul 2014",
                "reference_abstract": "The new global economy depends on workforce competencies in science, technology, engineering and mathematics more than ever before. To prepare a strong workforce, attracting and educating underrepresented minority students in science is a challenge within our traditional American educational approach. To meet this challenge, fostering significant learning in science that nurtures 21st Century skills in students is crucial. The purpose of this study was to analyze the effectiveness of a set of teaching and learning approaches that foster significant learning in sciences. Using a new introductory environmental science course in urban water quality management, the effect of a set of learner-centered teaching approaches, including hands-on learning, scientific inquiry, frequent feedback, and critical thinking exercises, was analyzed. The results of the pre- and post-course survey questions together with formative and summative assessments showed that our students\u2019 cognitive learning skills and interests in learning science were significantly improved."
            },
            {
                "reference_title": "Teaching research methodology in an online ODL environment: strategies followed and lessons learnt",
                "reference_link": "publication/282703310_Teaching_research_methodology_in_an_online_ODL_environment_strategies_followed_and_lessons_learnt",
                "reference_type": "Article",
                "reference_date": "Jan 2015",
                "reference_abstract": "This article provides an overview of an open and distance learning (ODL) honours\nonline research methodology module. The module was developed to address\nthe requirements of the Department of Higher Education and Training (DHET)\nfor the new Programme Quality Mix (PQM) honours degrees. This semester\nmodule involves 15 active weeks of learning, culminating in the submission of\na Portfolio of Evidence summative assessment task. Specific features of the\nmodule are described to illustrate how teaching the content was approached\nin an ODL context. The aim of the approach followed was to enhance student\nmotivation, while maintaining consistent progress in achieving the required\nlearning outcomes throughout the semester. Initial results and student feedback\nare presented."
            },
            {
                "reference_title": "Human-Centered Design as a Frame for Transition to Remote Teaching during the COVID-19 Pandemic",
                "reference_link": "publication/341794529_Human-Centered_Design_as_a_Frame_for_Transition_to_Remote_Teaching_during_the_COVID-19_Pandemic",
                "reference_type": "Article",
                "reference_date": "Jun 2020",
                "reference_abstract": "Teacher education programs all around the world are challenged with the emergency transition to remote teaching due to the COVID-19 pandemic. Human-centered design can help generate creative solutions to the pedagogical problems that teacher educators face during this transition. In this paper, we present a case of the Advanced Learning Technologies course offered to preservice teachers in the Learning Technologies minor program at a Midwestern University. Our human-centered design approach followed three premises: (a) Building empathy, (b) engaging in pedagogical problem solving and (c) establishing an online community of inquiry. We present our process and the results with a design frame that can be used in other teacher education contexts."
            },
            {
                "reference_title": "Peer learning in higher education: learning from and with each other",
                "reference_link": "publication/337907516_Peer_learning_in_higher_education_learning_from_and_with_each_other",
                "reference_type": "Book",
                "reference_date": "Jan 2013",
                "reference_abstract": null
            },
            {
                "reference_title": "Factors related to college students\u2019 self-directed learning with technology",
                "reference_link": "publication/319630459_Factors_related_to_college_students'_self-directed_learning_with_technology",
                "reference_type": "Article",
                "reference_date": "Sep 2017",
                "reference_abstract": "This study investigated factors influencing college students\u2019 self-directed learning with technology. A questionnaire was employed to obtain data from 153 college students on their self-directed learning readiness, the use of Web 2.0 tools for learning, online communication self-efficacy, and computer self-efficacy to predict their self-directed learning with technology. The data were analysed using sequential multiple regression and mediation analyses. The results showed that the predictor variables explained 19% of the variation in self-directed learning with technology. Self-directed learning readiness and the use of Web 2.0 tools for learning were found to be significant predictors of students\u2019 self-directed learning with technology. Moreover, the results indicate that the use of Web 2.0 tools for learning significantly mediated the influence of students\u2019 online communication self-efficacy and computer self-efficacy on their self-directed learning with technology. This study suggests that students be provided with scaffolding or assistance, not only for self-directed learning skills and the use of Web 2.0 tools for learning, but also for computer use and online communication in order to enhance their self-directed learning with technology."
            },
            {
                "reference_title": "Factors Affecting Learners With Disabilities-Instructor Interaction in Online Learning",
                "reference_link": "publication/311447955_Factors_Affecting_Learners_With_Disabilities-Instructor_Interaction_in_Online_Learning",
                "reference_type": "Article",
                "reference_date": "Dec 2016",
                "reference_abstract": "Little research is available documenting the success of students with various types of disabilities in online classroom environments. This study investigates which factors associated with learners with disabilities impact student outcomes in an online learning environment. Forty learners with disabilities participating in online higher education coursework were asked to respond to an electronic survey of 20 questions. Results indicated that there were two factors: (1) the teaching and social presences and (2) the facilitating and supporting of individual communication related to interaction among learners with disabilities and their instructors that impacted students\u2019 perceived learning achievement and class satisfaction. Respondents also indicated that social interaction factors, such as social presence, were correlated with less perceived learning achievement and satisfaction. This study has potential value because it found factors related to learner\u2013instructor control that may predict students with disabilities\u2019 perceived learning achievement and satisfaction."
            },
            {
                "reference_title": "Instructional design: The ADDIE approach",
                "reference_link": "publication/286059899_Instructional_design_The_ADDIE_approach",
                "reference_type": "Book",
                "reference_date": "Jan 2010",
                "reference_abstract": "The Analyze, Design, Develop, Implement, and Evaluate (ADDIE) process is used to introduce an approach to instruction design that has a proven record of success. Instructional Design: The ADDIE Approach is intended to serve as an overview of the ADDIE concept. The primary rationale for this book is to respond to the need for an instruction design primer that addresses the current proliferation of complex educational development models, particularly non-traditional approaches to learning, multimedia development and online learning environments. Many entry level instructional designers and students enrolled in related academic programs indicate they are better prepared to accomplish the challenging work of creating effective training and education materials after they have a thorough understanding of the ADDIE principles. However, a survey of instructional development applications indicate that the overwhelming majority of instructional design models are based on ADDIE, often do not present the ADDIE origins as part of their content, and are poorly applied by people unfamiliar with the ADDIE paradigm. The purpose of this book is to focus on fundamental ADDIE principles, written with a minimum of professional jargon. This is not an attempt to debate scholars or other educational professionals on the finer points of instructional design, however, the book's content is based on sound doctrine and supported by valid empirical research. The only bias toward the topic is that generic terms will be used as often as possible in order to make it easy for the reader to apply the concepts in the book to other specific situations. \u00a9 Springer Science+Business Media, LLC 2009. All rights reserved."
            },
            {
                "reference_title": "Visible Learning for Teachers: Maximising Impact on Learning",
                "reference_link": "publication/270585208_Visible_Learning_for_Teachers_Maximising_Impact_on_Learning",
                "reference_type": "Book",
                "reference_date": "Jan 2012",
                "reference_abstract": "Accession Number: 2012-07127-000. Partial author list: First Author & Affiliation: Hattie, John; Melbourne Education Research Institute, University of Melbourne, Melbourne, Australia. Release Date: 20120611. Publication Type: Book (0200). Format Covered: Print. ISBN: 978-0-415-69014-0, Hardcover; 978-0-415-69015-7, Paperback; 978-0-203-18152-2, Electronic. Language: English. Major Descriptor: Academic Achievement; Learning; School Based Intervention; Teachers; Teaching Methods. Minor Descriptor: Classroom Management; Meta Analysis; Preservice Teachers; Student Teachers. Classification: Curriculum & Programs & Teaching Methods (3530). Population: Human (10). Age Group: Childhood (birth-12 yrs) (100); Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300). Intended Audience: Psychology: Professional & Research (PS). References Available: Y. Page Count: 269."
            }
        ]
    },
    {
        "title": "Methods of Information in Medicine",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (131)",
        "abstract": "Abstract: Virtual reality (VR), as part of computer science, allows\ncomputer-based models of the real world to be generated, and provides\nhumans with a means to inieract with these models through new humancomputer\ninterfaces and, thus, to nearly realistically experience these\nmodels. This contribution explores the technical requirements for VR,\ndescribes technological advances and deficits, and analyzes the\nframework for future technological research and development. Although\nsome non-medical applications are discussed, this contribution focuses\nprimarily on medical applications of VR and outlines future prospects of\nmedical VR applications. Finally, possible hazards arising from the use of\nVR are discussed. The authors recommend an interdisciplinary approach\nto technology assessment of VR.",
        "reference": [
            {
                "reference_title": "The metaphysics of virtual reality",
                "reference_link": "publication/313183097_The_metaphysics_of_virtual_reality",
                "reference_type": "Article",
                "reference_date": "Jan 1991",
                "reference_abstract": null
            },
            {
                "reference_title": "THE ART OF ARTIFICIAL REALITY",
                "reference_link": "publication/297229031_THE_ART_OF_ARTIFICIAL_REALITY",
                "reference_type": "Article",
                "reference_date": "Jan 1991",
                "reference_abstract": null
            },
            {
                "reference_title": "A FUNCTIONAL VIEW OF PROTEINS",
                "reference_link": "publication/297228961_A_FUNCTIONAL_VIEW_OF_PROTEINS",
                "reference_type": "Article",
                "reference_date": "Jan 1991",
                "reference_abstract": null
            },
            {
                "reference_title": "Another world, inside artificial reality",
                "reference_link": "publication/291048663_Another_world_inside_artificial_reality",
                "reference_type": "Article",
                "reference_date": "Jan 1989",
                "reference_abstract": null
            },
            {
                "reference_title": "Virtual reality: A status report",
                "reference_link": "publication/284393270_Virtual_reality_A_status_report",
                "reference_type": "Article",
                "reference_date": "Jan 1991",
                "reference_abstract": null
            },
            {
                "reference_title": "Project GROPE - Haptic displays for scientific visualization",
                "reference_link": "publication/281070254_Project_GROPE_-_Haptic_displays_for_scientific_visualization",
                "reference_type": "Article",
                "reference_date": "Aug 1990",
                "reference_abstract": "We began in 1967 a project to develop a haptic + display for 6-D force fields of interacting protein molecules. We approached it in four stages: a 2-D system, a 3-D system tested with a simple task, a 6-D system tested with a simple task, and a full 6-D molecular docking system, our initial goal. This paper summarizes the entire project -- the four systems, the evaluation experiments, the results, and our observations. The molecular docking system results are new. Our principal conclusions are: Haptic display as an augmentation to visual display can improve perception and understanding both of force fields and of world models populated with impenetrable objects. Whereas man-machine systems can outperform computer-only systems by orders of magnitude on some problems, haptic-augmented interactive systems seem to give about a two-fold performance improvement over purely graphical interactive systems. Better technology may give somewhat more, but a ten-fold improvement does not seem to be in the cards. Chemists using GROPE-III can readily reproduce the true docking positions for drugs whose docking is known (but not to them) and can find very good docks for drugs whose true docks are unknown. The present tool promises to yield new chemistry research results; it is being actively used by research chemists. The most valuable result from using GROPE-III for drug docking is probably the radically improved situation awareness that serious users report. Chemists say they have a new understanding of the details of the receptor site and its force fields, and of why a particular drug docks well or poorly. We see various scientific/education applications for haptic displays but believe entertainment, not scientific visualization, will drive and pace the technology."
            },
            {
                "reference_title": "Virtual integral holography",
                "reference_link": "publication/253082479_Virtual_integral_holography",
                "reference_type": "Article",
                "reference_date": "Aug 1990",
                "reference_abstract": null
            },
            {
                "reference_title": "Use of a 3-D visualization system in the planning and evaluation of facial surgery",
                "reference_link": "publication/252692687_Use_of_a_3-D_visualization_system_in_the_planning_and_evaluation_of_facial_surgery",
                "reference_type": "Article",
                "reference_date": "Apr 1991",
                "reference_abstract": "An integrated system for the planning of maxillo-facial and cranial surgery is described. The system is based on the use of computer graphics to simulate surgical operations on the bone and to predict and present the outcome of these on the soft tissue and appearance of the face. The input data consists of a suitable set of contiguous X-ray CT slices. As well as the core system which provides the simulation and modelling an extensive range of support facilities is provided for measuring external and internal anatomy profile plotting and quantitative postsurgical assessment. A system for long term follow up of facial shape change is provide by an optical surface scanner. Facilities are also provided for the downloading of data to a numerically controlled milling machine for the production of customised prostheses."
            },
            {
                "reference_title": "Visual Issues In The Use Of A Head-Mounted Monocular Display",
                "reference_link": "publication/252112051_Visual_Issues_In_The_Use_Of_A_Head-Mounted_Monocular_Display",
                "reference_type": "Article",
                "reference_date": "Nov 1989",
                "reference_abstract": "A miniature display device, recently available commercially, is aimed at providing a portable, inexpensive means of visual information communication. The display is head-mounted in front of one eye with the other eye's view of the environment unobstructed. Various visual phenomena are associated with this design. The consequences of these phenomena for visual safety, comfort, and efficiency of the user were evaluated: (1) The monocular, partially occluded mode of operation interrupts binocular vision. Presenting disparate images to each eye results in binocular rivalry. The two images may appear superimposed, with one image perceived with greater clarity or com letely dominant. Most observers can, use the display comfortably in this rivalrous mode. In many cases, it is easier to use the display in a peripheral position, slightly above or below the line of sight, thus permitting normal binocular vision of the environment. (2) As a head-mounted device, the displayed image is perceived to move during head movements due to the response of the vestibulo-ocular reflex. These movements affect the visibility of small letters during active head rotations and sharp accelerations. Adaptation is likely to reduce this perceived image motion. No evidence for postural instability or motion sickness was noted as a result of these conflicts between vis-ual and vestibular inputs. (3) Small displacements of the image are noted even without head motion, resulting from eye movements and the virtual lack of display persiste ce. These movements are noticed sponta e ously by few observers and are unlikely to interfere with the display use in most tasks."
            },
            {
                "reference_title": "The Emerging Technology of Cyberspace",
                "reference_link": "publication/247866741_The_Emerging_Technology_of_Cyberspace",
                "reference_type": "Article",
                "reference_date": "Jan 1991",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Simbionte -\u00c9tica da computa\u00e7\u00e3o Symbiont -Ethics of computing Simbionte -\u00c9tica de la computaci\u00f3n",
        "date": "December 2020",
        "doi": "10.33448/rsd-v9i11.11273",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (13)",
        "abstract": "Resumo Neste trabalho se encontra uma conex\u00e3o entre a educa\u00e7\u00e3o matem\u00e1tica e a arquitetura do conhecimento da computa\u00e7\u00e3o, na no\u00e7\u00e3o de sistema formal. O que contamos aqui \u00e9 a hist\u00f3ria do fracasso do bloqueio da simbiose de sistemas de pensamento heterog\u00eaneos. Essa \u00e9 a hist\u00f3ria da computa\u00e7\u00e3o, como momento talvez culminante de um projeto de unifica\u00e7\u00e3o dos sistemas de pensamento. Partimos da transi\u00e7\u00e3o observ\u00e1vel entre a produ\u00e7\u00e3o de autoevid\u00eancia, conven\u00e7\u00e3o da matem\u00e1tica escolar cl\u00e1ssica, e a realiza\u00e7\u00e3o da m\u00e1quina de calcular. Num segundo momento caminhamos na dire\u00e7\u00e3o da constru\u00e7\u00e3o do conhecimento teorem\u00e1tico, que se prolonga na no\u00e7\u00e3o de m\u00e1quina configur\u00e1vel. Essa \u00e9 a experi\u00eancia comum dos sistemas computacionais. O trabalho ent\u00e3o se debru\u00e7a sobre a terceira etapa, que corresponde \u00e0 conex\u00e3o poss\u00edvel entre o ideal de formaliza\u00e7\u00e3o matem\u00e1tica e o conceito de linguagem computacional, que est\u00e1 na origem do conhecimento da ci\u00eancia da computa\u00e7\u00e3o, e que desemboca no \u00faltimo est\u00e1gio do percurso: o campo problem\u00e1tico dos sistemas autorreferentes, em que a tentativa de consubstanciar a recusa \u00e0 simbiose dos sistemas de pensamento heterog\u00eaneos entra em uma deriva que a far\u00e1 fracassar. O desenvolvimento conceitual assim engendrado acaba funcionando como conex\u00e3o com o exterior do pensamento. Um exterior que imp\u00f5e a considera\u00e7\u00e3o de que, \u00e0 semelhan\u00e7a da psican\u00e1lise, o que resta do fracasso da computa\u00e7\u00e3o como disciplina \u00e9 uma \u00e9tica. Palavras-chave: Experi\u00eancia computacional; Filosofia da computa\u00e7\u00e3o; Ci\u00eancia situada.",
        "reference": [
            {
                "reference_title": "Cin\u00e9ma, tome 2. L'Image-temps",
                "reference_link": "publication/320181892_Cinema_tome_2_L'Image-temps",
                "reference_type": "Book",
                "reference_date": "Jan 1985",
                "reference_abstract": null
            },
            {
                "reference_title": "A New Kind of Science",
                "reference_link": "publication/269880662_A_New_Kind_of_Science",
                "reference_type": "Article",
                "reference_date": "Nov 2003",
                "reference_abstract": null
            },
            {
                "reference_title": "On Non\u2010Computable Functions",
                "reference_link": "publication/242646656_On_Non-Computable_Functions",
                "reference_type": "Article",
                "reference_date": "May 1962",
                "reference_abstract": "The construction of non-computable functions used in this paper is based on the principle that a finite, non-empty set of non-negative integers has a largest element. Also, this principle is used only for sets which are exceptionally well-defined by current standards. No enumeration of computable functions is used, and in this sense the diagonal process is not employed. Thus, it appears that an apparently self-evident principle, of constant use in every area of mathematics, yields non-constructive entities."
            },
            {
                "reference_title": "Reassembling the Social",
                "reference_link": "publication/224892364_Reassembling_the_Social",
                "reference_type": "Chapter",
                "reference_date": "Jan 2005",
                "reference_abstract": null
            },
            {
                "reference_title": "On the Origin of Objects",
                "reference_link": "publication/220695197_On_the_Origin_of_Objects",
                "reference_type": "Book",
                "reference_date": "Jan 1996",
                "reference_abstract": null
            },
            {
                "reference_title": "Post-scriptum sobre las sociedades de control",
                "reference_link": "publication/26462816_Post-scriptum_sobre_las_sociedades_de_control",
                "reference_type": "Article",
                "reference_date": "Jan 2006",
                "reference_abstract": "La tesis central de este art\u00edculo es que \u00bflos centros de encierro\u00bf disciplinarios descritas por Foucault: \u00bfc\u00e1rcel, hospital, f\u00e1brica, escuela, familia, atraviesan una crisis generalizada\u00bf. Vivimos la decadencia de la \u00bfsociedad disciplinaria\u00bf, que fue \u00bfla sucesora de las sociedades de soberan\u00eda\u00bf, cuyos fines y funciones eran completamente distintos. Estas surgieron en los siglos XVII y XVIII hasta mediados del XX, y fueron el tema central de las investigaciones de Foucault. La sociedad actual es denominada como \u00bfsociedad de control\u00bf y \u00e9ste se ejerce fluidamente en espacios abiertos, en forma desterritorializada, mediante los psico-f\u00e1rmacos, el consumo televisivo, el marketing, el endeudamiento privado, el consumo, entre otras modalidades. Lo esencial en ellas son las cifras fluctuantes e intercambiables como las que muestran el valor de una moneda en las otras, el movimiento incesante del surf que sustituye los deportes lentos y estrat\u00e9gicos como el box. Las f\u00e1bricas son reemplazadas por las empresas, que son formaciones d\u00factiles y cambiantes, las m\u00e1quinas simples por sistemas computarizados de producci\u00f3n y control. La in-dividualidad es sustituida por \u00bfdivuales\u00bf externos, informatizados e informatizables, que se desplazan en un espacio virtual."
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 1990",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 1992",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2015",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 1945",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "TechTexC: Classification of Technical Texts using Convolution and Bidirectional Long Short Term Memory Network",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (10)",
        "abstract": "This paper illustrates the details description of technical text classification system and its results that developed as a part of participation in the shared task TechDofication 2020. The shared task consists of two sub-tasks: (i) first task identify the coarse-grained technical domain of given text in a specified language and (ii) the second task classify a text of computer science domain into fine-grained sub-domains. A classification system (called 'TechTexC') is developed to perform the classification task using three techniques: convolution neural network (CNN), bidirectional long short term memory (BiLSTM) network, and combined CNN with BiLSTM. Results show that CNN with BiLSTM model outperforms the other techniques concerning task-1 of sub-tasks (a, b, c and g) and task-2a. This combined model obtained f1 scores of 82.63 (sub-task a), 81.95 (sub-task b), 82.39 (sub-task c), 84.37 (sub-task g), and 67.44 (task-2a) on the development dataset. Moreover, in the case of test set, the combined CNN with BiLSTM approach achieved that higher accuracy for the subtasks 1a (70.76%), 1b (79.97%), 1c (65.45%), 1g (49.23%) and 2a (70.14%).",
        "reference": [
            {
                "reference_title": "Distributed Representations of Words and Phrases and their Compositionality",
                "reference_link": "publication/257882504_Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality",
                "reference_type": "Article",
                "reference_date": "Oct 2013",
                "reference_abstract": "The recently introduced continuous Skip-gram model is an efficient method for\nlearning high-quality distributed vector representations that capture a large\nnumber of precise syntactic and semantic word relationships. In this paper we\npresent several extensions that improve both the quality of the vectors and the\ntraining speed. By subsampling of the frequent words we obtain significant\nspeedup and also learn more regular word representations. We also describe a\nsimple alternative to the hierarchical softmax called negative sampling. An\ninherent limitation of word representations is their indifference to word order\nand their inability to represent idiomatic phrases. For example, the meanings\nof \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\".\nMotivated by this example, we present a simple method for finding phrases in\ntext, and show that learning good vector representations for millions of\nphrases is possible."
            },
            {
                "reference_title": "Understanding Convolutional Neural Networks for Text Classification",
                "reference_link": "publication/334115395_Understanding_Convolutional_Neural_Networks_for_Text_Classification",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Combining Contents and Citations for Scientific Document Classification",
                "reference_link": "publication/220935628_Combining_Contents_and_Citations_for_Scientific_Document_Classification",
                "reference_type": "Conference Paper",
                "reference_date": "Dec 2005",
                "reference_abstract": "This paper introduces a classification system that exploits the content information as well as citation structure for scientific\npaper classification. The system first applies a content-based statistical classification method which is similar to general\ntext classification. We investigate several classification methods including K-nearest neighbours, nearest centroid, naive\nBayes and decision trees. Among those methods, the K-nearest neighbours is found to outperform others while the rest perform\ncomparably. Using phrases in addition to words and a good feature selection strategy such as information gain can improve\nsystem accuracy and reduce training time in comparison with using words only. To combine citation links for classification,\nthe system proposes an iterative method to update the labellings of classified instances using citation links. Our results\nshow that, combining contents and citations significantly improves the system performance."
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2020",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2020",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2014",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": null,
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2017",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2015",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2016",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Self-Supervised Learning for Visual Summary Identification in Scientific Publications",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (30)",
        "abstract": "Providing visual summaries of scientific publications can increase information access for readers and thereby help deal with the exponential growth in the number of scientific publications. Nonetheless, efforts in providing visual publication summaries have been few and fart apart, primarily focusing on the biomedical domain. This is primarily because of the limited availability of annotated gold standards, which hampers the application of robust and high-performing supervised learning techniques. To address these problems we create a new benchmark dataset for selecting figures to serve as visual summaries of publications based on their abstracts, covering several domains in computer science. Moreover, we develop a self-supervised learning approach, based on heuristic matching of inline references to figures with figure captions. Experiments in both biomedical and computer science domains show that our model is able to outperform the state of the art despite being self-supervised and therefore not relying on any annotated training data.",
        "reference": [
            {
                "reference_title": "Picturing Science: Design Patterns in Graphical Abstracts",
                "reference_link": "publication/325186940_Picturing_Science_Design_Patterns_in_Graphical_Abstracts",
                "reference_type": "Chapter",
                "reference_date": "Jan 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Identifying the Central Figure of a Scientific Paper",
                "reference_link": "publication/339023675_Identifying_the_Central_Figure_of_a_Scientific_Paper",
                "reference_type": "Conference Paper",
                "reference_date": "Sep 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Original Semantics-Oriented Attention and Deep Fusion Network for Sentence Matching",
                "reference_link": "publication/336997170_Original_Semantics-Oriented_Attention_and_Deep_Fusion_Network_for_Sentence_Matching",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "SciBERT: A Pretrained Language Model for Scientific Text",
                "reference_link": "publication/336995622_SciBERT_A_Pretrained_Language_Model_for_Scientific_Text",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Argument Mining for Understanding Peer Reviews",
                "reference_link": "publication/334601696_Argument_Mining_for_Understanding_Peer_Reviews",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Investigating the Role of Argumentation in the Rhetorical Analysis of Scientific Publications with Neural Multi-Task Learning Models",
                "reference_link": "publication/334115745_Investigating_the_Role_of_Argumentation_in_the_Rhetorical_Analysis_of_Scientific_Publications_with_Neural_Multi-Task_Learning_Models",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Extracting Scientific Figures with Distantly Supervised Neural Networks",
                "reference_link": "publication/325495228_Extracting_Scientific_Figures_with_Distantly_Supervised_Neural_Networks",
                "reference_type": "Conference Paper",
                "reference_date": "May 2018",
                "reference_abstract": "Non-textual components such as charts, diagrams and tables provide key information in many scientific documents, but the lack of large labeled datasets has impeded the development of data-driven methods for scientific figure extraction. In this paper, we induce high-quality training labels for the task of figure extraction in a large number of scientific documents, with no human intervention. To accomplish this we leverage the auxiliary data provided in two large web collections of scientific documents (arXiv and PubMed) to locate figures and their associated captions in the rasterized PDF. We share the resulting dataset of over 5.5 million induced labels---4,000 times larger than the previous largest figure extraction dataset---with an average precision of 96.8%, to enable the development of modern data-driven methods for this task. We use this dataset to train a deep neural network for end-to-end figure detection, yielding a model that can be more easily extended to new domains compared to previous work. The model was successfully deployed in Semantic Scholar,\\footnote\\urlhttps://www.semanticscholar.org/ a large-scale academic search engine, and used to extract figures in 13 million scientific documents.\\footnoteA demo of our system is available at \\urlhttp://labs.semanticscholar.org/deepfigures/,and our dataset of induced labels can be downloaded at \\urlhttps://s3-us-west-2.amazonaws.com/ai2-s2-research-public/deepfigures/jcdl-deepfigures-labels.tar.gz. Code to run our system locally can be found at \\urlhttps://github.com/allenai/deepfigures-open."
            },
            {
                "reference_title": "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents",
                "reference_link": "publication/325447347_A_Discourse-Aware_Attention_Model_for_Abstractive_Summarization_of_Long_Documents",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Construction of the Literature Graph in Semantic Scholar",
                "reference_link": "publication/325445691_Construction_of_the_Literature_Graph_in_Semantic_Scholar",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Bilateral Multi-Perspective Matching for Natural Language Sentences",
                "reference_link": "publication/318829716_Bilateral_Multi-Perspective_Matching_for_Natural_Language_Sentences",
                "reference_type": "Conference Paper",
                "reference_date": "Aug 2017",
                "reference_abstract": "Natural language sentence matching is a fundamental technology for a variety of tasks. Previous approaches either match sentences from a single direction or only apply single granular (word-by-word or sentence-by-sentence) matching. In this work, we propose a bilateral multi-perspective matching (BiMPM) model. Given two sentences P and Q, our model first encodes them with a BiLSTM encoder. Next, we match the two encoded sentences in two directions P against Q and P against Q. In each matching direction, each time step of one sentence is matched against all time-steps of the other sentence from multiple perspectives. Then, another BiLSTM layer is utilized to aggregate the matching results into a fix-length matching vector. Finally, based on the matching vector, a decision is made through a fully connected layer. We evaluate our model on three tasks: paraphrase identification, natural language inference and answer sentence selection. Experimental results on standard benchmark datasets show that our model achieves the state-of-the-art performance on all tasks."
            }
        ]
    },
    {
        "title": "League of Brazilian Bioinformatics League of Brazilian Bioinformatics: a competition framework to promote scientific training",
        "date": "December 2020",
        "doi": "10.1101/2020.12.17.423357",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (18)",
        "abstract": "Background\nthe scientific training to become a bioinformatician includes multidisciplinary abilities, which increase the challenges to professional development.\n\nCompetition framework\nin order to improve and promote the ongoing training of the Brazilian bioinformatics community, we organize a national competition, with the main goal to develop human resources and abilities in Computational Biology at the national level. The competition framework was designed in three phases: 1) a one-day challenge composed of 60 multiple-choice questions covering Biology, Computer Science, and Bioinformatics knowledge; 2) five Computational Biology challenges to be solved in three days; and 3) development of an original project evaluated during the 15th X-meeting.\n\nResults\nthe first edition of the League of Brazilian Bioinformatics (LBB) counted 168 competitors and 59 groups, distributed into undergraduate students (14.4%), graduate students (12.6% master and 16.8%, Ph.D.), and other professional fields. The first phase selected 46 teams to proceed in the competition, while the second phase selected the three top-performing teams.\n\nConclusion\nduring the competition, we were able to stimulate teamwork in the main areas of Bioinformatics, with the engagement of all research-level competitors. Furthermore, we identified opportunities to deliver and offer better training to the community and we intend to apply the acquired experience in the second edition of the LBB, which will occur in 2021.\n\nSupplementary information\nSupplementary data are available at Bioinformatics",
        "reference": [
            {
                "reference_title": "The CAFA challenge reports improved protein function prediction and new functional annotations for hundreds of genes through experimental screens",
                "reference_link": "publication/337370188_The_CAFA_challenge_reports_improved_protein_function_prediction_and_new_functional_annotations_for_hundreds_of_genes_through_experimental_screens",
                "reference_type": "Article",
                "reference_date": "Nov 2019",
                "reference_abstract": "Background: \nThe Critical Assessment of Functional Annotation (CAFA) is an ongoing, global, community-driven effort to evaluate and improve the computational annotation of protein function.\n\nResults: \nHere, we report on the results of the third CAFA challenge, CAFA3, that featured an expanded analysis over the previous CAFA rounds, both in terms of volume of data analyzed and the types of analysis performed. In a novel and major new development, computational predictions and assessment goals drove some of the experimental assays, resulting in new functional annotations for more than 1000 genes. Specifically, we performed experimental whole-genome mutation screening in Candida albicans and Pseudomonas aureginosa genomes, which provided us with genome-wide experimental data for genes associated with biofilm formation and motility. We further performed targeted assays on selected genes in Drosophila melanogaster, which we suspected of being involved in long-term memory.\n\nConclusion: \nWe conclude that while predictions of the molecular function and biological process annotations have slightly improved over time, those of the cellular component have not. Term-centric prediction of experimental annotations remains equally challenging; although the performance of the top methods is significantly better than the expectations set by baseline methods in C. albicans and D. melanogaster, it leaves considerable room and need for improvement. Finally, we report that the CAFA community now involves a broad range of participants with expertise in bioinformatics, biological experimentation, biocuration, and bio-ontologies, working together to improve functional annotation, computational function prediction, and our ability to manage big data in the era of large experimental screens."
            },
            {
                "reference_title": "NCBI\u2019s Virus Discovery Hackathon: Engaging Research Communities to Identify Cloud Infrastructure Requirements",
                "reference_link": "publication/335848366_NCBI's_Virus_Discovery_Hackathon_Engaging_Research_Communities_to_Identify_Cloud_Infrastructure_Requirements",
                "reference_type": "Article",
                "reference_date": "Sep 2019",
                "reference_abstract": "A wealth of viral data sits untapped in publicly available metagenomic data sets when it might be extracted to create a usable index for the virological research community. We hypothesized that work of this complexity and scale could be done in a hackathon setting. Ten teams comprised of over 40 participants from six countries, assembled to create a crowd-sourced set of analysis and processing pipelines for a complex biological data set in a three-day event on the San Diego State University campus starting 9 January 2019. Prior to the hackathon, 141,676 metagenomic data sets from the National Center for Biotechnology Information (NCBI) Sequence Read Archive (SRA) were pre-assembled into contiguous assemblies (contigs) by NCBI staff. During the hackathon, a subset consisting of 2953 SRA data sets (approximately 55 million contigs) was selected, which were further filtered for a minimal length of 1 kb. This resulted in 4.2 million (Mio) contigs, which were aligned using BLAST against all known virus genomes, phylogenetically clustered and assigned metadata. Out of the 4.2 Mio contigs, 360,000 contigs were labeled with domains and an additional subset containing 4400 contigs was screened for virus or virus-like genes. The work yielded valuable insights into both SRA data and the cloud infrastructure required to support such efforts, revealing analysis bottlenecks and possible workarounds thereof. Mainly: (i) Conservative assemblies of SRA data improves initial analysis steps; (ii) existing bioinformatic software with weak multithreading/multicore support can be elevated by wrapper scripts to use all cores within a computing node; (iii) redesigning existing bioinformatic algorithms for a cloud infrastructure to facilitate its use for a wider audience; and (iv) a cloud infrastructure allows a diverse group of researchers to collaborate effectively. The scientific findings will be extended during a follow-up event. Here, we present the applied workflows, initial results, and lessons learned from the hackathon."
            },
            {
                "reference_title": "The Topology Prediction of Membrane Proteins: A Web-Based Tutorial",
                "reference_link": "publication/308925520_The_Topology_Prediction_of_Membrane_Proteins_A_Web-Based_Tutorial",
                "reference_type": "Article",
                "reference_date": "Oct 2016",
                "reference_abstract": "There is a great need for development of educational materials on the transfer of current bioinformatics knowledge to undergraduate students in bioscience departments. In this study, it is aimed to prepare an example in silico laboratory tutorial on the topology prediction of membrane proteins by bioinformatics tools. This laboratory tutorial is prepared for biochemistry lessons at bioscience departments (biology, chemistry, biochemistry, molecular biology and genetics, and faculty of medicine). The tutorial is intended for students who have not taken a bioinformatics course yet or already have taken a course as an introduction to bioinformatics. The tutorial is based on step-by-step explanations with illustrations. It can be applied under supervision of an instructor in the lessons, or it can be used as a self-study guide by students. In the tutorial, membrane-spanning regions and \u03b1-helices of membrane proteins were predicted by internet-based bioinformatics tools. According to the results achieved from internet-based bioinformatics tools, the algorithms and parameters used were effective on the accuracy of prediction. The importance of this laboratory tutorial lies on the facts that it provides an introduction to the bioinformatics and that it also demonstrates an in silico laboratory application to the students at natural sciences. The presented example education material is applicable easily at all departments that have internet connection. This study presents an alternative education material to the students in biochemistry laboratories in addition to classical laboratory experiments."
            },
            {
                "reference_title": "Healthcare Hackathons Provide Educational and Innovation Opportunities: A Case Study and Best Practice Recommendations",
                "reference_link": "publication/303870354_Healthcare_Hackathons_Provide_Educational_and_Innovation_Opportunities_A_Case_Study_and_Best_Practice_Recommendations",
                "reference_type": "Article",
                "reference_date": "Jul 2016",
                "reference_abstract": "Physicians and other healthcare professionals are often the end users of medical innovation; however, they are rarely involved in the beginning design stages. This often results in ineffective healthcare solutions with poor adoption rates. At the early design stage, innovation would benefit from input from healthcare professionals. This report describes the first-ever rehabilitation hackathon\u2014an interdisciplinary and competitive team event aimed at accelerating and improving healthcare solutions and providing an educational experience for participants. Hackathons are gaining traction as a way to accelerate innovation by bringing together a diverse group of interdisciplinary professionals from different industries who work collaboratively in teams and learn from each other, focus on a specific problem (\u201cpain point\u201d), develop a solution using design thinking techniques, pitch the solution to participants, gather fast feedback and quickly alter the prototype design (\u201cpivoting\u201d). 102 hackers including 19 (18.6 %) physicians and other professionals participated, and over the course of 2 days worked in teams, pitched ideas and developed design prototypes. Three awards were given for prototypes that may improve function in persons with disabilities. 43 hackers were women (42.2 %) and 59 men (57.8 %); they ranged in age from 16 to 79 years old; and, of the 75 hackers who reported their age, 63 (84 %) were less than 40 years old and 12 (16 %) were 40 years or older. This report contributes to the emerging literature on healthcare hackathons as a means of providing interdisciplinary education and training and supporting innovation."
            },
            {
                "reference_title": "An application of item response theory to psychological test development",
                "reference_link": "publication/303478941_An_application_of_item_response_theory_to_psychological_test_development",
                "reference_type": "Article",
                "reference_date": "Dec 2016",
                "reference_abstract": "Item response theory (IRT) has become a popular methodological framework for modeling response data from assessments in education and health; however, its use is not widespread among psychologists. This paper aims to provide a didactic application of IRT and to highlight some of these advantages for psychological test development. IRT was applied to two scales (a positive and a negative affect scale) of a self-report test. Respondents were 853 university students (57 % women) between the ages of 17 and 35 and who answered the scales. IRT analyses revealed that the positive affect scale has items with moderate discrimination and are measuring respondents below the average score more effectively. The negative affect scale also presented items with moderate discrimination and are evaluating respondents across the trait continuum; however, with much less precision. Some features of IRT are used to show how such results can improve the measurement of the scales. The authors illustrate and emphasize how knowledge of the features of IRT may allow test makers to refine and increase the validity and reliability of other psychological measures."
            },
            {
                "reference_title": "Learning Nucleic Acids Solving by Bioinformatics Problems",
                "reference_link": "publication/280868044_Learning_Nucleic_Acids_Solving_by_Bioinformatics_Problems",
                "reference_type": "Article",
                "reference_date": "Aug 2015",
                "reference_abstract": "The article describes the development of a new approach to teach molecular biology to undergraduate biology students. The 34 students who participated in this research belonged to the first period of the Biological Sciences teaching course of the Instituto Federal Goiano at Uruta\u00ed Campus, Brazil. They were registered in Cell Biology in the first semester of 2013. They received four 55 min-long expository/dialogued lectures that covered the content of \"structure and functions of nucleic acids\". Later the students were invited to attend four meetings (in a computer laboratory) in which some concepts of Bioinformatics were presented and some problems of the Rosalind platform were solved. The observations we report here are very useful as a broad groundwork to development new research. An interesting possibility is research into the effects of bioinformatics interventions that improve molecular biology learning. \u00a9 2015 by The International Union of Biochemistry and Molecular Biology, 2015.\n\u00a9 2015 The International Union of Biochemistry and Molecular Biology."
            },
            {
                "reference_title": "Health hackathon as a venue for interprofessional education: a qualitative interview study",
                "reference_link": "publication/338106979_Health_hackathon_as_a_venue_for_interprofessional_education_a_qualitative_interview_study",
                "reference_type": "Article",
                "reference_date": "Dec 2019",
                "reference_abstract": "A Health Hackathon provides an opportunity for healthcare professionals to collaborate with IT developers and designers to solve health issues using technology and thus serves as a potential venue for interprofessional education. The present paper reports the views and experiences of participants on how the KKU mHealth Hackathon 2017 served as a venue for interprofessional education. A phenomenological approach was used involving semi-structured in-depth interviews of three faculty members and three students who participated in the hackathon. Participants expressed their learning experiences during the event, as well as factors that promoted or hindered learning. Our findings suggest that a health hackathon can serve as a suitable venue for interprofessional education as interviewees reported how they had learnt to successfully collaborate in interprofessional teams, move beyond their prior views and appreciate complementary work from other professions, focus on solving problems practically, and create a collegial, collaborative atmosphere. There were also some potential downsides of the hackathon that could be solved with an improved design in future occasions. A Health Hackathon can be an important opportunity for interprofessional education. Further studies should focus on methods to reproduce these positive learning experiences, mitigate the negative aspects, and investigate their long-term effects."
            },
            {
                "reference_title": "Introducing Programming Skills for Life Science Students",
                "reference_link": "publication/331687595_Introducing_Programming_Skills_for_Life_Science_Students",
                "reference_type": "Article",
                "reference_date": "Mar 2019",
                "reference_abstract": "The advent of the high\u2010throughput next\u2010generation sequencing produced a large number of biological data. Knowledge discovery from the huge amount of available biological data requires researchers to develop solid skills in biology and computer science. As the majority of the Bioinformatics professionals are either computer science or life sciences graduates, to teach biology skills to computer science students and computational skills to life science students has become usual. In this article, we reported the experience of teaching programming for life science students. Our strategy is composed by explaining basic concepts of algorithms, abstraction of biological problems, and script programming using Python language. Based on the student's answers to an assessment questionnaire, we conclude that the course achieved positive results. They reported an improvement in their skills in programming and bioinformatics. Furthermore, the students approved the didactic adopted in the classes and evaluation methods (programming exercises and final presentation). This article is useful for other professors who want to implement an initial bioinformatics training for undergraduate or graduate students in life sciences. We believe that the strategies here demonstrated could be reproduced, which could help in the formation of a new generation of bioinformaticians with hybrid abilities in computation and biology."
            },
            {
                "reference_title": "An Extended Hackathon Model for Collaborative Education in Medical Innovation",
                "reference_link": "publication/328344518_An_Extended_Hackathon_Model_for_Collaborative_Education_in_Medical_Innovation",
                "reference_type": "Article",
                "reference_date": "Dec 2018",
                "reference_abstract": "To support the next generation of healthcare innovators - whether they be engineers, designers, clinicians, or business experts by training \u2013 education in the emerging field of medical innovation should be made easily and widely accessible to undergraduate students, graduate students, and young professionals, early in their careers. Currently, medical innovation curricula are taught through semester-long courses or year-long fellowships at a handful of universities, reaching only a limited demographic of participants. This study describes the structure and preliminary outcomes of a 1\u20132 week \u201cextended hackathon\u201d course that seeks to make medical innovation education and training more accessible and easily adoptable for academic medical centers. Eight extended hackathons were hosted in five international locations reaching 245 participants: Beijing (June 2015 and August 2016), Hong Kong (June 2016, 2017, and 2018), Curitiba (July 2016), Stanford (October 2017), and S\u00e3o Paulo (May 2018). Pre- and post-hackathon surveys asking respondents to self-assess their knowledge in ten categories of medical innovation were administered to quantify the perceived degree of learning. Participants hailed from a diverse range of educational backgrounds, domains of expertise, and academic institutions. On average, respondents (n = 161) saw a greater than twofold increase (114.1%, P < 0.001) from their pre- to post-hackathon scores. In this study, the extended hackathon is presented as a novel educational model to teach undergraduate and graduate students a foundational skillset for medical innovation. Participants reported gaining significant knowledge across all ten categories assessed. To more robustly assess the educational value of extended hackathons, a standardized assessment for medical innovation knowledge needs to be developed, and a larger sample size of participants surveyed."
            },
            {
                "reference_title": "Item response theory for measurement validity",
                "reference_link": "publication/279598567_Item_response_theory_for_measurement_validity",
                "reference_type": "Article",
                "reference_date": "Jun 2014",
                "reference_abstract": "Item response theory (IRT) is an important method of assessing the validity of measurement scales that is underutilized in the field of psychiatry. IRT describes the relationship between a latent trait (e.g., the construct that the scale proposes to assess), the properties of the items in the scale, and respondents' answers to the individual items. This paper introduces the basic premise, assumptions, and methods of IRT. To help explain these concepts we generate a hypothetical scale using three items from a modified, binary (yes/no) response version of the Center for Epidemiological Studies-Depression scale that was administered to 19, 399 respondents. We first conducted a factor analysis to confirm the unidimensionality of the three items and then proceeded with Mplus software to construct the 2-Parameter Logic (2-PL) IRT model of the data, a method which allows for estimates of both item discrimination and item difficulty. The utility of this information both for clinical purposes and for scale construction purposes is discussed. Copyright\u00a9 2014 by Editorial Department of the Shanghai Archives of Psychiatry."
            }
        ]
    },
    {
        "title": "Staying Safe in COVID-19",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References:",
        "abstract": "We protect the community. We protect ourselves. We decongest the health system. We stay safe in COVID-19. One of the many responses to the global call against the world pandemic of COVID-19 resulted in \u201cSafe in COVID-19\u201d, an electronic platform developed by the Institute of Computer Science of the Foundation for Research and Technology \u2013 Hellas (FORTH-ICS), which is intended for tracing suspect, probable and confirmed incidence cases.",
        "reference": []
    },
    {
        "title": "O_n$ is an $n$-MCFL",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (22)",
        "abstract": "Commutative properties in formal languages pose problems at the frontier of computer science, computational linguistics and computational group theory. A prominent problem of this kind is the position of the language $O_n$, the language that contains the same number of letters $a_i$ and $\\bar a_i$ with $1\\leq i\\leq n$, in the known classes of formal languages. It has recently been shown that $O_n$ is a Multiple Context-Free Language (MCFL). However the more precise conjecture of Nederhof that $O_n$ is an MCFL of dimension $n$ was left open. We present two proofs of this conjecture, both relying on tools from algebraic topology. On our way, we prove a variant of the necklace splitting theorem.",
        "reference": [
            {
                "reference_title": "The word problem of $\\mathbb{Z}^n$ is a multiple context-free language",
                "reference_link": "publication/313527029_The_word_problem_of_mathbbZn_is_a_multiple_context-free_language",
                "reference_type": "Article",
                "reference_date": "Feb 2017",
                "reference_abstract": "The \\emph{word problem} of a group $G = \\langle \\Sigma \\rangle$ can be defined as the set of formal words in $\\Sigma^*$ that represent the identity in $G$. When viewed as formal languages, this gives a strong connection between classes of groups and classes of formal languages. For example, Anisimov showed that a group is finite if and only if its word problem is a regular language, and Muller and Schupp showed that a group is virtually-free if and only if its word problem is a context-free language. Above this, not much was known, until Salvati showed recently that the word problem of $\\mathbb{Z}^2$ is multiple context-free, giving first such example. We generalize Salvati's result to show that the word problem of $\\mathbb{Z}^n$ is a multiple context-free language for any $n$."
            },
            {
                "reference_title": "A short proof that O_2 is an MCFL",
                "reference_link": "publication/306093779_A_short_proof_that_O_2_is_an_MCFL",
                "reference_type": "Conference Paper",
                "reference_date": "Mar 2016",
                "reference_abstract": "We present a new proof that $O_2$ is a multiple context-free language. It contrasts with a recent proof by Salvati (2015) in its avoidance of concepts that seem specific to two-dimensional geometry, such as the complex exponential function. Our simple proof creates realistic prospects of widening the results to higher dimensions. This finding is of central importance to the relation between extreme free word order and classes of grammars used to describe the syntax of natural language."
            },
            {
                "reference_title": "The Convergence of Mildly Context-Sensitive Grammar Formalisms",
                "reference_link": "publication/303969126_The_Convergence_of_Mildly_Context-Sensitive_Grammar_Formalisms",
                "reference_type": "Article",
                "reference_date": "Jan 1990",
                "reference_abstract": "Investigations of classes of grammars that are nontransformational and at the same time highly constrained are of interest both linguistically and mathematically. Context-free grammars (CFG) obviously form such a class. CFGs are not adequate (both weakly and strongly) to characterize some aspects of language structure. Thus how much more power beyond CFG is necessary to describe these phenomena is an important question. Based on certain properties of tree adjoining grammars (TAG) an approximate characterization of class of grammars, mildly context-sensitive grammars (MCSG), has been proposed earlier. In this paper, we have described the relationship between several different grammar formalisms, all of which belong to MCSG. In particular, we have shown that head grammars (HG), combinatory categorial grammars (CCG), and linear indexed grammars (LIG) and TAG are all weakly equivalent. These formalisms are all distinct from each other at least in the following aspects: (a) the formal objects and operations in each formalism, (b) the domain of locality over which dependencies are specified, (c) the degree to which recursion and the domain of dependencies are factored, and (d) the linguistic insights that are captured in the formal objects and operations in each formalism. A deeper understanding of this convergence is obtained by comparing these formalisms at the level of the derivation structures in each formalism. We have described a formalism, the linear context-free rewriting system (LCFR), as a first attempt to capture the closeness of the derivation structures of these formalisms. LCFRs thus make the notion of MCSGs more precise. We have shown that LCFRs are equivalent to muticomponent tree adjoining grammars (MCTAGs), and also briefly discussed some variants of TAGs, lexicalized TAGs, feature structure based TAGs, and TAGs in which local domination and linear precedence are factored TAG(LD/LP)."
            },
            {
                "reference_title": "MIX is a 2-MCFL and the word problem in Z 2 is captured by the IO and the OI hierarchies",
                "reference_link": "publication/274095313_MIX_is_a_2-MCFL_and_the_word_problem_in_Z_2_is_captured_by_the_IO_and_the_OI_hierarchies",
                "reference_type": "Article",
                "reference_date": "Mar 2015",
                "reference_abstract": "In this work we establish that the language and the language are 2-MCFLs. As 2-MCFLs form a class of languages that is included in both the IO and OI hierarchies, and as is the group language of a simple presentation of we exhibit here the first, to our knowledge, non-virtually-free group language (i.e. non-context-free group language) that is captured by the IO and OI hierarchies. Moreover, it was a long-standing open problem whether MIX was a mildly context sensitive language or not, and it was conjectured that it was not, so we close this conjecture by giving it a negative answer."
            },
            {
                "reference_title": "A Moment Problem in L 1 Approximation",
                "reference_link": "publication/270297876_A_Moment_Problem_in_L_1_Approximation",
                "reference_type": "Article",
                "reference_date": "Aug 1965",
                "reference_abstract": null
            },
            {
                "reference_title": "Groups, the Theory of ends, and context-free languages",
                "reference_link": "publication/265428651_Groups_the_Theory_of_ends_and_context-free_languages",
                "reference_type": "Article",
                "reference_date": "Jun 1983",
                "reference_abstract": null
            },
            {
                "reference_title": "Formal languages and their application to combinatorial group theory",
                "reference_link": "publication/265308127_Formal_languages_and_their_application_to_combinatorial_group_theory",
                "reference_type": "Article",
                "reference_date": "Jan 2005",
                "reference_abstract": null
            },
            {
                "reference_title": "MIX is not a tree-adjoining language",
                "reference_link": "publication/262401626_MIX_is_not_a_tree-adjoining_language",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2012",
                "reference_abstract": "The language MIX consists of all strings over the three-letter alphabet {a, b, c} that contain an equal number of occurrences of each letter. We prove Joshi's (1985) conjecture that MIX is not a tree-adjoining language."
            },
            {
                "reference_title": "On multiple context-free grammars",
                "reference_link": "publication/256555220_On_multiple_context-free_grammars",
                "reference_type": "Article",
                "reference_date": "Oct 1991",
                "reference_abstract": "Multiple context-free grammars (mcfg's) is a subclass of generalized context-free grammars introduced by Pollard (1984) in order to describe the syntax of natural languages. The class of languages generated by mcfg's (called multiple context-free languages or, shortly, mcfl's) properly includes the class of context-free languages and is properly included in the class of context-sensitive languages. First, the paper presents results on the generative capacity of mcfg's and also on the properties of mcfl's such as formal language-theoretic closure properties. Next, it is shown that the time complexity of the membership problem for multiple context-free languages is O(ne), where n is the length of an input string and e is a constant called the degree of a given mcfg. Head grammars (hg's) introduced by Pollard and tree adjoining grammars (tag's) introduced by Joshi et al. (1975) are also grammatical formalisms to describe the syntax of natural languages. The paper also presents the following results on the generative capacities of hg's, tag's and 2-mcfg's, which are a subclass of mcfg's: (1) The class HL of languages generated by hg's is the same as the one generated by tag's; (2) HL is the same as the one generated by left-wrapping hg's (or right-wrapping hg's) which is a proper subclass of hg's; (3) HL is properly included in the one generated by 2-mcfg's. As a corollary of (1), it is also shown that HL is a substitution-closed full AFL."
            },
            {
                "reference_title": "The Borsuk-Ulam Theorem and Bisection of Necklaces",
                "reference_link": "publication/247792365_The_Borsuk-Ulam_Theorem_and_Bisection_of_Necklaces",
                "reference_type": "Article",
                "reference_date": "Apr 1986",
                "reference_abstract": "The Borsuk-Ulam theorem of topology is applied to a problem in discrete mathematics. A bisection of a necklace with k colors of beads is a collection of intervals whose union captures half the beads of each color. Every necklace with k colors has a bisection formed by at most k cuts. Higher-dimensional generalizations are considered."
            }
        ]
    },
    {
        "title": "AUTONOMOUS LUGGAGE CARRIER HEXAPOD",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (10)",
        "abstract": "In the last few years, there has been a lot of advancements in Robotics and Computer Science, to an extent that Robotic Devices are now included in our daily life and help us in our day to day activities. The more important thing is that this is not the peak, this is just the beginning, there is a lot of research going on in this field with the aim of making our quality of life better. This paper presents a similar project undertaken to help elderly people in carrying their luggage around the airport terminal. This paper presents the methodology and algorithms used to develop the device. The prototype has been tested in open spaces, while simulations have been done to analyze its scalability to accommodate multiple robots coexisting.",
        "reference": [
            {
                "reference_title": "A Bioinspired Gait Transition Model for a Hexapod Robot",
                "reference_link": "publication/327408779_A_Bioinspired_Gait_Transition_Model_for_a_Hexapod_Robot",
                "reference_type": "Article",
                "reference_date": "Sep 2018",
                "reference_abstract": "Inspired by the analysis of the ant locomotion observed by the high-speed camera, an ant-like gait transition model for the hexapod robot is proposed in this paper. The model which consists of the central neural system (CNS), neural network (NN), and central pattern generators (CPGs) can produce the rhythmic signals for different gaits and can realize the transition of these gait automatically and smoothly according to the change of terrain. The proposed model suggests the neural mechanisms of the ant gait transition and can improve the environmental adaptability of the hexapod robot. The numerical simulation and corresponding physical experiment are implemented in this paper to verify the proposed method."
            },
            {
                "reference_title": "A Hexapod Robot with Non-Collocated Actuators",
                "reference_link": "publication/325984619_A_Hexapod_Robot_with_Non-Collocated_Actuators",
                "reference_type": "Article",
                "reference_date": "Jun 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Live video monitoring robot controlled by web over internet",
                "reference_link": "publication/321478845_Live_video_monitoring_robot_controlled_by_web_over_internet",
                "reference_type": "Article",
                "reference_date": "Nov 2017",
                "reference_abstract": "Future is all about robots, robot can perform tasks where humans cannot, Robots have huge applications in military and industrial area for lifting heavy weights, for accurate placements, for repeating the same task number of times, where human are not efficient. Generally robot is a mix of electronic, electrical and mechanical engineering and can do the tasks automatically on its own or under the supervision of humans. The camera is the eye for robot, call as robovision helps in monitoring security system and also can reach into the places where the human eye cannot reach. This paper presents about developing a live video streaming robot controlled from the website. We designed the web, controlling for the robot to move left, right, front and back while streaming video. As we move to the smart environment or IoT (Internet of Things) by smart devices the system we developed here connects over the internet and can be operated with smart mobile phone using a web browser. The Raspberry Pi model B chip acts as heart for this system robot, the sufficient motors, surveillance camera R pi 2 are connected to Raspberry pi."
            },
            {
                "reference_title": "ImageNet Large Scale Visual Recognition Challenge",
                "reference_link": "publication/265295439_ImageNet_Large_Scale_Visual_Recognition_Challenge",
                "reference_type": "Article",
                "reference_date": "Sep 2014",
                "reference_abstract": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in\nobject category classification and detection on hundreds of object categories\nand millions of images. The challenge has been run annually from 2010 to\npresent, attracting participation from more than fifty institutions.\nThis paper describes the creation of this benchmark dataset and the advances\nin object recognition that have been possible as a result. We discuss the\nchallenges of collecting large-scale ground truth annotation, highlight key\nbreakthroughs in categorical object recognition, provide detailed a analysis of\nthe current state of the field of large-scale image classification and object\ndetection, and compare the state-of-the-art computer vision accuracy with human\naccuracy. We conclude with lessons learned in the five years of the challenge,\nand propose future directions and improvements."
            },
            {
                "reference_title": "Locomotion Analysis of Hexapod Robot",
                "reference_link": "publication/221907923_Locomotion_Analysis_of_Hexapod_Robot",
                "reference_type": "Chapter",
                "reference_date": "Mar 2010",
                "reference_abstract": "In this chapter, the locomotion of symmetric hexapods has been studied in detail. We have presented a comprehensive study of hexagonal hexapod gaits including normal and fault tolerant ones. Gaits of rectangular and hexagonal six-legged robots have been compared from several aspects: stability, fault tolerance, terrain adaptability and walking ability. To facilitate simulations and experiments we have provided integrated kinematics of swinging and supporting legs for continuous gaits. Hexagonal hexapod robots have been shown to be more flexible than rectangular ones. Moreover, hexagonal hexapods have many feasible gaits. In addition to the well-know insect gait and mammal gait, a new mixed gait for hexagonal six-legged robots has been proposed in this chapter which entails some features of both insect and mammal gaits. Except classified by legs movement as mentioned above, hexapod robots gaits are categorized according to the number of supporting legs during walking, as 3+3 tripod, 4+2 fault tolerant quadruped, and 5+1 one by one gaits. On account to the introduction of mixed gait, each numbered gait has one more form. Among three tripod-gait forms, the most stable is the mixed one. The mammal gait can reach the longest stride; whereas the continuous insect gait has the shortest maximum stride and poorest stability. Thanks to their six legs, hexapod robots have redundancy and fault tolerance. Gaits where one leg is lost or two opposite legs are lost have been discussed in recent times. In this chapter we have tackled also the cases in which two adjacent legs or two separated by a normal leg are damaged. Algorithms for realizing these two fault-tolerant gaits have been detailed and validated with simulations."
            },
            {
                "reference_title": "YOLO v3-Tiny: Object Detection and Recognition using one stage improved model",
                "reference_link": "publication/340893960_YOLO_v3-Tiny_Object_Detection_and_Recognition_using_one_stage_improved_model",
                "reference_type": "Conference Paper",
                "reference_date": "Mar 2020",
                "reference_abstract": null
            },
            {
                "reference_title": "You Only Look Once: Unified, Real-Time Object Detection",
                "reference_link": "publication/311609522_You_Only_Look_Once_Unified_Real-Time_Object_Detection",
                "reference_type": "Conference Paper",
                "reference_date": "Jun 2016",
                "reference_abstract": null
            },
            {
                "reference_title": "Design, construction and control of hexapod walking robot",
                "reference_link": "publication/304290829_Design_construction_and_control_of_hexapod_walking_robot",
                "reference_type": "Conference Paper",
                "reference_date": "Nov 2015",
                "reference_abstract": "This paper deals with design, construction and control of a hexapod (i.e. six-legged) walking robot. Basic characteristics of legged robots, a few existing robots and their pros and cons are described in the introduction of this paper. Next, basic gaits, which are used by legged robots for their locomotion are mentioned here. Main part of the paper is focused on the result of our project \u2014 on the six-legged robot, which can walk using tripod, wave and ripple gait and which is equipped with sonars, force-sensitive resistors and encoders. This robot is controlled and monitored from an user interface program, which can display data from the sensors and the positions of robot legs. The robot can be used to test and verify algorithms, gaits and features of hexapod walking robots."
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2018",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2015",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Comparative Study of Robotics Curricula",
        "date": "December 2020",
        "doi": "10.1109/TE.2020.3041667",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (16)",
        "abstract": "Contribution: The information described in this study provides a starting point for discussing an effective robotics curriculum offered by any engineering university or institute. Background: Robotics is a multidisciplinary field that includes mechanical engineering, electrical and electronic engineering, and computer science. Several universities have established departments of robotics to teach their students robotics education; however, a comprehensive curriculum to deliver robotics education has not yet been fully developed. Research Questions: What are the significant component courses offered by existing departments of robotics? Methodology: This article investigates component courses among departments of robotics through a title-based aggregation of existing courses and textual analyses. Findings: From a title-based aggregation of robotics curricula from 19 departments established by 2018, control engineering, programming, technical drawing/design, and electronic engineering/electronic circuits were found to be the core courses. From textual analyses of the curricula, robotics departments' course titles were similar to those of mechanical engineering departments.",
        "reference": [
            {
                "reference_title": "The Effectiveness of Integrating Educational Robotic Activities into Higher Education Computer Science Curricula: A Case Study in a Developing Country",
                "reference_link": "publication/315066323_The_Effectiveness_of_Integrating_Educational_Robotic_Activities_into_Higher_Education_Computer_Science_Curricula_A_Case_Study_in_a_Developing_Country",
                "reference_type": "Conference Paper",
                "reference_date": "Mar 2017",
                "reference_abstract": "In this paper, we present a case study to investigate the effects of educational robotics on a formal undergraduate Computer Science education in a developing country. The key contributions of this paper include a longitudinal study design, spanning the whole duration of one taught course, and its focus on continually assessing the effectiveness and the impact of robotic-based exercises. The study assessed the students\u2019 motivation, engagement and level of understanding in learning general computer programming. The survey results indicate that there are benefits which can be gained from such activities and educational robotics is a promising tool in developing engaging study curricula. We hope that our experience from this study together with the free materials and data available for download will be beneficial to other practitioners working with educational robotics in different parts of the world."
            },
            {
                "reference_title": "Robotics engineering program and curriculum development",
                "reference_link": "publication/282927812_Robotics_engineering_program_and_curriculum_development",
                "reference_type": "Article",
                "reference_date": "Feb 2015",
                "reference_abstract": "Robotics, as a multi-disciplinary branch of science and technology, is concerned with a variety of computer-controlled electro-mechanical structures designed for a multitude of applications. Less than a decade ago, robotics was a graduate-level field of research in engineering & computer science offered only at a few large engineering institutions. It has now become an engineering discipline of its own with a small but increasing number of schools offering an undergraduate engineering degree in robotics engineering. This article provides an overview of new undergraduate curricula in robotics engineering and the challenges associated with its curriculum development."
            },
            {
                "reference_title": "A One-Year Introductory Robotics Curriculum for Computer Science Upperclassmen",
                "reference_link": "publication/258792932_A_One-Year_Introductory_Robotics_Curriculum_for_Computer_Science_Upperclassmen",
                "reference_type": "Article",
                "reference_date": "Feb 2013",
                "reference_abstract": "This paper describes a one-year introductory robotics course sequence focusing on computational aspects of robotics for third- and fourth-year students. The key challenges this curriculum addresses are scalability, i.e., how to teach a robotics class with a limited amount of hardware to a large audience,student assessment, i.e., how to assess the students' success on robotic design and programming assignments, and depth versus breadth, i.e., how to down-select content from the interdisciplinary field of robotics to computer science students. This is achieved by combining simulation-based laboratory assignments, which can be conducted anywhere and anytime, with compatible hardware devices that allow a seamless transition from simulation to real hardware, and a focus on performance-based assessment with an open-ended final project/competition. Content learning and retention is assessed for a subset of students who successfully went through the proposed curriculum. All class materials as well as hardware-in particular, a low-cost, highly articulated robotic arm developed for teaching advanced robotics concepts-are open-source and available online."
            },
            {
                "reference_title": "A 10-Year Mechatronics Curriculum Development Initiative: Relevance, Content, and Results\u2014Part II",
                "reference_link": "publication/224565399_A_10-Year_Mechatronics_Curriculum_Development_Initiative_Relevance_Content_and_Results-Part_II",
                "reference_type": "Article",
                "reference_date": "Jun 2010",
                "reference_abstract": "This paper describes the second and third phases of a comprehensive mechatronics curriculum development effort. They encompass the development of two advanced mechatronics courses (\u00c2\u00bfSimulation and Modeling of Mechatronic Systems\u00c2\u00bf and \u00c2\u00bfSensors and Actuators for Mechatronic Systems\u00c2\u00bf), the formulation of a Mechatronics concentration, and offshoot research activities in the mechatronics area. The first phase involved the design of an \u00c2\u00bfIntroduction to Mechatronics\u00c2\u00bf course and the infusion of mechatronic activities throughout the curriculum and in outreach activities and has been described in a companion paper \u00c2\u00bfA 10-Year Mechatronics Curriculum Development Initiative: Relevance, Content, and Results-Part I\u00c2\u00bf (IEEE Transactions on Education, vol. 53, no. 2, May 2010)."
            },
            {
                "reference_title": "Robotics course \u2014 A challenge for computer science students",
                "reference_link": "publication/317417939_Robotics_course_-_A_challenge_for_computer_science_students",
                "reference_type": "Conference Paper",
                "reference_date": "Apr 2017",
                "reference_abstract": "Creating a good syllabus for any subject is challenging. This is even more so for the courses that are supported by hardware components, or the courses that evolve rapidly, as technology evolves. A course that encompasses both of the challenges is the course of Robotics. Even more so, this subject could be thought using different approaches, for different type of the audience that takes the course. For example, the subject could be thought to mechanical engineers, which are usually acquainted with hardware, or to software students, that usually lack the hardware knowledge. This paper elaborates on the evolution of creating the syllabus for the course Robotics thought in a Computer Science curriculum. The challenges that were successfully overcome, the increased student satisfaction and the creation of more difficult student projects in this course are given as a result of such an evolution of this particular Robotics course. The increased interest resulted in generating more Diploma Theses in the area of Robotics."
            },
            {
                "reference_title": "Robotics for All Ages: A Standard Robotics Curriculum for K\u201316",
                "reference_link": "publication/303406320_Robotics_for_All_Ages_A_Standard_Robotics_Curriculum_for_K-16",
                "reference_type": "Article",
                "reference_date": "May 2016",
                "reference_abstract": "The introduction of robotics in science, technology, engineering, and math (STEM) education is quickly becoming a very polarizing topic. As a discipline, robotics research is tackling new and challenging problems that resonate with public perception and also provide exciting and fun opportunities to see science in action. The exposure of students at various levels to robotics is controversial since many view it as a fad, and others rightly indicate that there is still a need for coherent analysis of robotics in the classroom to assess the impact on education and engagement. In this work, we argue that for robotics to be effectively incorporated into the educational pipeline, there is a need for a comprehensive robotics curriculum. Such a curriculum should be designed to be intentionally flexible; however, its development would enable principled study in a variety of learning environments and over the long term. We propose the nature of the standard curriculum and present recommendations on how it can be practically applied. Our aim in the long term is to provide resources that will elevate robotics education. The existence of these resources will advance the current level of understanding from anecdotal and short-lived inquiry so that robotics education can be evidence-based and sustainable."
            },
            {
                "reference_title": "The Text Mining Handbook: Advanced Approaches in Analyzing Unstructured Data",
                "reference_link": "publication/280113623_The_Text_Mining_Handbook_Advanced_Approaches_in_Analyzing_Unstructured_Data",
                "reference_type": "Book",
                "reference_date": "Dec 2006",
                "reference_abstract": "Text mining tries to solve the crisis of information overload by combining techniques from data mining, machine learning, natural language processing, information retrieval, and knowledge management. In addition to providing an in-depth examination of core text mining and link detection algorithms and operations, this book examines advanced pre-processing techniques, knowledge representation considerations, and visualization approaches. Finally, it explores current real-world, mission-critical applications of text mining and link detection in such varied fields as M&A business intelligence, genomics research and counter-terrorism activities."
            },
            {
                "reference_title": "The Robotic Decathlon: Project-Based Learning Labs and Curriculum Design for an Introductory Robotics Course",
                "reference_link": "publication/260587516_The_Robotic_Decathlon_Project-Based_Learning_Labs_and_Curriculum_Design_for_an_Introductory_Robotics_Course",
                "reference_type": "Article",
                "reference_date": "Feb 2013",
                "reference_abstract": "This paper presents a series of novel project-based learning labs for an introductory robotics course that are developed into a semester-long Robotic Decathlon. The last three events of the Robotic Decathlon are used as three final one-week-long project tasks; these replace a previous course project that was a semester-long robotics competition. The course assessment shows that this new approach enhances student learning with respect to the standard lecture/test style of teaching, and that the three shorter final project tasks make the course easier to manage and more enjoyable for the students."
            },
            {
                "reference_title": "Design-Oriented Enhanced Robotics Curriculum",
                "reference_link": "publication/260587289_Design-Oriented_Enhanced_Robotics_Curriculum",
                "reference_type": "Article",
                "reference_date": "Feb 2013",
                "reference_abstract": "This paper presents an innovative two-course, laboratory-based, and design-oriented robotics educational model. The robotics curriculum exposed senior-level undergraduate students to major robotics concepts, and enhanced the student learning experience in hybrid learning environments by incorporating the IEEE Region-5 annual robotics competition with open-ended design challenges, by establishing a robotics club, and by implementing a K-12 mentorship program. A four-faculty team developed two elective courses in which the theoretical concepts underlying the robotics design competition topics were supported with corresponding laboratory activities. Students took the courses sequentially. They were formed into diverse teams, whose performance was assessed via weekly team presentations and participation in an outreach day. The best robot design teams participated in the IEEE Region-5 competitions. All students participated in a service-learning activity, acting as mentors during local K-12 robotics competitions while enhancing their own robotics comprehension. The robotics curriculum's two-year evaluation results illustrate the consistent efficacy of the curriculum during the project duration, indicating a successful robotics educational model for other academic institutions to follow."
            },
            {
                "reference_title": "BigDog, the Rough-Terrain Quaduped Robot",
                "reference_link": "publication/252191628_BigDog_the_Rough-Terrain_Quaduped_Robot",
                "reference_type": "Article",
                "reference_date": "Jul 2011",
                "reference_abstract": "Less than half the Earth's landmass is accessible to existing wheeled and tracked vehicles. But people and animals using their legs can go almost anywhere. Our mission at Boston Dynamics is to develop a new breed of rough-terrain robots that capture the mobility, autonomy and speed of living creatures. Such robots will travel in outdoor terrain that is too steep, rutted, rocky, wet, muddy, and snowy for conventional vehicles. They will travel in cities and in our homes, doing chores and providing care, where steps, stairways and household clutter limit the utility of wheeled vehicles. Robots meeting these goals will have terrain sensors, sophisticated computing and power systems, advanced actuators and dynamic controls. We will give a status report on BigDog, an example of such rough-terrain robots."
            }
        ]
    },
    {
        "title": "Analysis of co-authorship networks among Brazilian graduate programs in computer science",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (36)",
        "abstract": "The growth and popularization of platforms on scientific production have been the subject of several studies, producing relevant analyses of coauthorship behavior among groups of researchers. Researchers and their scientific productions can be analyzed as coauthorship social networks, so researchers are linked through common publications. In this context, coauthoring networks can be analyzed to find patterns that can describe or characterize them. This work presents the analysis and characterization of co-authorship networks of academic Brazilian graduate programs in computer science. To this end, data from the curricula of Brazilian researchers were collected and modeled as coauthoring networks among the graduate programs that researchers participate in. Each network topology was analyzed regarding complex network measurements and three qualitative indices that evaluate the publications quality. In addition, the coauthorship networks of the graduate programs were characterized in relation to the evaluation received by CAPES, which attributes a qualitative grade to the graduate programs in Brazil. The results indicate some of the most relevant topological measures for the programs characterization and evaluate at different qualitative rates and indicate a pattern of the graduate programs best evaluated by CAPES.",
        "reference": [
            {
                "reference_title": "BASiNET\u2014BiologicAl Sequences NETwork: a case study on coding and non-coding RNAs identification",
                "reference_link": "publication/325590744_BASiNET-BiologicAl_Sequences_NETwork_a_case_study_on_coding_and_non-coding_RNAs_identification",
                "reference_type": "Article",
                "reference_date": "Jun 2018",
                "reference_abstract": "With the emergence of Next Generation Sequencing (NGS) technologies, a large volume of sequence data in particular de novo sequencing was rapidly produced at relatively low costs. In this context, computational tools are increasingly important to assist in the identification of relevant information to understand the functioning of organisms. This work introduces BASiNET, an alignment-free tool for classifying biological sequences based on the feature extraction from complex network measurements. The method initially transform the sequences and represents them as complex networks. Then it extracts topological measures and constructs a feature vector that is used to classify the sequences. The method was evaluated in the classification of coding and non-coding RNAs of 13 species and compared to the CNCI, PLEK and CPC2 methods. BASiNET outperformed all compared methods in all adopted organisms and datasets. BASiNET have classified sequences in all organisms with high accuracy and low standard deviation, showing that the method is robust and non-biased by the organism. The proposed methodology is implemented in open source in R language and freely available for download at https://cran.r-project.org/package=BASiNET."
            },
            {
                "reference_title": "\"Brazilian style science\" - an analysis of the difference between Brazilian and international Computer Science departments and graduate programs using social networks analysis and bibliometrics",
                "reference_link": "publication/319533197_Brazilian_style_science_-_an_analysis_of_the_difference_between_Brazilian_and_international_Computer_Science_departments_and_graduate_programs_using_social_networks_analysis_and_bibliometrics",
                "reference_type": "Article",
                "reference_date": "Sep 2017",
                "reference_abstract": "In this paper, we compare the Brazilian Computer Science Graduate Programs of levels 6 and 7 (which CAPES assume to be equivalent to good international ones) and those departments best ranked in three different international rankings. We used both bibliometric and social networks analysis metrics, and we could see that there is a great difference between the results achieved by the national and international programs. The results from a principal component analysis show that the Brazilian programs are a class of their own and do not share the characteristics of the best international departments. We also analyzed the CAPES grade system and show that it is consistent, even if based on different metrics than those used internationally. It is safe to conclude that there is a \"Brazilian Science\" that is different from the worldwide accepted one and that derives, in a certain way, from an observer effect. We discuss the observer effect in Science and how scientific output could actually be measured."
            },
            {
                "reference_title": "A Complex Network-Based Approach to the Analysis and Classification of Images",
                "reference_link": "publication/283505567_A_Complex_Network-Based_Approach_to_the_Analysis_and_Classification_of_Images",
                "reference_type": "Conference Paper",
                "reference_date": "Nov 2015",
                "reference_abstract": "Complex network is a topic related with a plurality of knowledge from various areas and has been applied with success in all of them. However, it is a recent area considering its application in image pattern recognition. There are few works in the literature that use the complex networks for image characterization following its analysis and classification. An image can be interpreted as a complex network wherein each pixel represents a vertex and the weighted edges are generated according to the location and intensity between two pixels. Thus, the present paper aims to investigate this type of application and explore different measurements that can be extracted from complex networks to better characterize an image. One special type of measure that we applied were those based on motifs, which are employed in several areas. However, to the best of our knowledge, motifs were never explored in complex networks representing images. The results demonstrate that our proposed methodology presented great potential, reaching up to 89.81% of accuracy for the classification of public domain image texture datasets."
            },
            {
                "reference_title": "Network analysis of Zentralblatt MATH data",
                "reference_link": "publication/265415078_Network_analysis_of_Zentralblatt_MATH_data",
                "reference_type": "Article",
                "reference_date": "Sep 2014",
                "reference_abstract": "We analyze the data about works (papers, books) from the time period\n1990-2010 that are collected in Zentralblatt MATH database. The data were\nconverted into four 2-mode networks (works $\\times$ authors, works $\\times$\njournals, works $\\times$ keywords and works $\\times$ MSCs) and into a partition\nof works by publication year. The networks were analyzed using Pajek -- a\nprogram for analysis and visualization of large networks. We explore the\ndistributions of some properties of works and the collaborations among\nmathematicians. We also take a closer look at the characteristics of the field\nof graph theory as were realized with the publications."
            },
            {
                "reference_title": "BraX-Ray: An X-Ray of the Brazilian Computer Science Graduate Programs",
                "reference_link": "publication/261609312_BraX-Ray_An_X-Ray_of_the_Brazilian_Computer_Science_Graduate_Programs",
                "reference_type": "Article",
                "reference_date": "Apr 2014",
                "reference_abstract": "Research productivity assessment is increasingly relevant for allocation of research funds. On one hand, this assessment is challenging because it involves both qualitative and quantitative analysis of several characteristics, most of them subjective in nature. On the other hand, current tools and academic social networks make bibliometric data web-available to everyone for free. Those tools, especially when combined with other data, are able to create a rich environment from which information on research productivity can be extracted. In this context, our work aims at characterizing the Brazilian Computer Science graduate programs and the relationship among themselves. We (i) present views of the programs from different perspectives, (ii) rank the programs according to each perspective and a combination of them, (iii) show correlation between assessment metrics, (iv) discuss how programs relate to another, and (v) infer aspects that boost programs' research productivity. The results indicate that programs with a higher insertion in the coauthorship network topology also possess a higher research productivity between 2004 and 2009."
            },
            {
                "reference_title": "Classification of Texture based on Bag-of-Visual-Words through Complex Networks",
                "reference_link": "publication/333171319_Classification_of_Texture_based_on_Bag-of-Visual-Words_through_Complex_Networks",
                "reference_type": "Article",
                "reference_date": "May 2019",
                "reference_abstract": "Over the last years complex data (e.g. images) have been growing in a very fast pace. This demands the ability to describe and to categorize them. To solve this problem it is essential to develop efficient and effective vision-based expert techniques. Hence, the cornerstone of our work is to propose a new methodology, called BoVW-CN, that combines Bag-of-Visual-Words and complex networks for describing keypoints detected in a given image. Our insight is that describing just the relevant points of an image we can achieve a more cost-effective and better image description. The obtained results testify that BoVW-CN, applied to public image datasets, outperforms the widely used state-of-the-art methods. We not only obtained good accuracies (e.g. 78.18%), but also performed analyses to find the best trade-off between computational cost and accuracy. Besides, to the best of our knowledge, our work is the first one to propose such integration of Bag-of-Visual-Words and complex networks through a texture-based focus."
            },
            {
                "reference_title": "Combining SURF descriptor and complex networks for face recognition",
                "reference_link": "publication/313803044_Combining_SURF_descriptor_and_complex_networks_for_face_recognition",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2016",
                "reference_abstract": null
            },
            {
                "reference_title": "Principles of Data Integration",
                "reference_link": "publication/287308064_Principles_of_Data_Integration",
                "reference_type": "Article",
                "reference_date": "Jan 2012",
                "reference_abstract": "How do you approach answering queries when your data is stored in multiple databases that were designed independently by different people? This is first comprehensive book on data integration and is written by three of the most respected experts in the field. This book provides an extensive introduction to the theory and concepts underlying today's data integration techniques, with detailed, instruction for their application using concrete examples throughout to explain the concepts. Data integration is the problem of answering queries that span multiple data sources (e.g., databases, web pages). Data integration problems surface in multiple contexts, including enterprise information integration, query processing on the Web, coordination between government agencies and collaboration between scientists. In some cases, data integration is the key bottleneck to making progress in a field. The authors provide a working knowledge of data integration concepts and techniques, giving you the tools you need to develop a complete and concise package of algorithms and applications. * Offers a range of data integration solutions enabling you to focus on what is most relevant to the problem at hand. *Enables you to build your own algorithms and implement your own data integration applications *Companion website with numerous project-based exercises and solutions. Video interview(s) and video lectures from authors. Links to commercially available software allowing readers to build their own algorithms and implement their own data integration applications. Facebook page for reader input during and after publication."
            },
            {
                "reference_title": "The Proof and Measurement of Association Between Two Things",
                "reference_link": "publication/261588607_The_Proof_and_Measurement_of_Association_Between_Two_Things",
                "reference_type": "Article",
                "reference_date": "Jan 1904",
                "reference_abstract": "Note:\nRepublished in: Am J Psychol. 100(3-4) 441-71 (1987).\nRepublished in: Int J Epidemiol. 39(5):1137-50 (2010)."
            },
            {
                "reference_title": "A feature selection technique for inference of graphs from their known topological properties: Revealing scale-free gene regulatory networks",
                "reference_link": "publication/260183560_A_feature_selection_technique_for_inference_of_graphs_from_their_known_topological_properties_Revealing_scale-free_gene_regulatory_networks",
                "reference_type": "Article",
                "reference_date": "Jul 2014",
                "reference_abstract": "An important problem in bioinformatics is the inference of gene regulatory networks (GRNs) from expression profiles. In general, the main limitations faced by GRN inference methods are the small number of samples with huge dimensionalities and the noisy nature of the expression measurements. Alternatives are thus needed to obtain better accuracy for the GRNs inference problem. Many pattern recognition techniques rely on prior knowledge about the problem in addition to the training data to gain statistical estimation power. This work addresses the GRN inference problem by modeling prior knowledge about the network topology. The main contribution of this paper is a novel methodology that aggregates scale-free properties to a classical low-cost feature selection method, known as Sequential Floating Forward Selection (SFFS), for guiding the inference task. Such methodology explores the search space iteratively by applying a scale-free property to reduce the search space. In this way, the search space traversed by the method integrates the exploration of all combinations of predictors set when the number of combinations is small (dimensionality <= 2) with a floating search when the number of combinations becomes\nexplosive (dimensionality >= 3). This process is guided by scale-free prior information.\nExperimental results using synthetic and real data show that this technique provides smaller estimation errors than those obtained without guiding the SFFS application by the scale-free model, thus maintaining the robustness of the SFFS method. Therefore, we show that the proposed framework may be applied in combination with other existing GRN inference methods to improve the prediction accuracy of networks with scale-free properties."
            }
        ]
    },
    {
        "title": "Alg\u00e8bre Informatique pour l'Enseignement Secondaire",
        "date": "December 2020",
        "doi": null,
        "conferance": "Conference: S\u00e9minaire tenu \u00e0 l\u2019Institut National de Formation P\u00e9dagogique",
        "citations_count": null,
        "reference_count": "References (1)",
        "abstract": "Ce s\u00e9minaire souhaite initier les enseignants en math\u00e9matiques du cycle secondaire\u00e0 l'alg\u00e8bre informatique. Ils pourront ainsi pr\u00e9parer leurs cours,\u00e9laborer des exercices et pr\u00e9senter des solutions avec une meilleure pr\u00e9cision, rapidit\u00e9 et clart\u00e9. Nous verrons comment effectuer des calculs vectoriels, d\u00e9terminer des propri\u00e9t\u00e9s arithm\u00e9tiques, r\u00e9soudre des syst\u00e8mes d'\u00e9quations, etudier des fonctions et tracer des courbes avec le logiciel de calcul formel Maxima.",
        "reference": [
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": null,
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Yefimov E.LENA. RU-EN AE",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (6)",
        "abstract": "Some people wrongly believe that A. Turing\u2019s works that underlie all modern computer science never discussed \u201cphysical\u201d robots. This is not so, since Turing did speak about such machines, though making a reservation that this discussion was still premature. In particular, in his 1948 report [8], he suggested that a physical intelligent machine equipped with motors, cameras and loudspeakers, when wandering through the fields of England, would present \u201cthe danger to the ordinary citizen would be serious.\u201d [8, ]. Due to this imperfection of technology in the field of knowledge that we now call robotics, the methodology that he proposed was based on human speech, or rather on text. Other natural human skills were too difficult to implement, while the exchange of cues via written messages was much more accessible for engineering implementation in Turing\u2019s time. Nevertheless, since then, the progress of computer technology has taken forms that the founder of artificial intelligence could not have foreseen.",
        "reference": [
            {
                "reference_title": "A Voice and Nothing More",
                "reference_link": "publication/329654159_A_Voice_and_Nothing_More",
                "reference_type": "Book",
                "reference_date": "Jan 2006",
                "reference_abstract": null
            },
            {
                "reference_title": "Understanding Media: The Extensions of Man",
                "reference_link": "publication/324352231_Understanding_Media_The_Extensions_of_Man",
                "reference_type": "Article",
                "reference_date": "Jul 1969",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": null,
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2007",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 1965",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Jan 2017",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Mastering Atari, Go, chess and shogi by planning with a learned model",
        "date": "December 2020",
        "doi": "10.1038/s41586-020-03051-4",
        "conferance": null,
        "citations_count": "Citations (27)",
        "reference_count": "References (52)",
        "abstract": "Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess\u00b9 and Go\u00b2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games\u00b3\u2014the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled\u2074\u2014the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi\u2014canonical environments for high-performance planning\u2014the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm\u2075 that was supplied with the rules of the game.",
        "reference": [
            {
                "reference_title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning",
                "reference_link": "publication/336911787_Grandmaster_level_in_StarCraft_II_using_multi-agent_reinforcement_learning",
                "reference_type": "Article",
                "reference_date": "Nov 2019",
                "reference_abstract": "Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1,2,3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems\u2074. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players."
            },
            {
                "reference_title": "Planning chemical syntheses with deep neural networks and symbolic AI",
                "reference_link": "publication/324066687_Planning_chemical_syntheses_with_deep_neural_networks_and_symbolic_AI",
                "reference_type": "Article",
                "reference_date": "Mar 2018",
                "reference_abstract": "Computers teach themselves to make molecules\nChemical reaction databases that are automatically filled from the literature have made the planning of chemical syntheses, whereby target molecules are broken down into smaller and smaller building blocks, vastly easier over the past few decades. However, humans must still search these databases manually to find the best way to make a molecule. This involves many steps and choices. Some degree of automation has been achieved by encoding 'rules' of synthesis into computer programs, but this is time consuming owing to the numerous rules and subtleties involved. Here, Mark Waller and colleagues apply deep neural networks to plan chemical syntheses. They trained an algorithm on essentially every reaction published before 2015 so that it could learn the 'rules' itself and then predict synthetic routes to various small molecules not included in the training set. In blind testing, trained chemists could not distinguish between the solutions found by the algorithm and those taken from the literature."
            },
            {
                "reference_title": "Learning and Querying Fast Generative Models for Reinforcement Learning",
                "reference_link": "publication/323027085_Learning_and_Querying_Fast_Generative_Models_for_Reinforcement_Learning",
                "reference_type": "Article",
                "reference_date": "Feb 2018",
                "reference_abstract": "A key challenge in model-based reinforcement learning (RL) is to synthesize computationally efficient and accurate environment models. We show that carefully designed generative models that learn and operate on compact state representations, so-called state-space models, substantially reduce the computational costs for predicting outcomes of sequences of actions. Extensive experiments establish that state-space models accurately capture the dynamics of Atari games from the Arcade Learning Environment from raw pixels. The computational speed-up of state-space models while maintaining high accuracy makes their application in RL feasible: We demonstrate that agents which query these models for decision making outperform strong model-free baselines on the game MSPACMAN, demonstrating the potential of using learned environment models for planning."
            },
            {
                "reference_title": "Mastering the game of Go without human knowledge",
                "reference_link": "publication/320473480_Mastering_the_game_of_Go_without_human_knowledge",
                "reference_type": "Article",
                "reference_date": "Oct 2017",
                "reference_abstract": "A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. \u00a9 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved."
            },
            {
                "reference_title": "TreeqN and ATreEC: Differentiable tree-structured models for deep reinforcement learning",
                "reference_link": "publication/337393821_TreeqN_and_ATreEC_Differentiable_tree-structured_models_for_deep_reinforcement_learning",
                "reference_type": "Conference Paper",
                "reference_date": "Apr 2018",
                "reference_abstract": "Learning Representations, ICLR 2018 - Conference Track Proceedings.All right reserved. Combining deep model-free reinforcement learning with on-line planning is a promising approach to building on the successes of deep RL. On-line planning with look-ahead trees has proven successful in environments where transition models are known a priori. However, in complex environments where transition models need to be learned from data, the deficiencies of learned models have limited their utility for planning. To address these challenges, we propose TreeQN, a differentiable, recursive, tree-structured model that serves as a drop-in replacement for any value function network in deep RL with discrete actions. TreeQN dynamically constructs a tree by recursively applying a transition model in a learned abstract state space and then aggregating predicted rewards and state-values using a tree backup to estimate Q-values. We also propose ATreeC, an actor-critic variant that augments TreeQN with a softmax layer to form a stochastic policy network. Both approaches are trained end-to-end, such that the learned model is optimised for its actual use in the tree. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a box-pushing task, as well as n-step DQN and value prediction networks (Oh et al., 2017) on multiple Atari games. Furthermore, we present ablation studies that demonstrate the effect of different auxiliary losses on learning transition models."
            },
            {
                "reference_title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play",
                "reference_link": "publication/329473176_A_general_reinforcement_learning_algorithm_that_masters_chess_shogi_and_Go_through_self-play",
                "reference_type": "Article",
                "reference_date": "Dec 2018",
                "reference_abstract": "The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go."
            },
            {
                "reference_title": "Superhuman AI for heads-up no-limit poker: Libratus beats top professionals",
                "reference_link": "publication/321878591_Superhuman_AI_for_heads-up_no-limit_poker_Libratus_beats_top_professionals",
                "reference_type": "Article",
                "reference_date": "Dec 2017",
                "reference_abstract": "No-limit Texas hold\u2019em is the most popular form of poker. Despite AI successes in perfect-information games, the private information and massive game tree have made no-limit poker difficult to tackle. We present Libratus, an AI that, in a 120,000-hand competition, defeated four top human specialist professionals in heads-up no-limit Texas hold\u2019em, the leading benchmark and long-standing challenge problem in imperfect-information game solving. Our game-theoretic approach features application-independent techniques: an algorithm for computing a blueprint for the overall strategy, an algorithm that fleshes out the details of the strategy for subgames that are reached during play, and a self-improver algorithm that fixes potential weaknesses that opponents have identified in the blueprint strategy."
            },
            {
                "reference_title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents",
                "reference_link": "publication/319895937_Revisiting_the_Arcade_Learning_Environment_Evaluation_Protocols_and_Open_Problems_for_General_Agents",
                "reference_type": "Article",
                "reference_date": "Sep 2017",
                "reference_abstract": "The Arcade Learning Environment (ALE) is an evaluation platform that poses the challenge of building AI agents with general competency across dozens of Atari 2600 games. It supports a variety of different problem settings and it has been receiving increasing attention from the scientific community, leading to some high-profile success stories such as the much publicized Deep Q-Networks (DQN). In this article we take a big picture look at how the ALE is being used by the research community. We show how diverse the evaluation methodologies in the ALE have become with time, and highlight some key concerns when evaluating agents in the ALE. We use this discussion to present some methodological best practices and provide new benchmark results using these best practices. To further the progress in the field, we introduce a new version of the ALE that supports multiple game modes and provides a form of stochasticity we call sticky actions. We conclude this big picture look by revisiting challenges posed when the ALE was introduced, summarizing the state-of-the-art in various problems and highlighting problems that remain open."
            },
            {
                "reference_title": "Prioritized Experience Replay",
                "reference_link": "publication/319770330_Prioritized_Experience_Replay",
                "reference_type": "Conference Paper",
                "reference_date": "Nov 2016",
                "reference_abstract": "Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games."
            },
            {
                "reference_title": "Imagenet classification with deep convolutional neural networks",
                "reference_link": "publication/319770183_Imagenet_classification_with_deep_convolutional_neural_networks",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2012",
                "reference_abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif- ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implemen- tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called dropout that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry"
            }
        ]
    },
    {
        "title": "A Machine-Learning based Approach to Support Academic Decision-Making at Higher Educational Institutions",
        "date": "December 2020",
        "doi": "10.1109/ISNCC49221.2020.9297177",
        "conferance": "Conference: International Symposium on Networks, Computers and Communications (ISNCC-2020)",
        "citations_count": null,
        "reference_count": "References (21)",
        "abstract": "Taking appropriate decisions in the academic processes at a university has a great impact in improving the quality of education and can have an important benefit for students, faculty members, and the entire academic community. In this paper, we propose a decision support solution providing accurate analysis, better decision support, and reporting and planning capability to assist decision-makers in order to enhance the quality of educational processes. To achieve this goal, a set of machine learning is used. Experiments are conducted on real data describing the College of Computer Science and Engineering (CCSE) at Taibah University in Saudi Arabia. Results show that we can predict graduation rates in a real case study to support decision-making. In addition, a comparison between four techniques of machine learning namely Support Vector Machine, Na\u00efve Bayes, Decision Tree, and Random Forest is held using accuracy, recall, precision, and F-measure.",
        "reference": [
            {
                "reference_title": "Usage of Machine Learning for Strategic Decision Making at Higher Educational Institutions",
                "reference_link": "publication/333263659_Usage_of_Machine_Learning_for_Strategic_Decision_Making_at_Higher_Educational_Institutions",
                "reference_type": "Article",
                "reference_date": "May 2019",
                "reference_abstract": "Decisions made at the strategic level of Higher Educational Institutions (HEIs) affect policies, strategies, and actions that the institutions make as a whole. Decision\u2019s structures at HEIs are depicted in the paper and their effectiveness in supporting the institutions\u2019 governance. The disengagement of the stakeholders and the lack of using efficient computational algorithms lead to a) the decision process takes longer, b) the \u201cwhole picture\u201d is not involved along with all data necessary and c) small academic impact is produced by the decision, among others. Machine Learning is an emerging field of artificial intelligence that using various algorithms analyzes information and provides a richer understanding of the data contained in a specific context. Based on the author\u2019s previous works we focus on supporting decision making at a strategic level, being deans\u2019 concerns the preeminent mission to bolster. In our study, three supervised classification algorithms are deployed to predict graduation rates from real data about undergraduate engineering students in South America. Analysis of ROC curve and accuracy are executed as measures of effectiveness to compare and evaluate Decision Tree, Logistic Regression and Random Forest, where this last one demonstrates the best outcomes."
            },
            {
                "reference_title": "Comparative Analysis of Bayes and Lazy Classification Algorithms",
                "reference_link": "publication/329800085_Comparative_Analysis_of_Bayes_and_Lazy_Classification_Algorithms",
                "reference_type": "Article",
                "reference_date": "Aug 2013",
                "reference_abstract": null
            },
            {
                "reference_title": "A business intelligence based solution to support academic affairs: case of Taibah University",
                "reference_link": "publication/329196787_A_business_intelligence_based_solution_to_support_academic_affairs_case_of_Taibah_University",
                "reference_type": "Article",
                "reference_date": "Nov 2018",
                "reference_abstract": "Business intelligence (BI) is a set of technologies and strategies allowing analyzing data and providing information to help\nin the decision-making process. Today, BI is used in many sectors and shows marvelous results. In this paper, we are\ninterested in supporting decision-making in higher education. BI concepts when integrated to the process of academic\naffairs can make significant improvement. The present paper aims at describing a BI solution to support academic affairs at\nTaibah University. Using BI, we can take advantages of a set of analytical tools that support decision-making for different\ntypes of users (students, faculty members, administrators and decision makers). The proposed BI solution consists\nessentially of three mains tasks: (1) collecting data from different sources using the three operations (Extract, Transform\nand Load), (2) proposing a multidimensional solution that describes the academic processes, and (3) visualizing results\nthrough a set of dashboards and reports. Experiments are made using the SQL Server Data Tools. Three steps are detailed\nwhich are: SQL Server Integration Services, SQL Server Analysis Services and SQL Server Reporting Services. The\nproposed solution provides many statistical and predictive indicators needed in academic tasks."
            },
            {
                "reference_title": "Reducing uncertainties in land cover change models using sensitivity analysis",
                "reference_link": "publication/319804147_Reducing_uncertainties_in_land_cover_change_models_using_sensitivity_analysis",
                "reference_type": "Article",
                "reference_date": "Jun 2018",
                "reference_abstract": "Land cover change (LCC) models aim to track spatiotemporal changes made in land cover. In most cases, LCC models contain uncertainties in their main components (i.e., input parameters and model structure). These uncertainties propagate through the modeling system, which generates uncertainties in the model outputs. The aim of this manuscript is to propose an approach to reduce uncertainty of LCC prediction models. The main objective of the proposed approach is to apply a sensitivity analysis method, based on belief function theory, to determine parameters and structures that have a high contribution in the variability of the predictions of the LCC model. Our approach is applied to four common LCC models (i.e., DINAMICA, SLEUTH, CA-MARKOV, and LCM). Results show that uncertainty of the model parameters and structure has meaningful impacts on the final decisions of LCC models. Ignoring this uncertainty can lead to erroneous decision about land changes. Therefore, the presented approach is very useful to identify the most relevant uncertainty sources that need to be processed to improve the accuracy of LCC models. The applicability and effectiveness of the proposed approach are demonstrated through a case study based on the Cairo region. Results show that 13% of the agriculture and 3.8% of the desert lands in 2014 would be converted to urban areas in 2025."
            },
            {
                "reference_title": "A novel decision support system for the interpretation of remote sensing big data",
                "reference_link": "publication/318931207_A_novel_decision_support_system_for_the_interpretation_of_remote_sensing_big_data",
                "reference_type": "Article",
                "reference_date": "Mar 2018",
                "reference_abstract": "Applications of remote sensing (RS) data cover several fields such as: cartography, surveillance, land-use planning, archaeology, environmental studies, resources management, etc. However, the amount of RS data has grown considerably due to the increase of aerial and satellite sensors. With this continuous increase, the necessity of having automated tools for the interpretation and analysis of RS big data is clearly obvious. The manual interpretation becomes a time consuming and expensive task. In this paper, a novel tool for interpreting and analyzing RS big data is described. The proposed system allows knowledge gathering for decision support in RS fields. It helps users easily make decisions in many fields related to RS by providing descriptive, predictive and prescriptive analytics. The paper outlines the design and development of a framework based on three steps: RS data acquisition, modeling, and analysis & interpretation. The performance of the proposed system has been demonstrated through three models: clustering, decision tree and association rules. Results show that the proposed tool can provide efficient decision support (descriptive and predictive) which can be adapted to several RS users\u2019 requests. Additionally, assessing these results show good performances of the developed tool."
            },
            {
                "reference_title": "Predicting Achievement of Students in Smart Campus",
                "reference_link": "publication/328254591_Predicting_Achievement_of_Students_in_Smart_Campus",
                "reference_type": "Article",
                "reference_date": "Oct 2018",
                "reference_abstract": "Isolate data among different campus information systems and not many effective information among the big data generated by these systems cause that it is a challenge for predicting achievement of students. This paper design a student achievement predicting framework, which includes data processing and student achievement predicting. In the data processing, data extraction, data cleaning, and feature extraction are designed. Using these data in data warehouse, we propose a layer-supervised multi-layer perceptron based method (LSMLP) to predict the achievement of students. Supervisions are fed to corresponding each hidden layer of MLP to improve the performance of student achievement prediction. Compared with SVM, Naive Bayes, Logistic Regression, and Multilayer Perceptron, our method get better performance."
            },
            {
                "reference_title": "Analysis of student behavior in learning management systems through a Big Data framework",
                "reference_link": "publication/326967110_Analysis_of_student_behavior_in_learning_management_systems_through_a_Big_Data_framework",
                "reference_type": "Article",
                "reference_date": "Aug 2018",
                "reference_abstract": "In recent years, learning management systems (LMSs) have played a fundamental role in higher education teaching models. A new line of research has been opened relating to the analysis of student behavior within an LMS, in the search for patterns that improve the learning process. Current e-learning platforms allow for recording student activity, thereby enabling the exploration of events generated in the use of LMS tools. This paper presents a case study conducted at the Catholic University of Murcia, where student behavior in the past four academic years was analyzed according to learning modality (that is, on-campus, online, and blended), considering the number of accesses to the LMS, tools employed by students and their associated events. Given the difficulty of managing the large volume of data generated by users in the LMS (up to 70 GB in this study), statistical and association rule techniques were performed using a Big Data framework, thus speeding up the statistical analysis of the data. The obtained results are demonstrated using visual analytic techniques, and evaluated in order to detect trends and deficiencies in the use of the LMS by students."
            },
            {
                "reference_title": "Survey of Needs and Expectations for Academic Advising in a Hong Kong University",
                "reference_link": "publication/321958648_Survey_of_Needs_and_Expectations_for_Academic_Advising_in_a_Hong_Kong_University",
                "reference_type": "Article",
                "reference_date": "Jan 2017",
                "reference_abstract": "Universities in Hong Kong implemented a new 4-year undergraduate curriculum in 2012, and many initiated academic advising programs to help students from different academic backgrounds and with various levels of preparedness to review their options and manage challenges in college. For this study, we administered a questionnaire survey to discover students' views on and expectations for academic advising. The results show an overall positive evaluation of academic advising from students, who expected academic advisors to help them understand their study options and preferred a developmental over a prescriptive approach. Students reported that discussing career issues was their greatest need for academic advising."
            },
            {
                "reference_title": "Sensitivity Analysis Approach to Model Epistemic and Aleatory Imperfection: Application to Land Cover Change Prediction Model",
                "reference_link": "publication/320515856_Sensitivity_Analysis_Approach_to_Model_Epistemic_and_Aleatory_Imperfection_Application_to_Land_Cover_Change_Prediction_Model",
                "reference_type": "Article",
                "reference_date": "Oct 2017",
                "reference_abstract": null
            },
            {
                "reference_title": "Support-vector networks",
                "reference_link": "publication/312538118_Support-vector_networks",
                "reference_type": "Article",
                "reference_date": "Jan 2009",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Quantum-Classical Mechanics: Principles, Applications, and Prospects (plenary presentation) _International Webinar on Advances in Physics, Chemistry, Mathematics, Computer Sciences & Biological Sciences_18 - 20 December 2020",
        "date": "December 2020",
        "doi": "10.13140/RG.2.2.15139.48168",
        "conferance": "Conference: CONIAPS XXVI",
        "citations_count": null,
        "reference_count": null,
        "abstract": "Quantum-classical electron as organizing principle in nature",
        "reference": []
    },
    {
        "title": "International Journal of Advance Research in Computer Science and Management Studies Development of Simulator for Simple Command Post Integrated Data Display using Assurance Case",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (5)",
        "abstract": "Software design for mission critical systems is challenging and often needs utmost care in ensuring all the essential requirements of system are met without any compromise on quality and reliability of system deliverables. Present paper has explored an approach of developing software for command post integrated data display Simulator based on Assurance Case driven development. This paper has systematically illustrated how software development for a software system can be carried out using Assurance Cases. The developed code was tested using rigorous test cases to ensure that the goals and sub-goals are met with adequate reliability and safety and thus the overall simulator goal. The design was further verified with collected artifacts through actual testing to ensure that the developed Simulator is working as per the stated requirements under all specified operating conditions. We concluded with discussion of important results and suggestions for addressing the problems we faced while developing the Simulator software using Assurance Case.",
        "reference": [
            {
                "reference_title": "Assurance Cases in Model-Driven Development of the Pacemaker Software",
                "reference_link": "publication/221431021_Assurance_Cases_in_Model-Driven_Development_of_the_Pacemaker_Software",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2010",
                "reference_abstract": "We discuss the construction of an assurance case for the pacemaker software. The software is developed following a model-based\ntechnique that combined formal modeling of the system, systematic code generation from the formal model, and measurement of\ntiming behavior of the implementation. We show how the structure of the assurance case reflects our development approach."
            },
            {
                "reference_title": "Using an Assurance Case to Support Independent Assessment of the Transition to a New GPS Ground Control System",
                "reference_link": "publication/4371243_Using_an_Assurance_Case_to_Support_Independent_Assessment_of_the_Transition_to_a_New_GPS_Ground_Control_System",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2008",
                "reference_abstract": "We describe a specific application of assurance cases to the problem of ensuring that a transition from a legacy system to its replacement will not compromise mission assurance objectives. The application in question was the transition of the Global Positioning System (GPS) to a new ground-control system. The transition, which took place over five days, required uninterrupted control of the GPS satellite constellation while control was transferred from a 1970s-era mainframe to a distributed architecture. We created an assurance case so that the procedural documentation we had could be restructured into a form amenable to analysis. The analysis concluded that there were no major hazards; this conclusion was validated by a successful transition."
            },
            {
                "reference_title": "Assurance Based Development of Critical Systems",
                "reference_link": "publication/4261637_Assurance_Based_Development_of_Critical_Systems",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2007",
                "reference_abstract": "Assurance based development (ABD) is the synergistic construction of a critical computing system and an assurance case that sets out the dependability claims for the system and argues that the available evidence justifies those claims. Co-developing the system and its assurance case helps software developers to make technology choices that address the specific dependability goal of each component. This approach gives developers: (1) confidence that the technologies selected will support the system's dependability goal and (2) flexibility to deploy expensive technology, such as formal verification, only on components whose assurance needs demand it. ABD simplifies the detection - and thereby avoidance - of potential assurance difficulties as they arise, rather than after development is complete. In this paper, we present ABD together with a case study of its use."
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Sep 2015",
                "reference_abstract": null
            },
            {
                "reference_title": null,
                "reference_link": null,
                "reference_type": null,
                "reference_date": "Oct 2015",
                "reference_abstract": null
            }
        ]
    },
    {
        "title": "Free Computer Science Project Topics and Materials in PDF",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": null,
        "abstract": "Free computer science project topics and research materials in PDF download for final year undergraduate and postgraduate students 2021. NCE, BSC, MSC, PGD, and Ph.D.",
        "reference": []
    },
    {
        "title": "Some 2-Class Groups Related to the Narrow 2-Class Field of Some Real Quadratic Number Fields--A Preliminary Hueristic Investigation--Corrected and Updated",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References",
        "abstract": "This is a corrected and updated version of the article that was published in Pinnacle Mathematics and Computer Science Journal in 2019. See my \"Narrow 2-Class Field Heuristic Figures\" article on Research Gate for the figures in the present article.",
        "reference": []
    },
    {
        "title": "A Survey on Crop recommendation System: Techniques, Challenges",
        "date": "December 2020",
        "doi": null,
        "conferance": "Conference: \u201cDisruptive Technology for Engineering and Scientific Applications",
        "citations_count": null,
        "reference_count": null,
        "abstract": "Agriculture is the cornerstone of a developing economy like India, because the income depends on agriculture for the majority of its population. Traditional farming approaches are still in existence, thus giving farmers minimal crop yields in the end which are less beneficial to farmers. Therefore, in order to maximize crop yields for a given input, we present analysis of different methods which will be useful in developing a smart farming recommendation system. Data mining is an important field of computer science which can be applied very effectively to the agricultural sector. The farmers are facing the difficulty of choosing the right crop to cultivate in right time which in turn reduces the crop yield and profit for agricultural stakeholders. This paper studies various techniques used for crop yield prediction and crop recommendation.",
        "reference": []
    },
    {
        "title": "Analytic proofs for logics of evidence and truth",
        "date": "December 2020",
        "doi": null,
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (16)",
        "abstract": "This paper presents a sound, complete, and decidable analytic\ntableau system for the logic of evidence and truth LETF, introduced in \nRodrigues, Bueno-Soler \\& Carnielli (Synthese, DOI: 10.1007/s11229-020-02571-w, 2020). \nLETF is an extension of the logic of first-degree entailment (FDE), also known as Belnap-Dunn logic. FDE is a widely studied four-valued paraconsistent logic, \nwith applications in computer science and in the algebra of processes. \nLETF extends FDE in a natural way, by adding a classicality operator o, which recovers \nclassical logic for propositions in its scope, and a non-classicality operator *, \nthe dual of o.",
        "reference": [
            {
                "reference_title": "An epistemic approach to paraconsistency: a logic of evidence and truth",
                "reference_link": "publication/321024784_An_epistemic_approach_to_paraconsistency_a_logic_of_evidence_and_truth",
                "reference_type": "Article",
                "reference_date": "Sep 2019",
                "reference_abstract": "The purpose of this paper is to present a paraconsistent formal system and a corresponding intended interpretation according to which true contradictions are not tolerated. Contradictions are, instead, epistemically understood as conflicting evidence, where evidence for a proposition A is understood as reasons for believing that A is true. The paper defines a paraconsistent and paracomplete natural deduction system, called the Basic Logic of Evidence (BLE), and extends it to the Logic of Evidence and Truth (LETj). The latter is a logic of formal inconsistency and undeterminedness that is able to express not only preservation of evidence but also preservation of truth. LETj is anti-dialetheist in the sense that, according to the intuitive interpretation proposed here, its consequence relation is trivial in the presence of any true contradiction. Adequate semantics and a decision method are presented for both BLE and LETj, as well as some technical results that fit the intended interpretation."
            },
            {
                "reference_title": "Valuation Semantics for Intuitionic Propositional Calculus and some of its Subcalculi",
                "reference_link": "publication/277263695_Valuation_Semantics_for_Intuitionic_Propositional_Calculus_and_some_of_its_Subcalculi",
                "reference_type": "Article",
                "reference_date": "Apr 2010",
                "reference_abstract": "In this paper, we present valuation semantics for the Propositional Intuitionistic Calculus (also called Heyting Calculus) and three important subcalculi: the Implicative, the Positive and the Minimal Calculus (also known as Kolmogoroff or Johansson Calculus).\nAlgorithms based in our de?nitions yields decision methods for these calculi."
            },
            {
                "reference_title": "Tableau systems for logics of formal inconsistency",
                "reference_link": "publication/236647961_Tableau_systems_for_logics_of_formal_inconsistency",
                "reference_type": "Conference Paper",
                "reference_date": "Jan 2001",
                "reference_abstract": "The logics of formal inconsistency (LFI's) are logics that allow to explicitly formalize the concepts of consistency and inconsistency by means of formulas of their language. Contradictoriness, on the other hand, can always be expressed in any logic, provided its language includes a symbol for negation. Besides being able to represent the distinction between contradiction and inconsistency, LFI's are non-explosive logics, in the sense that a contradiction does not entail arbitrary statements, but yet are gently explosive, in the sense that, adjoining the additional requirement of consistency, then contradictoriness do cause explosion. Several logics can be seen as LFI's, among them the great majority of paraconsistent systems developed under the Brazilian and Polish tradition. We present here tableau systems for some important LFI's: bC, Ci and LFI1."
            },
            {
                "reference_title": "Nearly every normal modal logic is paranormal",
                "reference_link": "publication/236647823_Nearly_every_normal_modal_logic_is_paranormal",
                "reference_type": "Article",
                "reference_date": "Jan 2005",
                "reference_abstract": "An overcomplete logic is a logic that \u2018ceases to make the difference\u2019: According to such a logic, all inferences hold independently of the nature of the statements involved. A negation-inconsistent logic is a logic having at least one model that satisfies both some statement and its negation. A negation-incomplete logic has at least one model according to which neither some statement nor its negation are satisfied. Paraconsistent logics are negation-inconsistent yet non-overcomplete; paracomplete logics are negation-incomplete yet non-overcomplete. A paranormal logic is simply a logic that is both paraconsistent and paracomplete. Despite being perfectly consistent and complete with respect to classical negation, nearly every normal modal logic, in its ordinary language and interpretation, admits to some latent paranormality: It is paracomplete with respect to a negation defined as an impossibility operator, and paraconsistent with respect to a negation defined as non-necessity. In fact, as is shown here, even in languages without a primitive classical negation, normal modal logics can often be alternatively characterized directly by way of their paranormal negations and related operators. So, instead of talking about \u2018necessity\u2019, \u2018possibility\u2019, and so on, modal logics could be seen just as devices tailored for the study of (modal) negation. This paper shows how and to what extent this alternative characterization of modal logics can be realized."
            },
            {
                "reference_title": "Process Algebra with Four-Valued Logic.",
                "reference_link": "publication/220202761_Process_Algebra_with_Four-Valued_Logic",
                "reference_type": "Article",
                "reference_date": "Jan 2000",
                "reference_abstract": "We propose a combination of a fragment of four-valued logic and process algebra. This fragment is geared to a simple relation with process algebra via the conditional guard construct, and can easily be extended to a truth-functionally complete logic. We present an operational semantics in SOS-style, and a completeness result for ACP with conditionals and four- valued logic. Completeness is preserved under the restriction to some other non-classical logics."
            },
            {
                "reference_title": "How a Computer Should Think",
                "reference_link": "publication/338335034_How_a_Computer_Should_Think",
                "reference_type": "Chapter",
                "reference_date": "Jan 2019",
                "reference_abstract": "I propose that a certain four-valued logic should sometimes be used. It is to be understood that I use \u201clogic\u201d in a narrow sense, the old sense: a logic as an organon, a tool, a canon of inference. And it is also to be understood that I use \u201cshould\u201d in a straightforward normative sense."
            },
            {
                "reference_title": "A semantical study of some propositional calculi",
                "reference_link": "publication/268829317_A_semantical_study_of_some_propositional_calculi",
                "reference_type": "Article",
                "reference_date": null,
                "reference_abstract": null
            },
            {
                "reference_title": "A four-valued semantics for terminological logics",
                "reference_link": "publication/222631361_A_four-valued_semantics_for_terminological_logics",
                "reference_type": "Article",
                "reference_date": "Apr 1989",
                "reference_abstract": "An intuitive four-valued semantics can be used to develop expressively powerful terminological logics which have tractable subsumption. The subsumptions supported by the logic are a type of \u201cstructural\u201d subsumption, where each structural component of one concept must have an analogue in the other concept. Structural subsumption captures an important set of subsumptions, similar to the subsumptions computed in kl-one and nikl. Thus the trade-off between expressive power and computational tractability which plagues terminological logics based on standard, two-valued semantics can be avoided while still retaining a useful and semantically supported set of subsumptions."
            },
            {
                "reference_title": "Multivalued logics: A uniform approach to reasoning in artificial intelligence",
                "reference_link": "publication/220541998_Multivalued_logics_A_uniform_approach_to_reasoning_in_artificial_intelligence",
                "reference_type": "Article",
                "reference_date": "Sep 1988",
                "reference_abstract": "This paper describes a uniform formalization of much of the current work in artificial intelligence on inference systems. We show that many of these systems, including first-order theorem provers, assumption-based truth maintenance systems (atmss), and unimplemented formal systems such as default logic or circumscription, can be subsumed under a single general framework.We begin by defining this framework, which is based on a mathematical structure known as a bilattice. We present a formal definition of inference using this structure and show that this definition generalizes work involving atmss and some simple nonmonotonic logics.Following the theoretical description, we describe a constructive approach to inference in this setting; the resulting generalization of both conventional inference and atmss is achieved without incurring any substantial computational overhead. We show that our approach can also be used to implement a default reasoner, and discuss a combination of default and atms methods that enables us to formally describe an \u201cincremental\u201d default reasoning system. This incremental system does not need to perform consistency checks before drawing tentative conclusions, but can instead adjust its beliefs when a default premise or conclusion is overturned in the face of convincing contradictory evidence. The system is therefore much more computationally viable than earlier approaches.Finally, we discuss the implementation of our ideas. We begin by considering general issues that need to be addressed when implementing a multivalued approach such as that we are proposing, and then turn to specific examples showing the results of an existing implementation. This single implementation is used to solve a digital simulation task using first-order logic, a diagnostic task using atmss as suggested by de Kleer and Williams, a problem in default reasoning as in Reiter's default logic or McCarthy's circumscription, and to solve the same problem more efficiently by combining default methods with justification information. All of these applications use the same general-purpose bilattice theorem prover and differ only in the choice of bilattice being considered.Le pr\u00e9sent article d\u00e9crit la formalisation uniforme d'une grande partie du travail courant dans le domain des syst\u00e8mes d'inf\u00e9rence. L'article montre que bon nombre de ces syst\u00e8mes, dont les d\u00e9monstrateurs de th\u00e9or\u00e8me de premier ordre, les syst\u00e8mes de v\u00e9rit\u00e9s logiques bas\u00e9es sur des hypoth\u00e8ses (ATMS) et les syst\u00e8mes formels qui ne sont pas mis en oeuvre, telle la logique implicite et la circonscription, peuvent \u011btre subsum\u00e9s dans un seul cadre g\u00e9n\u00e9ral.Nous commen\u00e7ons par d\u00e9finir ce cadre, qui est bas\u00e9 sur une structure mathematique appel\u00e9e treillis bidimensionnel. Nous donnons ensuite une d\u00e9finition formelle de l'inf\u00e9rence en utilisant cette structure, et montrons que cette d\u00e9finition g\u00e9n\u00e9ralise le travail mettant. en jeu les ATMS et une certaine forme de logique non monotone.Nous poursuivons, apr\u00e8s cette description th\u00e9orique, par la pr\u00e9sentation d'une approche constructive de l'inf\u00e9rence vue dans ce cadre; il en r\u00e9sulte une g\u00e9neralisation \u00e0 la fois de l'inf\u00e9rence conventionnelle et des ATMS, qui s'obtient sans n\u00e9cessiter aucun temps syst\u00e8me substantiel. Nous montrons que notre m\u00e9thode peut \u00e9galement servir \u00e0 mettre en oeuvre un syst\u00e8me de raisonnement implicite et discutons de la combinaison de m\u00e9thodes implicites et bas\u00e9es sur les ATMS, qui nous permet de faire la description formelle d'un syst\u00e9me de raisonnement implicite et incr\u00e9mentiel. Ce syst\u00e9me incr\u00e9mentiel n'a pas besoin d'effectuer des v\u00e9rifications de coh\u00e9rence avant de tirer des conclusions pr\u00e9liminaires; il peut, au lieu de cela, adapter ses croyances lorsque des pr\u00e9misses ou des conclusions implicites sont renvers\u00e9es en d\u00e9pit d'une \u00e9vidence constradictoire persuasive. De ce fait, ce syst\u00e8me est, du point de vue informatique, bien plus viable que les premi\u00e8res m\u00e9thodes.Enfin, nous discutons de la mise en oeuvre de nos id\u00e9es. Nous pr\u00e9sentons d'abord les questions qu'il faut analyser lorsque l'on veut mettre en oeuvre une m\u00e9thode multivalu\u00e9e, comme celle que nous proposons, puis nous passons \u00e0 des exemples sp\u00e9cifiques qui montrent les r\u00e9sultats d'une mise en oeuvre existante. Celle-ci sert \u00e0 r\u00e9soudre une t\u01ceche de simulation num\u00e9rique ayant recours \u00e0 la logique de premier ordre, un t\u01ceche de diagnostic ayant recours aux ATMS, tel que propos\u00e9 par de Kleer et Williams, un probl\u00e8me de raisonnement implicite tel qu'en comportent la logique implicite de Reiter et la circonscription de McCarthy, et \u00e0 r\u00e9soudre ce m\u011bme probl\u00e8me de fa\u00e7on plus efficace en combinant les m\u00e9thodes implicites avec des donn\u00e9es justificatives. Toutes ces applications font appel au m\u011bme d\u00e9monstrateur universel de th\u00e9or\u00e8me de treillis bidimensionnel et ne different que par le choix de du treillis bidimensionnel \u00e9tudi\u00e9."
            },
            {
                "reference_title": "A Sequent Calculus for Reasoning in Four-valued Description Logics",
                "reference_link": "publication/2620802_A_Sequent_Calculus_for_Reasoning_in_Four-valued_Description_Logics",
                "reference_type": "Article",
                "reference_date": "Jul 1999",
                "reference_abstract": ". Description Logics (DLs, for short) provide a logical reconstruction of the so-called frame-based knowledge representation languages. Originally, four-valued DLs have been proposed in order to develop expressively powerful DLs with tractable subsumption algorithms. Recently, four-valued DLs have been proposed as a model for (multimedia) document retrieval. In this context, the main reasoning task is instance checking. Unfortunately, the known subsumption algorithms for four-valued DLs, based on \"structural\" subsumption, do not work with respect to the semantics proposed in the DL-based approach to document retrieval. Moreover, they are unsuitable for solving the instance checking problem, as this latter problem is more general than the subsumption problem. We present an alternative decision procedure for four-valued DLs with the aim to solve these problems. The decision procedure is a sequent calculus for instance checking. Since in general the four-valued subsumption p..."
            }
        ]
    },
    {
        "title": "An Advanced Analysis of Cloud Computing Concepts Based on the Computer Science Ontology",
        "date": "December 2020",
        "doi": "10.32604/cmc.2021.013771",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (54)",
        "abstract": "Our primary research hypothesis stands on a simple idea: The evolution of top-rated publications on a particular theme depends heavily on the progress and maturity of related topics. And this even when there are no clear relations or some concepts appear to cease to exist and leave place for newer ones starting many years ago. We implemented our model based on Computer Science Ontol-ogy (CSO) and analyzed 44 years of publications. Then we derived the most important concepts related to Cloud Computing (CC) from the scientific collection offered by Clarivate Analytics. Our methodology includes data extraction using advanced web crawling techniques, data preparation, statistical data analysis , and graphical representations. We obtained related concepts after aggregating the scores using the Jaccard coefficient and CSO Ontology. Our article reveals the contribution of Cloud Computing topics in research papers in leading scientific journals and the relationships between the field of Cloud Computing and the interdependent subdivisions identified in the broader framework of Computer Science.",
        "reference": [
            {
                "reference_title": "Electronic Word-of-Mouth for Online Retailers: Predictors of Volume and Valence",
                "reference_link": "publication/330890037_Electronic_Word-of-Mouth_for_Online_Retailers_Predictors_of_Volume_and_Valence",
                "reference_type": "Article",
                "reference_date": "Feb 2019",
                "reference_abstract": "The goal of this research was to build a model that evaluates the influence of affective commitment, high-sacrifice commitment, and satisfaction on the customers\u2019 word-of-mouth concerning an online retailer. Two word-of-mouth dimensions were considered: volume and valence. A survey was administered to 282 respondents and structural equation modeling techniques were used to process the data and test the hypotheses. Our findings show that satisfaction and high-sacrifice commitment have an important impact on both word-of-mouth volume and valence, while affective commitment only influences word-of-mouth valence. This paper offers detailed explanations of these results in light of other theories and studies in the field."
            },
            {
                "reference_title": "Towards evaluation of cloud ontologies",
                "reference_link": "publication/330013697_Towards_evaluation_of_cloud_ontologies",
                "reference_type": "Article",
                "reference_date": "Dec 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Cloud-assisted gamification for education and learning \u2013 Recent advances and challenges",
                "reference_link": "publication/331443619_Cloud-assisted_gamification_for_education_and_learning_-_Recent_advances_and_challenges",
                "reference_type": "Article",
                "reference_date": "Mar 2019",
                "reference_abstract": "Gamification has gained considerable interest in education circles due to its capability of enhancing the learning process among students. In the future, it is expected that gamification will overtake the traditional way of learning resulting in issues such as scalability, upgradation of learning modules. To address these issues, merging gamification with cloud computing seems a viable solution. However, the employability of gamification through cloud computing is still in its infant stage. Hence, this article investigates the applicability of gamification through cloud computing and presents a comprehensive survey of state-of-the-art gamification in education and learning. We also identify the subject areas that can be gamified and taught using the cloud service. The critical elements and minimum requirements necessary to gamify education are also identified. Moreover, a specific cloud-assisted gamification architecture is proposed and discussed together with its possible applications. The article is concluded with the research challenges and suggestions for future work."
            },
            {
                "reference_title": "A Systematic Literature Review of Cloud Computing Use in Supply Chain Integration",
                "reference_link": "publication/330747150_A_Systematic_Literature_Review_of_Cloud_Computing_Use_in_Supply_Chain_Integration",
                "reference_type": "Article",
                "reference_date": "Mar 2019",
                "reference_abstract": "This paper analyzes the current state of research into Cloud Computing and Supply Chain Integration with the\nobjective to identify the findings to date, the areas of study developed and research gaps to provide guidance for\nfuture research. For this, a Systematic Literature Review was conducted, with 77 papers addressing the Cloud\nComputing-Supply Chain Integration relationship identified for analysis. These papers provide evidence of a\npositive relationship between the adoption of Cloud Computing use in process/activity integration, technology/\nsystem integration, and supply chain partner integration. The reviewed literature also indicates that Cloud\nComputing use in supply chain can also have an impact on the integration of the supply chain\u2019s information,\nphysical and/or financial flows."
            },
            {
                "reference_title": "Trusted Cloud Computing Architectures for Infrastructure as a Service: Survey and Systematic Literature Review",
                "reference_link": "publication/330111231_Trusted_Cloud_Computing_Architectures_for_Infrastructure_as_a_Service_Survey_and_Systematic_Literature_Review",
                "reference_type": "Article",
                "reference_date": "May 2019",
                "reference_abstract": "Cloud computing is no longer the future but the present. Security and trust are critical in cloud computing, but how can cloud service tenants trust cloud service providers to store all their private data on the cloud? Trusted computing is one of the new technologies in the last decade, and the integration between cloud computing and trusted computing can create a new architecture for infrastructure as a service that motivates more cloud service tenants to trust cloud service providers. This paper provides a survey and systematic literature review on the suggested architectures for this integration."
            },
            {
                "reference_title": "Understanding the effect of cloud computing on organizational agility: An empirical examination",
                "reference_link": "publication/329341629_Understanding_the_effect_of_cloud_computing_on_organizational_agility_An_empirical_examination",
                "reference_type": "Article",
                "reference_date": "Dec 2018",
                "reference_abstract": "The emergence of cloud computing has significantly changed the model used for existing information technology and enhanced agility of a firm through its pay per use mode. However, few studies have focused on this phenomenon, and prior studies are unclear regarding the impact of cloud computing on organizational agility. Therefore, this study investigates the link between cloud computing and organizational agility based on survey data from users of the Alibaba cloud in China. The empirical analysis is conducted using the partial least squares (PLS) based structural equation modeling with SmartPLS 2.0. We propose that two cloud computing related capabilities (i.e., CI flexibility and CI integration) are critical for firms to improve their agility. Based on the IT\u2014agility contradiction, we analyze the moderating effect of IT spending on cloud computing. This study provides a new perspective for understanding cloud computing technology in practice and theory."
            },
            {
                "reference_title": "Internet of Things (IoT), mobile cloud, cloudlet, mobile IoT, IoT cloud, fog, mobile edge, and edge emerging computing paradigms: Disambiguation and research directions",
                "reference_link": "publication/328734311_Internet_of_Things_IoT_mobile_cloud_cloudlet_mobile_IoT_IoT_cloud_fog_mobile_edge_and_edge_emerging_computing_paradigms_Disambiguation_and_research_directions",
                "reference_type": "Article",
                "reference_date": "Nov 2018",
                "reference_abstract": "Currently, we are experiencing a technological shift, which is expected to change the way we program and interact with the world. Cloud computing and mobile computing are two prominent research areas that have already had such an impact. The Internet of Things (IoT), which is concerned with building a network of Internet-enabled devices to promote a smart environment, is another promising area of research. Numerous emerging computing paradigms related to those areas of research and/or their intersections have come into play. These include Mobile Cloud Computing (MCC), cloudlet computing, mobile clouds, mobile IoT computing, IoT cloud computing, fog computing, Mobile Edge Computing (MEC), edge computing, the Web of Things (WoT), the Semantic WoT (SWoT), the Wisdom WoT (W2T), opportunistic sensing, participatory sensing, mobile crowdsensing, and mobile crowdsourcing. Unfortunately, those paradigms suffer from the lack of standard definitions, and so we frequently encounter a single term referring to various paradigms or several terms referring to a single paradigm. Accordingly, this paper attempts to disambiguate those paradigms and explain how and where they fit in the above three areas of research and/or their intersections before it becomes a serious problem. They are tracked back to their inception as much as possible. This is in addition to discussing research directions in each area. The paper also introduces technologies related to the IoT such as ubiquitous and pervasive computing, the Internet of Nano Things (IoNT), and the Internet of Underwater Things (IoUT)."
            },
            {
                "reference_title": "GiPlot: An interactive cloud-based tool for visualizing and interpreting large spectral data sets",
                "reference_link": "publication/328632266_GiPlot_An_interactive_cloud-based_tool_for_visualizing_and_interpreting_large_spectral_data_sets",
                "reference_type": "Article",
                "reference_date": "Oct 2018",
                "reference_abstract": "Latest advances in technology and the growing amount of experimental and business data have increased the number of users accessing on-line tools dedicated to quickly visualize and analyse large data sets. This paper describes the development and functionality of a new interactive cloud computing based plotting tool (GiPlot - Google-based Interactive Plot) easy-to-use for universal data. It has interactive features that facilitate data share and interpretation, and selection of specific data suitable for further uses and detailed studies. It also allows quick and step-by-step visualizations of the impact of various experimental conditions on spectral data sets. For a detailed illustration of the features of this interactive plotting tool, we have used mainly spectral data for a given solute dissolved in mixed solvents and for changes in the absorption and/or fluorescence properties of a solute solution in the presence of different chemical stimuli. The most important features and functionalities of this new tool have also been summarized and suggestively highlighted through a short collection of video tutorials containing many examples, developed by the authors of this paper as a support for both the tool and this paper."
            },
            {
                "reference_title": "Examining the role of the AIS research literature using the natural experiment of the 2018 JIS conference on cloud computing",
                "reference_link": "publication/328086523_Examining_the_role_of_the_AIS_research_literature_using_the_natural_experiment_of_the_2018_JIS_conference_on_cloud_computing",
                "reference_type": "Article",
                "reference_date": "Oct 2018",
                "reference_abstract": "This paper examines the nature and purpose of the AIS research literature, doing so in a focused setting that enables a more nuanced analysis than is possible when reviewing the field as a whole. Specifically, I examine the population of the AIS literature on cloud computing, a technology chosen because it was the subject of the recently held 2018 Journal of Information Systems annual conference. I treat the conference as a natural experiment because the choice of papers for that conference is the best indication of the current state of thinking about not just cloud computing in AIS, but of the kind of research considered desirable in the field. The published and conference papers are analyzed using the frameworks of Alles, Kogan and Vasarhelyi (2008) and O'Leary (2008) who provide a guide for how AIS researchers can write papers that give them a comparative advantage relative to other, competing researchers, and to do the right research at the right time in the right way. Based on this analysis, I conclude that the AIS cloud computing literature largely fails to establish a clear role for itself relative to either the field of accounting or of cloud computing. That is largely the result of the AIS community being unwilling to distinguish between cloud computing as an end in itself as a research subject and the cloud as a means towards an end as a medium of data exchange. Adopting a broad perspective on the domain of AIS research yields a larger set of research topics but increases the riskiness of producing research that fails to add value to the broader literature. This is a lesson that extends beyond cloud computing to the AIS research literature as a whole."
            },
            {
                "reference_title": "Recent security challenges in cloud computing",
                "reference_link": "publication/327992751_Recent_security_challenges_in_cloud_computing",
                "reference_type": "Article",
                "reference_date": "Oct 2018",
                "reference_abstract": "Cloud computing is an archetype that enables access to a shared pool of computing resources for cloud users in an on-demand or pay-per-use, fashion. Cloud computing offers several benefits to users and organizations, in terms of capital expenditure and savings in operational expenditure. Despite the existence of such benefits, there are some obstacles that place restrictions on the usage of cloud computing. Security is a major issue that is always considered. The lack of this vital feature results in the negative impact of the computing archetype thus resulting in personal, ethical, and financial harm. This paper will focus and explore the security challenges that are faced by cloud entities. These entities include Cloud Service Provider, the Data Owner and Cloud User. Focusing on the crypto-cloud that constitutes of different Communication, Computation, and Service Level Agreement. Studying the causes and effects of various cyber attacks it will provide the necessary upgrades."
            }
        ]
    },
    {
        "title": "Algorithms as a Basis of Modern Applied Mathematics",
        "date": "January 2021",
        "doi": "10.1007/978-3-030-61334-1",
        "conferance": null,
        "citations_count": null,
        "reference_count": null,
        "abstract": "This book offers a self-contained guide to advanced algorithms and their applications in various fields of science. Gathering contributions by authoritative researchers in the field of mathematics, statistics and computer science, it aims at offering a comprehensive and up-to-date view of algorithms, including the theory behind them, as well as practical considerations, current limitations and solutions. It covers applications in energy management, decision making, computer networks, materials science, mechanics and process optimization. It offers an integrated and timely guide to important algorithms, and represents a valuable reference resource for graduate students and researchers in various fields of applied mathematics, statistics and engineering.",
        "reference": []
    },
    {
        "title": "ASSISTECH: An Accidental Journey into Assistive Technology",
        "date": "January 2021",
        "doi": "10.1007/978-3-030-47487-4_5",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (17)",
        "abstract": "ASSISTECH is an inter-disciplinary laboratory located in Amar Nath Sashi Khosla School of Information Technology and is born out of the initial work of faculty and students in Department of Computer Science and Engineering. ASSISTECH has achieved some success in its 12 + years of existence in not only designing affordable products and solutions for mobility and education of visually impaired but also in building an ecosystem of translational research and dissemination to reach the end users. It is not a journey of any major technological breakthrough but of perseverance in creating an ecosystem for delivery and reaching out and connecting to people with passion to make a difference in this domain. In this article I have tried to capture this unusual journey of direct societal connect and impact\u2014not very common in an academic Institution.",
        "reference": [
            {
                "reference_title": "Basic Identity Tags (BITs) in Tactile Perception of 2D Shape",
                "reference_link": "publication/327792473_Basic_Identity_Tags_BITs_in_Tactile_Perception_of_2D_Shape",
                "reference_type": "Article",
                "reference_date": "Mar 2018",
                "reference_abstract": "Effective design of tactile graphics in education of the blind and visually impaired has long been debated by researchers, particularly whether pictorial sources are required. This work demonstrates that users of tactile graphics recognize distinctive abstract features (as the authors call them, \"basic identity tags\" or BITs), to differentiate, identify and remember embossed 2D shapes. A series of experiments which were conducted in New Delhi, India and Indianapolis, USA have been discussed. Understanding BITs can expand strategies for the design of effective tactile graphics."
            },
            {
                "reference_title": "Assistive technology solutions for aiding travel of pedestrians with visual impairment",
                "reference_link": "publication/319170291_Assistive_technology_solutions_for_aiding_travel_of_pedestrians_with_visual_impairment",
                "reference_type": "Article",
                "reference_date": "Aug 2017",
                "reference_abstract": "This work systematically reviews the assistive technology solutions for pedestrians with visual impairment and reveals that most of the existing solutions address a specific part of the travel problem. Technology-centered approach with limited focus on the user needs is one of the major concerns in the design of most of the systems. State-of-the-art sensor technology and processing techniques are being used to capture details of the surrounding environment. The real challenge is in conveying this information in a simplified and understandable form especially when the alternate senses of hearing, touch, and smell have much lesser perception bandwidth than that of vision. A lot of systems are at prototyping stages and need to be evaluated and validated by the real users. Conveying the required information promptly through the preferred interface to ensure safety, orientation, and independent mobility is still an unresolved problem. Based on observations and detailed review of available literature, the authors proposed that holistic solutions need to be developed with the close involvement of users from the initial to the final validation stages. Analysis reveals that several factors need serious consideration in the design of such assistive technology solutions."
            },
            {
                "reference_title": "Tactile Diagrams for the Visually Impaired",
                "reference_link": "publication/312337915_Tactile_Diagrams_for_the_Visually_Impaired",
                "reference_type": "Article",
                "reference_date": "Jan 2017",
                "reference_abstract": "In this information-driven era, the use of pictorial forms of communication has become widely popular, making the world increasingly inaccessible to visually impaired and blind individuals. Accessing print media has also been a challenge for this population. Braille, a standard writing system for perception through touch, and audio translations of written text have made books accessible to the visually impaired. However, both these formats fail to make pictures and graphics accessible to the blind."
            },
            {
                "reference_title": "DETC2007-35238 CANE MOUNTED KNEE-ABOVE OBSTACLE DETECTION AND WARNING SYSTEM FOR THE VISUALLY IMPAIRED",
                "reference_link": "publication/265149514_DETC2007-35238_CANE_MOUNTED_KNEE-ABOVE_OBSTACLE_DETECTION_AND_WARNING_SYSTEM_FOR_THE_VISUALLY_IMPAIRED",
                "reference_type": "Article",
                "reference_date": "Jan 2007",
                "reference_abstract": "There are numerous constraints that visually challenged people face in independent mobility and navigation. They primarily use the white cane as a mobility aid allowing them to detect close by obstacles on the ground. The detection of objects above knee height is almost impossible and is a major hindrance for them. In this work, we have reported the design and implementation of a detachable unit which acts to augment the functionality of the existing white cane, to allow knee-above obstacle detection. This unit consists of an ultrasonic ranger and a vibrator controlled by an eight bit microcontroller to offer an increased detection range of three meters. The distance information is conveyed to the user through non-interfering multi-frequency vibratory stimuli, the frequency of vibration indicating the proximity of obstacles. This unit is also capable of detecting fast moving obstacles.Considerable effort has gone into the electromechanical design of this unit conveying the vibrations effectively and ensuring that it is easily attachable on the existing white cane without sighted assistance. A crucial design optimization goal was cost - the unit has been developed as a \"low cost \" device which is affordable by the poor in developing countries."
            },
            {
                "reference_title": "Smart Backing Cane For Visually Impaired",
                "reference_link": "publication/342401860_Smart_Backing_Cane_For_Visually_Impaired",
                "reference_type": "Article",
                "reference_date": "May 2020",
                "reference_abstract": null
            },
            {
                "reference_title": "Making Legacy Digital Content Accessible at Source",
                "reference_link": "publication/335059613_Making_Legacy_Digital_Content_Accessible_at_Source",
                "reference_type": "Conference Paper",
                "reference_date": "May 2019",
                "reference_abstract": "Nearly three decades have passed since the Unicode standard was first published in 1991. A lot of electronically generated content is still locked inside legacy encodings. This can be attributed to lack of software support for Unicode at the time of content creation. The target output originally was print and this went on unnoticed. Later, to meet the growing demand for digital content, the same content in legacy encodings had to be exported to unsearchable PDF's/EPUB's. Conversion to Unicode has been a challenge because digital publishing applications cannot provide built in conversion support for the multitude of legacy encodings. Conversion tools, even where available, are external to the source application and require manual effort not only for text export/import but also for correcting errors in conversion and changes in document layout.\nWe have tried to address this problem for Devanagari script where the digital publishing application is InDesign[1] or PageMaker and the textual content is in the form of text and not images. InDesign allows import of PageMaker documents and InDesign's scripting allows access and modification of the document content directly. The tools have been successfully used to convert legacy Devanagari content from 5 distinct legacy encodings in over 100 textbooks meant for K-12 schools in India."
            },
            {
                "reference_title": "Pushpak: Voice Command-based eBook Navigator",
                "reference_link": "publication/335058997_Pushpak_Voice_Command-based_eBook_Navigator",
                "reference_type": "Conference Paper",
                "reference_date": "May 2019",
                "reference_abstract": "Effective utilization of screen reading software requires a user to remember various keystrokes/gestures. For beginners, this requirement increases the learning curve. This is even more challenging on touch screen devices due to limited possible gestures and lack of semantic intuition. In this project, we have implemented a voice command-based eBook navigator to address the challenge of the heavy learning curve, as voice command provides flexibility through the intent understanding module."
            },
            {
                "reference_title": "Microsoft Excel Chart Accessibility: An Affordable and Effective Solution",
                "reference_link": "publication/327894701_Microsoft_Excel_Chart_Accessibility_An_Affordable_and_Effective_Solution",
                "reference_type": "Conference Paper",
                "reference_date": "Feb 2016",
                "reference_abstract": null
            },
            {
                "reference_title": "FPGA-Based Controllers for Compact Low Power Refreshable Braille Display",
                "reference_link": "publication/326949329_FPGA-Based_Controllers_for_Compact_Low_Power_Refreshable_Braille_Display",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Edutactile - A Tool for Rapid Generation of Accurate Guideline-Compliant Tactile Graphics for Science and Mathematics",
                "reference_link": "publication/295261506_Edutactile_-_A_Tool_for_Rapid_Generation_of_Accurate_Guideline-Compliant_Tactile_Graphics_for_Science_and_Mathematics",
                "reference_type": "Conference Paper",
                "reference_date": "Jul 2014",
                "reference_abstract": "In this paper the authors have presented the design and implementation of Edutactile, a cross-platform software which automates the process of creation of tactile diagrams. Edutactile provides for automated application of guidelines or presets as well as Braille translation and thus abstracts away the production related issues. This relieves special educators for the visually challenged from having to learn the workings of the graphics editing software (Photoshop, CorelDraw) which are currently being used to produced tactile graphics and instead focus on the content of the diagram."
            }
        ]
    },
    {
        "title": "Peter Marwedel and the Department of Computer Science of the TU Dortmund University",
        "date": "January 2021",
        "doi": "10.1007/978-3-030-47487-4_1",
        "conferance": null,
        "citations_count": null,
        "reference_count": null,
        "abstract": "A seventieth birthday is an appropriate occasion to look back on the life path so far. In the following, this is done for Professor Peter Marwedel from the perspective of the Department of Computer Science of TU Dortmund University, where he spent the essential time of his professional life.",
        "reference": []
    },
    {
        "title": "Valid attacks in argumentation frameworks with recursive attacks",
        "date": "February 2021",
        "doi": "10.1007/s10472-020-09693-4",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (25)",
        "abstract": "The purpose of this work is to study a generalisation of Dung\u2019s abstract argumentation frameworks that allows representing recursive attacks, that is, a class of attacks whose targets are other attacks. We do this by developing a theory of argumentation where the classic role of attacks in defeating arguments is replaced by a subset of them, which is \u201cextension-dependent\u201d and which, intuitively, represents a set of \u201cvalid attacks\u201d with respect to the extension. The studied theory displays a conservative generalisation of Dung\u2019s semantics (complete, preferred, stable and grounded) and also of its principles (conflict-freeness, acceptability and admissibility). Furthermore, despite its conceptual differences, we are also able to show that our theory agrees with the AFRA interpretation of recursive attacks for the complete, preferred, stable and grounded semantics and with a recent flattening method.",
        "reference": [
            {
                "reference_title": "Argumentation Frameworks with Recursive Attacks and Evidence-Based Supports",
                "reference_link": "publication/324580317_Argumentation_Frameworks_with_Recursive_Attacks_and_Evidence-Based_Supports",
                "reference_type": "Chapter",
                "reference_date": "Jan 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "Logical Encoding of Argumentation Frameworks with Higher-Order Attacks",
                "reference_link": "publication/329740552_Logical_Encoding_of_Argumentation_Frameworks_with_Higher-Order_Attacks",
                "reference_type": "Conference Paper",
                "reference_date": "Nov 2018",
                "reference_abstract": null
            },
            {
                "reference_title": "On the Acceptability Semantics of Argumentation Frameworks with Recursive Attack and Support.",
                "reference_link": "publication/324842028_On_the_Acceptability_Semantics_of_Argumentation_Frameworks_with_Recursive_Attack_and_Support",
                "reference_type": "Conference Paper",
                "reference_date": "Sep 2016",
                "reference_abstract": "The Attack-Support Argumentation Framework (ASAF) is an abstract argumentation framework that provides a unified setting for representing attack and support for arguments, as well as attack and support for the attack and support relations at any level. Currently, the extensions of the ASAF are obtained by translating it into a Dung's Argumentation Framework (AF). In this work we provide the ASAF with the ability of determining its extensions without requiring such a translation. We follow an extension-based approach for characterizing the acceptability semantics directly on the ASAF, considering the complete, preferred, stable and grounded semantics. Finally, we show that the proposed characterization satisfies different results from Dung's argumentation theory."
            },
            {
                "reference_title": "On the acceptability of arguments and its fundamental role in nonmonotonic reasoning and logic programming",
                "reference_link": "publication/285432624_On_the_acceptability_of_arguments_and_its_fundamental_role_in_nonmonotonic_reasoning_and_logic_programming",
                "reference_type": "Article",
                "reference_date": "Jan 1993",
                "reference_abstract": null
            },
            {
                "reference_title": "An approach to abstract argumentation with recursive attack and support",
                "reference_link": "publication/273399547_An_approach_to_abstract_argumentation_with_recursive_attack_and_support",
                "reference_type": "Article",
                "reference_date": "Dec 2015",
                "reference_abstract": "This work introduces the Attack\u2013Support Argumentation Framework (ASAF), an approach to abstract argumentation that allows for the representation and combination of attack and support relations. This framework extends the Argumen-tation Framework with Recursive Attacks (AFRA) in two ways. Firstly, it adds a support relation enabling to express support for arguments; this support can also be given to attacks, and to the support relation itself. Secondly, it extends AFRA's attack relation by allowing attacks to the aforementioned support relation. Moreover, since the support relation of the ASAF has a necessity interpretation, the ASAF also extends the Argumentation Framework with Necessities (AFN). Thus, the ASAF provides a unified framework for representing attack and support for arguments, as well as attack and support for the attack and support relations at any level."
            },
            {
                "reference_title": "Inductive Defense for Sceptical Semantics of Extended Argumentation",
                "reference_link": "publication/266982034_Inductive_Defense_for_Sceptical_Semantics_of_Extended_Argumentation",
                "reference_type": "Article",
                "reference_date": "Apr 2011",
                "reference_abstract": "An abstract argumentation framework may have many extensions. Which extension should be adopted as the semantics depends on the sceptical attitudes of the reasoners. Different degrees of scepticism lead to different semantics ranging from the grounded extension as the most sceptical semantics to preferred extensions as the least sceptical semantics. Extending abstract argumentation to allow attacks to be attacked, subjects attacks to argumentation and hence gives rise to a new dimension of scepticism for characterizing how sceptically attacks are accepted. In this article, we present a semantics based on the notion of inductive (grounded) defense of attacks which is sceptical towards the acceptance of attacks but credulous towards the acceptance of arguments.We show that the semantics preserves fundamental properties of abstract argumentation including the monotonicity of the characteristic function. We further show that any extension of the semantics proposed by Gabbay; Baroni, Cerutti, Giacomin and Guida; and Modgil contains a sceptical part being an extension of our semantics, and a credulous part resulted from its credulousness towards the acceptance of attacks. We then introduce a stratified form of extended argumentation which still allows an unbounded number of levels of attacks against attacks while assuring that all proposed semantics coincide. We also develop a sound and complete dialectical proof procedure for the presented semantics following a model of dispute that alternates between argumentation to accept arguments and to accept attacks."
            },
            {
                "reference_title": "Complete Extensions as Clark's Completion Semantics",
                "reference_link": "publication/261020710_Complete_Extensions_as_Clark's_Completion_Semantics",
                "reference_type": "Conference Paper",
                "reference_date": "Oct 2013",
                "reference_abstract": "According to Dung, the sets of arguments which can be considered as admissible from an argumentation framework can be regarded as logical models of a given logic program. Clark's completions defines a basic logic programming semantics which has influenced modern non-monotonic semantics such as Answer Set Semantics. The Complete Semantics is a fundamental argumentation semantics which identifies a set of admissible sets which contains the grounded, stable, preferred and ideal semantics. In this paper we introduce a characterization of the complete semantics in terms of logical models using Clark's completions. Given that we use a unique mapping which characterizes the grounded, stable, preferred and ideal semantics, our characterization argues for a strong bridge between argumentation semantics and logic programming semantics with negation as failure. This paper also seeks to draw attention to the correspondence between the complete semantics of argumentation frameworks and models of Clark's completion, since this correspondence also allows us to identify the possibility of computing argumentation frameworks based on integer programming."
            },
            {
                "reference_title": "Modelling defeasible and prioritized support in bipolar argumentation",
                "reference_link": "publication/257514780_Modelling_defeasible_and_prioritized_support_in_bipolar_argumentation",
                "reference_type": "Article",
                "reference_date": "Dec 2012",
                "reference_abstract": "Cayrol and Lagasquie-Schiex introduce bipolar argumentation frameworks by introducing a second relation on the arguments for representing the support among them. The main drawback of their approach is that they cannot encode defeasible support, for instance they cannot model an attack towards a support relation. In this paper, we introduce a way to model defeasible support in bipolar argumentation frameworks. We use the methodology of meta-argumentation in which Dung\u2019s theory is used to reason about itself. Dung\u2019s well-known admissibility semantics can be used on this meta-argumentation framework to compute the acceptable arguments, and all properties of Dung\u2019s classical theory are preserved. Moreover, we show how different contexts can lead to the alternative strengthening of the support relation over the attack relation, and converse. Finally, we present two applications of our methodology for modeling support, the case of arguments provided with an internal structure and the case of abstract dialectical frameworks."
            },
            {
                "reference_title": "Answer-set programming encodings for argumentation frameworks",
                "reference_link": "publication/228653271_Answer-set_programming_encodings_for_argumentation_frameworks",
                "reference_type": "Article",
                "reference_date": "Jun 2010",
                "reference_abstract": "We present reductions from Dung\u2019s argumentation framework (AF) and generalizations thereof to logic programs under the answer-set semantics. The reduction is based on a fixed disjunctive datalog program (the interpreter) and its input which is the only part depending on the AF to process. We discuss the reductions, which are the basis for the system ASPARTIX in detail and show their adequacy in terms of computational complexity."
            },
            {
                "reference_title": "AFRA: Argumentation framework with recursive attacks",
                "reference_link": "publication/223682054_AFRA_Argumentation_framework_with_recursive_attacks",
                "reference_type": "Article",
                "reference_date": "Jan 2011",
                "reference_abstract": "The issue of representing attacks to attacks in argumentation is receiving an increasing attention as a useful conceptual modelling tool in several contexts. In this paper we present AFRA, a formalism encompassing unlimited recursive attacks within argumentation frameworks. AFRA satisfies the basic requirements of definition simplicity and rigorous compatibility with Dung\u2019s theory of argumentation. This paper provides a complete development of the AFRA formalism complemented by illustrative examples and a detailed comparison with other recursive attack formalizations."
            }
        ]
    },
    {
        "title": "A review of human skin detection applications based on image processing",
        "date": "February 2021",
        "doi": "10.11591/eei.v10i1.2497",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (77)",
        "abstract": "In computer science, virtual image processing is the use of a digital computer to manipulate digital images through an algorithm for many applications. To begin with a new research topic, the must trend application that gets many requests to develop should know. Therefore, many applications based on human skin and human life are reviewed in this article, such as detection, classification, blocking, cryptography, identification, localization, steganography, segmentation, tracking, and recognition. In this article, the published articles with the topic of human skin-based image processing are investigated. The international publishers, such as Springer, IEEE, arXiv, and Elsevier are selected. The searching is implemented with the duration criteria of 2015-2019. It noted that human skin detection and recognition are the most repetitive articles with 43% and 28.5%, respectively of the total number of the investigated articles. The usage of human skin models is being widely used in the image processing of various applications. Keywords: Applications of human skin Dataset Skin color modeling Skin detection This is an open access article under the CC BY-SA license.",
        "reference": [
            {
                "reference_title": "Convolutional Neural Networks for Leaf Image-Based Plant Disease Classification",
                "reference_link": "publication/339601352_Convolutional_Neural_Networks_for_Leaf_Image-Based_Plant_Disease_Classification",
                "reference_type": "Article",
                "reference_date": "Dec 2019",
                "reference_abstract": "span lang=\"EN-US\">Plant pathologists desire soft computing technology for accurate and reliable diagnosis of plant diseases. In this study, we propose an efficient soybean disease identification method based on a transfer learning approach by using a pre-trained convolutional neural network (CNN\u2019s) such as AlexNet, GoogleNet, VGG16, ResNet101, and DensNet201. The proposed convolutional neural networks were trained using 1200 plant village image dataset of diseased and healthy soybean leaves, to identify three soybean diseases out of healthy leaves. Pre-trained CNN used to enable a fast and easy system implementation in practice. We used the five-fold cross-validation strategy to analyze the performance of networks. In this study, we used a pre-trained convolutional neural network as feature extractors and classifiers. The experimental results based on the proposed approach using pre-trained AlexNet, GoogleNet, VGG16, ResNet101, and DensNet201 networks achieve an accuracy of 95%, 96.4 %, 96.4 %, 92.1%, 93.6% respectively. The experimental results for the identification of soybean diseases indicated that the proposed networks model achieves the highest accuracy</span"
            },
            {
                "reference_title": "Forecasting Financial Budget Time Series: ARIMA Random Walk vs LSTM Neural Network",
                "reference_link": "publication/339600773_Forecasting_Financial_Budget_Time_Series_ARIMA_Random_Walk_vs_LSTM_Neural_Network",
                "reference_type": "Article",
                "reference_date": "Dec 2019",
                "reference_abstract": "div style=\"\u2019text-align: justify;\">\nFinancial time series are volatile, non-stationary and non-linear data that are affected by external economic factors. There is several performant predictive approaches such as univariate ARIMA model and more recently Recurrent Neural Network. The accurate forecasting of budget data is a strategic and challenging task for an optimal management of resources, it requires the use of the most accurate model. We propose a predictive approach that uses and compares the Machine Learning ARIMA model and Deep Learning Recurrent LSTM model. The application and the comparative analysis shows that the LSTM model outperforms the ARIMA model, mainly thanks to the LSTMs ability to learn non-linear relationship from data.\n</div"
            },
            {
                "reference_title": "Adaptive ANN based differential protective relay for reliable power transformer protection operation during energisation",
                "reference_link": "publication/339600670_Adaptive_ANN_based_differential_protective_relay_for_reliable_power_transformer_protection_operation_during_energisation",
                "reference_type": "Article",
                "reference_date": "Dec 2019",
                "reference_abstract": "Power transformer is the most expensive equipment in electrical power system that needs continuous monitoring and fast protection response. Differential relay is usually used in power transformer protection scheme. This protection compares the difference of currents between transformer primary and secondary sides, with which a tripping signal to the circuit breaker is asserted. However, when power transformers are energized, the magnetizing inrush current is present and due to its high magnitude, the relay mal-operates. To prevent mal-operation, methods revolving around the fact that the relay should be able to discriminate between the magnetizing inrush current and the fault current must be studied. This paper presents an Artificial Neural Network(ANN) based differential relay that is designed to enable the differential relay to correct its mal-operation during energization by training the ANN and testing it with harmonic current as the restraining element. The MATLAB software is used to implement and evaluate the proposed differential relay. It is shown that the ANN based differential relay is indeed an adaptive relay when it is appropriately trained using the Network Fitting Tool. The improved differential relay models also include a reset part which enables automatic reset of the relays. Using the techniques of 2nd harmonic restraint and ANN to design a differential relay thus illustrates that the latter can successfully differentiate between magnetizing inrush and internal fault currents. With the new adaptive ANN-based differential relay, there is no mal-operation of the relay during energization. The ANN based differential relay shows better performance in terms of its ability to differentiate fault against energization current. Amazingly, the response time, when there is an internal fault, is 1 ms compared to 4.5 ms of the conventional 2nd harmonic restraint based relay."
            },
            {
                "reference_title": "Multiple features fusion based video face tracking",
                "reference_link": "publication/332236198_Multiple_features_fusion_based_video_face_tracking",
                "reference_type": "Article",
                "reference_date": "Aug 2019",
                "reference_abstract": "With the development of monitoring equipment and artificial intelligence technology, video face tracking under the big data background has become an important research hot spot in the field of public security. In order to track robustly under the circumstances of illumination variation, background clutter, fast motion, partial occlusion and so on, this paper proposed an algorithm combining a multi-feature fusion in the frame of particle filter and an improved mechanism, which consists of three main steps. At first, the color and edge features of human face were extracted from the video sequence. Meanwhile, color histograms and edge orientation histograms (EOH) were used to describe the facial features and beneficial to improve the efficiency of calculation. Then we employed a self-adaptive features fusion strategy to calculate the particle weight, which can effectively enhance the reliability of face tracking. Moreover, in order to solve the computational efficiency problem caused by too many particles, we added the integral histogram method to simplify the calculation complexity. At last, the object model was updated between the current object model and the initial model for alleviating the model drifts. Experiments conducted on testing dataset show that this proposed approach can robustly track single face with the cases of complex backgrounds, such as similar skin color, illumination change and occlusion, and perform better than color-based and edge-based methods in terms of both quantitative metrics and visual quality."
            },
            {
                "reference_title": "FASSEG: a FAce Semantic SEGmentation repository for face image analysis",
                "reference_link": "publication/331997975_FASSEG_a_FAce_Semantic_SEGmentation_repository_for_face_image_analysis",
                "reference_type": "Article",
                "reference_date": "Mar 2019",
                "reference_abstract": "The FASSEG repository is composed by four subsets containing face images useful for training and testing automatic methods for the task of face segmentation. Three subsets, namely frontal01, frontal02, and frontal 03 are specifically built for performing frontal face\nsegmentation. Frontal01 contains 70 original RGB images and the corresponding roughly\nlabelled ground-truth masks. Frontal02 contains the same image data, with high-precision\nlabelled ground-truth masks. Frontal03 consists in 150 annotated face masks of twins captured in various orientations, illumination conditions and facial expressions. The last subset, namely multipose01, contains more than 200 faces in multiple poses and the corresponding groundtruth masks. For all face images, ground-truth masks are labelled on six classes (mouth, nose, eyes, hair, skin, and background)."
            },
            {
                "reference_title": "A Pornographic Image and Video Filtering Application Using Optimized Nudity Recognition and Detection Algorithm",
                "reference_link": "publication/331762020_A_Pornographic_Image_and_Video_Filtering_Application_Using_Optimized_Nudity_Recognition_and_Detection_Algorithm",
                "reference_type": "Conference Paper",
                "reference_date": "Nov 2018",
                "reference_abstract": "The combination of multimedia technology and Internet provides an amiss channel for pornographic contents accessible by certain sensitive groups of people. Furthermore, the same channel provides the easiest medium to distribute illicit images and videos without an autonomous content supervision process. In this study, an application was developed grounded from a pixel-based approach and a skin tone detection filter to identify images and videos with a large skin color count and considered as pornographic in nature. With nudity detection algorithm as the foundation of the system, all multimedia files were preprocessed, segmented, and filtered to analyze skin-colored pixels by processing in YCbCr space and then classifying it as skin or non-skin pixels. Afterwards, the percentage of skin pixels relative to the size of the frames is calculated to be part of the mean baseline for nudity and non-nudity materials. Lastly, the application classifies the files as nude or not, and then filter it. The application was evaluated by supplying a dataset of 1,239 multimedia files (Images = 986; Videos = 253) collected from the Web. On the final testing set, the application obtained a precision of 90.33% and accuracy of 80.23% using the supplied dataset."
            },
            {
                "reference_title": "An End-to-End Face Recognition Method with Alignment Learning",
                "reference_link": "publication/338665045_An_End-to-End_Face_Recognition_Method_with_Alignment_Learning",
                "reference_type": "Article",
                "reference_date": "Mar 2020",
                "reference_abstract": "Many effective methods have been proposed for face recognition in the past decade and the face recognition accuracy is also gradually improved, but these algorithms usually need to perform face alignment process based on the prior knowledge of facial structure before extracting facial features. The face recognition system usually consists of face detection, face alignment, facial feature extraction, etc., which are independent of each other, and it is difficult to design and train the end-to-end face recognition model. In this paper, an end-to-end face recognition method based on spatial transformation layer is proposed. Specifically, the spatial transformation layer is placed in front of the feature extraction layer of the face recognition network, and the face region is aligned by alignment learning which requires neither prior knowledge nor artificially defined geometric transformation. The face identity category information allows the convolutional neural network to automatically learn the most appropriate face alignment. Simulation experiments on CASIA-WebFace, LFW (Labeled Face in the Wild) and YTF (Youtube Face) face database have shown that the suggested alignment learning algorithm in this paper can realize the end-to-end face recognition and can effectively improve the face recognition rate as well."
            },
            {
                "reference_title": "Focusnet: An Attention-Based Fully Convolutional Network for Medical Image Segmentation",
                "reference_link": "publication/334427899_Focusnet_An_Attention-Based_Fully_Convolutional_Network_for_Medical_Image_Segmentation",
                "reference_type": "Conference Paper",
                "reference_date": "Apr 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Robust Face Detection Using Hybrid Skin Color Matching under Different Illuminations",
                "reference_link": "publication/332583297_Robust_Face_Detection_Using_Hybrid_Skin_Color_Matching_under_Different_Illuminations",
                "reference_type": "Conference Paper",
                "reference_date": "Feb 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "High-performance Gesture Recognition System",
                "reference_link": "publication/332085418_High-performance_Gesture_Recognition_System",
                "reference_type": "Article",
                "reference_date": "Mar 2019",
                "reference_abstract": "With advances in technology, motion sensing has been applied in a wide range of devices. The system used for motion sensing captures images via a camera and analyzes the acquired images, also known as human-computer interaction, wherein gesture-controlled machines are considered as the most convenient kind and are most commonly used. However, the existing gesture recognition algorithm often has a long computation time and uses a huge amount of memory, preventing the algorithm from being implemented in embedded systems. To alleviate these problems, in this paper, we propose to reduce the computation time of the system by employing the lifting-based discrete wavelet transform (DWT). The different frequency bands featured in a lifting-based discrete wavelet can swiftly distinguish the face region from the hand region, reduce the image resolution, and thus reduce memory usage. In addition to recognizing gestures in a swift and accurate manner, the gesture recognition approach proposed in this study is also compatible with embedded systems. Our experimental results suggest that the proposed approach can reduce the execution time by up to 66% while achieving a high identification rate."
            }
        ]
    },
    {
        "title": "Nonlinear Control Strategies for a UAV Carrying a Load with Swing Attenuation",
        "date": "March 2021",
        "doi": "10.1016/j.apm.2020.09.027",
        "conferance": null,
        "citations_count": null,
        "reference_count": "References (37)",
        "abstract": "Two novel nonlinear control schemes for Unmanned Aerial Vehicles (UAV) carrying a load and their comparative results are presented in this paper. The goal is to carry the load to a desired position, with oscillation attenuation along the trajectory. The proposed control structures are hierarchical schemes consisting of nonlinear controllers to stabilize the vehicle translational movements and the payload swing together with a well-known state-dependent differential Riccati equation controller to stabilize the rotational dynamics. We present new methodologies where two nonlinear controllers are proposed to obtain precise aerial vehicle positioning and efficient load oscillation reduction by exploiting the natural coupling between the horizontal quadcopter movement and the payload oscillation. It is shown that asymptotic stability can be guaranteed by the use of the Lyapunov approach and La Salle\u2019s invariance principle. Numerical experiments were carried out to validate the nonlinear control behaviors where the results show improvements with respect to a strategy from the literature.",
        "reference": [
            {
                "reference_title": "Energy-Based Control and LMI-Based Control for a Quadrotor Transporting a Payload",
                "reference_link": "publication/337188502_Energy-Based_Control_and_LMI-Based_Control_for_a_Quadrotor_Transporting_a_Payload",
                "reference_type": "Article",
                "reference_date": "Nov 2019",
                "reference_abstract": "This paper presents the control of a quadrotor with a cable-suspended payload. The proposed control structure is a hierarchical scheme consisting of an energy-based control (EBC) to stabilize the vehicle translational dynamics and to attenuate the payload oscillation, together with a nonlinear state feedback controller based on an linear matrix inequality (LMI) to control the quadrotor rotational dynamics. The payload swing control is based on an energy approach and the passivity properties of the system\u2019s translational dynamics. The main advantage of the proposed EBC strategy is that it does not require excessive computations and complex partial differential equations (PDEs) for implementing the control algorithm. We present a new methodology for using an LMI to synthesize the controller gains for Lipschitz nonlinear systems with larger Lipschitz constants than other classical techniques based on LMIs. This theoretical approach is applied to the quadrotor rotational dynamics. Stability proofs based on the Lyapunov theory for the controller design are presented. The designed control scheme allows for the stabilization of the system in all its states for the three-dimensional case. Numerical simulations demonstrating the effectiveness of the controller are provided."
            },
            {
                "reference_title": "A Flight Control System for Small Unmanned Aerial Vehicle",
                "reference_link": "publication/325336816_A_Flight_Control_System_for_Small_Unmanned_Aerial_Vehicle",
                "reference_type": "Article",
                "reference_date": "May 2018",
                "reference_abstract": "The program adaptation of the controller for the flight control system (FCS) of an unmanned aerial vehicle (UAV) is considered. Linearized flight dynamic models depend mainly on the true airspeed of the UAV, which is measured by the onboard air data system. This enables its use for program adaptation of the FCS over the full range of altitudes and velocities, which define the flight operating range. FCS with program adaptation, based on static feedback (SF), is selected. The SF parameters for every sub-range of the true airspeed are determined using the linear matrix inequality approach in the case of discrete systems for synthesis of a suboptimal robust H\u221e-controller. The use of the Lagrange interpolation between true airspeed sub-ranges provides continuous adaptation. The efficiency of the proposed approach is shown against an example of the heading stabilization system."
            },
            {
                "reference_title": "A trajectory tracking control law for a quadrotor with slung load",
                "reference_link": "publication/334840642_A_trajectory_tracking_control_law_for_a_quadrotor_with_slung_load",
                "reference_type": "Article",
                "reference_date": "Aug 2019",
                "reference_abstract": "We present a trajectory tracking controller for the full dynamics of a quadrotor vehicle carrying a slung load attached by a string. The full dynamic system is modeled as two connected subsystems, the string\u2013load subsystem, with dynamics identical to that of a standard quadrotor in free flight, and the quadrotor subsystem with attitude kinematics and dynamics. A trajectory tracking controller for the position of the point-mass load is designed based on existing Lyapunov-based trajectory tracking controllers for free flying quadrotors which are further backstepped through the quadrotor attitude dynamics. A parameterized Lyapunov function is provided for the full system dynamics with a negative semi-definite time derivative. The proposed controller is proven to drive the load position error to zero and the origin of the error system is exponentially stable. Simulation results attest the performance of the proposed controller for aggressive trajectories and its robustness and validity are further highlighted by experimental results with a model-scale vehicle and slung load."
            },
            {
                "reference_title": "Nonlinear Backstepping Control of a Quadrotor-Slung Load System",
                "reference_link": "publication/334617203_Nonlinear_Backstepping_Control_of_a_Quadrotor-Slung_Load_System",
                "reference_type": "Article",
                "reference_date": "Jul 2019",
                "reference_abstract": null
            },
            {
                "reference_title": "Energy-Based Nonlinear Adaptive Control Design for the Quadrotor UAV System With a Suspended Payload",
                "reference_link": "publication/331611320_Energy-Based_Nonlinear_Adaptive_Control_Design_for_the_Quadrotor_UAV_System_With_a_Suspended_Payload",
                "reference_type": "Article",
                "reference_date": "Mar 2019",
                "reference_abstract": "In this paper, the control problem for an underactuated quadrotor unmanned aerial vehicle(UAV) with a suspended payload is investigated. An energy-based nonlinear controller is proposed that is able to control the quadrotor UAV's position and the payload's swing angle asymptotically. An adaptive control design is developed to compensate for the unknown length of the cable which is used to connect the UAV and the payload. The Lyapunov-based stability analysis is employed together to prove the stability of the closed-loop system. Detailed real-time experimental results illustrate the good performance of the proposed controller."
            },
            {
                "reference_title": "Disturbance suppression for quadrotor UAV using sliding-mode-observer-based equivalent-input-disturbance approach",
                "reference_link": "publication/331376339_Disturbance_suppression_for_quadrotor_UAV_using_sliding-mode-observer-based_equivalent-input-disturbance_approach",
                "reference_type": "Article",
                "reference_date": "Feb 2019",
                "reference_abstract": "This paper presents a new control scheme for quadrotor unmanned aerial vehicle attitude control and disturbance suppression. A quadrotor dynamic model is divided into two subsystems: fully-actuated and under-actuated. While the PID method is used to control the fully-actuated subsystem, a sliding-mode-observer-based equivalent-input-disturbance approach is used to control the under-actuated subsystem. The system design is simple, and it is globally uniformly ultimately bounded. Simulations and comparisons demonstrate the effectiveness of the method."
            },
            {
                "reference_title": "Avoiding obstacles in cooperative load transportation",
                "reference_link": "publication/330849897_Avoiding_obstacles_in_cooperative_load_transportation",
                "reference_type": "Article",
                "reference_date": "Feb 2019",
                "reference_abstract": "This work deals with load transportation by quadrotors, when the load is attached to the vehicles through flexible cables. More specifically, two quadrotors are used to carry a single load, which is attached to both vehicles, through such kind of cables. The idea of using two quadrotors working cooperatively to carry the load is adopted to suppress any load oscillation in the direction of movement, what would happen if just one UAV were used. As a consequence of using two UAVs (or more than two) it can happen collisions between the vehicles when carrying the load, caused by the forces the load exert on the two vehicles, whose tendency is to bring the vehicles closer one to the other when they accelerate forward. The paper proposes a strategy to avoid such collisions and any collision with obstacles eventually present in the working space as well. Simulated results are shown and discussed, using two AR.Drone\u00ae2.0 quadrotor to carry the load, which validate the proposed strategy."
            },
            {
                "reference_title": "Dynamics and control of a quadrotor with a payload suspended through an elastic cable",
                "reference_link": "publication/318334713_Dynamics_and_control_of_a_quadrotor_with_a_payload_suspended_through_an_elastic_cable",
                "reference_type": "Conference Paper",
                "reference_date": "May 2017",
                "reference_abstract": null
            },
            {
                "reference_title": "Dynamics and control of a quadrotor with a cable suspended payload",
                "reference_link": "publication/317724853_Dynamics_and_control_of_a_quadrotor_with_a_cable_suspended_payload",
                "reference_type": "Conference Paper",
                "reference_date": "Apr 2017",
                "reference_abstract": null
            },
            {
                "reference_title": "Robust guaranteed cost tracking control of quadrotor UAV with uncertainties",
                "reference_link": "publication/316077998_Robust_guaranteed_cost_tracking_control_of_quadrotor_UAV_with_uncertainties",
                "reference_type": "Article",
                "reference_date": "Apr 2017",
                "reference_abstract": "In this paper, a robust guaranteed cost controller (RGCC) is proposed for quadrotor UAV system with uncertainties to address set-point tracking problem. A sufficient condition of the existence for RGCC is derived by Lyapunov stability theorem. The designed RGCC not only guarantees the whole closed-loop system asymptotically stable but also makes the quadratic performance level built for the closed-loop system have an upper bound irrespective to all admissible parameter uncertainties. Then, an optimal robust guaranteed cost controller is developed to minimize the upper bound of performance level. Simulation results verify the presented control algorithms possess small overshoot and short setting time, with which the quadrotor has ability to perform set-point tracking task well."
            }
        ]
    }
]